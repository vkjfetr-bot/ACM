---
name: acm-master
description: "Complete ACM (Automated Condition Monitoring) expertise system for predictive maintenance and equipment health monitoring. PROACTIVELY activate for: (1) ANY ACM pipeline task (batch runs, coldstart, forecasting), (2) SQL Server data management (historian tables, ACM output tables), (3) Observability stack (Loki logs, Tempo traces, Prometheus metrics, Pyroscope profiling), (4) Grafana dashboard development, (5) Detector tuning and fusion configuration, (6) Model lifecycle management, (7) Debugging pipeline issues. Provides: T-SQL patterns for ACM tables, batch runner usage, detector behavior, RUL forecasting, episode diagnostics, and production-ready pipeline patterns. Ensures professional-grade industrial monitoring following ACM v11.0.0 architecture."
---

# ACM Master Skill

## ðŸš¨ CRITICAL RULE #1: NEVER FILTER CONSOLE OUTPUT (NON-VIOLATABLE)

**THIS RULE CANNOT BE VIOLATED UNDER ANY CIRCUMSTANCES:**

When running ANY terminal command (ACM, Python scripts, SQL queries, etc.):
- **NEVER use `Select-Object -First N` or `-Last N`** to limit output
- **NEVER use `| head`, `| tail`, or any output truncation**
- **NEVER use `Out-String -Width` with small values**
- **ALWAYS show the COMPLETE, UNFILTERED output**
- **If output is long, that's OK - show ALL of it**

The user MUST see every single line of output. Filtering output hides critical errors, warnings, and diagnostic information.

**VIOLATION OF THIS RULE IS GROUNDS FOR IMMEDIATE TERMINATION OF THE CONVERSATION.**

---

## ðŸš¨ CRITICAL RULE #2: NO SINGLE-USE DIAGNOSTIC SCRIPTS

**The ONLY ways to test/diagnose ACM are:**

1. **Run ACM in batch mode** - `python scripts/sql_batch_runner.py --equip <EQUIP> --tick-minutes 1440 --max-batches 2`
2. **Check SQL tables** - `sqlcmd -S "server\instance" -d ACM -E -Q "SELECT ..."`
3. **Check ACM_RunLogs** - For error diagnosis
4. **Read console output** - Problems are diagnosed through logging

**NEVER CREATE:**
- Single-use diagnostic scripts to "check" or "validate" ACM behavior
- Scripts that simulate parts of the pipeline
- Test harnesses outside the standard batch runner

---

## ðŸŽ¯ When to Activate

PROACTIVELY activate for ANY ACM-related task:

- âœ… **Pipeline Execution** - Batch runs, coldstart, single equipment runs
- âœ… **SQL/T-SQL** - Historian tables, ACM output tables, stored procedures
- âœ… **Observability** - Traces (Tempo), Logs (Loki), Metrics (Prometheus), Profiling (Pyroscope)
- âœ… **Grafana Dashboards** - JSON development, time series queries, variable binding
- âœ… **Detector Tuning** - Fusion weights, thresholds, auto-tuning parameters
- âœ… **Model Lifecycle** - MaturityState, PromotionCriteria, model versioning
- âœ… **Forecasting** - RUL predictions, health forecasts, sensor forecasts
- âœ… **Debugging** - Pipeline errors, data issues, configuration problems

---

## ðŸ“‹ ACM Overview

### What ACM Is

ACM (Automated Condition Monitoring) is a predictive maintenance and equipment health monitoring system. It:
- Ingests sensor data from industrial equipment (FD_FAN, GAS_TURBINE, etc.) via SQL Server
- Runs multi-detector anomaly detection algorithms
- Calculates health scores and detects operating regimes
- Forecasts Remaining Useful Life (RUL) with Monte Carlo simulations
- Visualizes results through Grafana dashboards for operations teams

### Current Version: v11.0.0

**Key V11 Features:**
- ONLINE/OFFLINE pipeline mode separation (`--mode auto/online/offline`)
- MaturityState lifecycle (COLDSTART â†’ LEARNING â†’ CONVERGED â†’ DEPRECATED)
- Unified confidence model with ReliabilityStatus for all outputs
- RUL reliability gating (NOT_RELIABLE when model not CONVERGED)
- UNKNOWN regime (label=-1) for low-confidence assignments
- DataContract validation at pipeline entry
- Seasonality detection and adjustment

### Active Detectors (6 heads)

| Detector | Column Prefix | What's Wrong? | Fault Types |
|----------|---------------|---------------|-------------|
| **AR1** | `ar1_z` | Sensor drifting/spiking | Sensor degradation, control loop issues |
| **PCA-SPE** | `pca_spe_z` | Sensors are decoupled | Mechanical coupling loss, structural fatigue |
| **PCA-TÂ²** | `pca_t2_z` | Operating point abnormal | Process upset, load imbalance |
| **IForest** | `iforest_z` | Rare state detected | Novel failure mode, rare transient |
| **GMM** | `gmm_z` | Doesn't match known clusters | Regime transition, mode confusion |
| **OMR** | `omr_z` | Sensors don't predict each other | Fouling, wear, calibration drift |

**Removed Detectors:**
- `mhal_z` (Mahalanobis): Removed v10.2.0 - redundant with PCA-TÂ²
- `river_hst_z` (River HST): Removed - not implemented

---

## ðŸ”§ Pipeline Execution

### Primary Entry Points

```powershell
# Standard batch processing (RECOMMENDED for testing)
python scripts/sql_batch_runner.py --equip FD_FAN --tick-minutes 1440 --max-batches 2 --start-from-beginning

# Multiple equipment
python scripts/sql_batch_runner.py --equip FD_FAN GAS_TURBINE --tick-minutes 1440 --max-workers 2

# Resume from last run
python scripts/sql_batch_runner.py --equip FD_FAN --tick-minutes 1440 --resume

# Single equipment run (internal, rarely used directly)
python -m core.acm_main --equip FD_FAN --start-time "2024-01-01T00:00:00" --end-time "2024-01-02T00:00:00"
```

### Batch Runner Arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--equip` | Equipment name(s) | `FD_FAN GAS_TURBINE` |
| `--tick-minutes` | Window size in minutes | `1440` (1 day) |
| `--max-batches` | Limit number of batches | `2` |
| `--start-from-beginning` | Reset and start from earliest data | Flag |
| `--resume` | Continue from last completed batch | Flag |
| `--dry-run` | Show what would run without executing | Flag |
| `--max-workers` | Parallel equipment processing | `2` |
| `--mode` | Pipeline mode | `auto`, `online`, `offline` |

### Understanding Pipeline Phases

```
COLDSTART â†’ DATA_LOADING â†’ FEATURES â†’ DETECTORS â†’ FUSION â†’ FORECASTING â†’ PERSIST
```

Each phase logs with component tags:
- `[COLDSTART]` - Initial model training
- `[DATA]` - Data loading and validation
- `[FEAT]` - Feature engineering
- `[MODEL]` - Detector fitting/scoring
- `[REGIME]` - Operating regime detection
- `[FUSE]` - Multi-detector fusion
- `[FORECAST]` - RUL and health predictions
- `[OUTPUT]` - SQL persistence

---

## ï¿½ Script Relationships & Entry Points

### Entry Points Hierarchy

```
1. scripts/sql_batch_runner.py (PRODUCTION - Primary entry point)
   â””â”€â”€ core/acm_main.py::run_acm() (called via subprocess)
       â””â”€â”€ All pipeline phases (see Pipeline Phase Sequence below)

2. python -m core.acm_main --equip EQUIPMENT (TESTING - Single run)
   â””â”€â”€ core/acm_main.py::run_acm() (direct call)
       â””â”€â”€ All pipeline phases

3. core/acm.py (ALTERNATIVE - Mode-aware router)
   â”œâ”€â”€ Parses --mode (auto/online/offline)
   â”œâ”€â”€ Detects mode based on cached models if auto
   â””â”€â”€ Calls core/acm_main.py::run_acm() with mode
```

### Script Relationships

```
sql_batch_runner.py
â”œâ”€â”€ Purpose: Continuous batch processing, coldstart management, multi-equipment
â”œâ”€â”€ Calls: core/acm_main.py via subprocess (python -m core.acm_main)
â”œâ”€â”€ Manages: Coldstart state, batch windows, resume from last run
â”œâ”€â”€ SQL Tables: Reads ACM_ColdstartState, writes ACM_Runs
â””â”€â”€ Arguments:
    --equip FD_FAN GAS_TURBINE  # Multiple equipment
    --tick-minutes 1440          # Batch window size
    --max-workers 2              # Parallel equipment processing
    --start-from-beginning       # Full reset (coldstart)
    --resume                     # Continue from last run
    --max-batches 1              # Limit batches (testing)

core/acm_main.py
â”œâ”€â”€ Purpose: Single pipeline run (train/score/forecast)
â”œâ”€â”€ Imports: All core modules (see Module Dependency Graph)
â”œâ”€â”€ Manages: Model training, scoring, persistence
â””â”€â”€ Arguments:
    --equip FD_FAN               # Single equipment
    --start-time "2024-01-01T00:00:00"
    --end-time "2024-01-31T23:59:59"
    --mode offline|online|auto   # Pipeline mode

scripts/sql/verify_acm_connection.py
â”œâ”€â”€ Purpose: Test SQL Server connectivity
â”œâ”€â”€ Calls: core/sql_client.SQLClient
â””â”€â”€ Output: Connection test result

scripts/sql/export_comprehensive_schema.py
â”œâ”€â”€ Purpose: Export SQL schema to markdown
â”œâ”€â”€ Calls: SQL INFORMATION_SCHEMA
â””â”€â”€ Output: docs/sql/COMPREHENSIVE_SCHEMA_REFERENCE.md

scripts/sql/populate_acm_config.py
â”œâ”€â”€ Purpose: Sync config_table.csv to SQL ACM_Config
â”œâ”€â”€ Reads: configs/config_table.csv
â””â”€â”€ Writes: SQL ACM_Config table
```

---

## ðŸ”„ Pipeline Phase Sequence (acm_main.py)

The main pipeline executes in this order. Each phase corresponds to a timed section in the output:

```
PHASE 1: INITIALIZATION (startup)
â”œâ”€â”€ Parse CLI arguments (--equip, --start-time, --end-time, --mode)
â”œâ”€â”€ Load config from SQL (ConfigDict)
â”œâ”€â”€ Determine PipelineMode (ONLINE/OFFLINE/AUTO)
â”œâ”€â”€ Initialize OutputManager with SQL client
â””â”€â”€ Create RunID for this execution

PHASE 2: DATA CONTRACT VALIDATION (data.contract)
â”œâ”€â”€ DataContract.validate(raw_data)
â”œâ”€â”€ Check sensor coverage (min 70% required)
â”œâ”€â”€ Write ACM_DataContractValidation
â””â”€â”€ Fail fast if validation fails

PHASE 3: DATA LOADING (load_data)
â”œâ”€â”€ Load historian data from SQL (stored procedure)
â”œâ”€â”€ Apply coldstart split (60% train / 40% score)
â”œâ”€â”€ Validate timestamp column and cadence
â””â”€â”€ Output: train DataFrame, score DataFrame

PHASE 4: BASELINE SEEDING (baseline.seed)
â”œâ”€â”€ Load baseline from ACM_BaselineBuffer
â”œâ”€â”€ Check for overlap with score data
â””â”€â”€ Apply baseline for normalization

PHASE 5: SEASONALITY DETECTION (seasonality.detect)
â”œâ”€â”€ SeasonalityHandler.detect_patterns()
â”œâ”€â”€ Detect DAILY/WEEKLY cycles using FFT
â”œâ”€â”€ Apply seasonal adjustment if enabled (v11)
â””â”€â”€ Write ACM_SeasonalPatterns

PHASE 6: DATA QUALITY GUARDRAILS (data.guardrails)
â”œâ”€â”€ Check train/score overlap
â”œâ”€â”€ Validate variance and coverage
â”œâ”€â”€ Write ACM_DataQuality
â””â”€â”€ Output quality metrics

PHASE 7: FEATURE ENGINEERING (features.build + features.impute)
â”œâ”€â”€ fast_features.compute_all_features()
â”œâ”€â”€ Build rolling stats, lag features, z-scores
â”œâ”€â”€ Impute missing values from train medians
â”œâ”€â”€ Compute feature hash for caching
â””â”€â”€ Output: Feature matrices (train_features, score_features)

PHASE 8: MODEL LOADING/TRAINING (train.detector_fit)
â”œâ”€â”€ Check for cached models in ModelRegistry
â”œâ”€â”€ If OFFLINE or models missing:
â”‚   â”œâ”€â”€ Fit AR1 detector (ar1_detector.py)
â”‚   â”œâ”€â”€ Fit PCA detector (pca via sklearn)
â”‚   â”œâ”€â”€ Fit IForest detector (sklearn.ensemble)
â”‚   â”œâ”€â”€ Fit GMM detector (sklearn.mixture)
â”‚   â””â”€â”€ Fit OMR detector (omr.py)
â”œâ”€â”€ If ONLINE: Load all detectors from cache
â””â”€â”€ Output: Trained detector objects

PHASE 9: TRANSFER LEARNING CHECK (v11)
â”œâ”€â”€ AssetSimilarity.load_profiles_from_sql()
â”œâ”€â”€ Build profile for current equipment
â”œâ”€â”€ find_similar() to match equipment
â””â”€â”€ Log transfer learning opportunity

PHASE 10: DETECTOR SCORING (score.detector_score)
â”œâ”€â”€ Score all detectors on score data
â”œâ”€â”€ Compute z-scores per detector
â”œâ”€â”€ Output: scores_wide DataFrame with detector columns
â””â”€â”€ Columns: ar1_z, pca_spe_z, pca_t2_z, iforest_z, gmm_z, omr_z

PHASE 11: REGIME LABELING (regimes.label)
â”œâ”€â”€ regimes.label() with regime context
â”œâ”€â”€ Auto-k selection (silhouette/BIC scoring)
â”œâ”€â”€ Clustering on raw sensor values (GMM or KMeans)
â”œâ”€â”€ UNKNOWN regime (-1) for low-confidence assignments
â”œâ”€â”€ Write ACM_RegimeDefinitions
â””â”€â”€ Output: Regime labels per row

PHASE 12: MODEL PERSISTENCE (models.persistence.save)
â”œâ”€â”€ Save all models to SQL ModelRegistry
â”œâ”€â”€ Increment model version
â””â”€â”€ Write metadata to ACM_ModelHistory

PHASE 13: MODEL LIFECYCLE (v11)
â”œâ”€â”€ load_model_state_from_sql()
â”œâ”€â”€ Update model state with run metrics
â”œâ”€â”€ Check promotion criteria (LEARNING -> CONVERGED)
â”œâ”€â”€ Write ACM_ActiveModels
â””â”€â”€ Output: MaturityState (COLDSTART/LEARNING/CONVERGED/DEPRECATED)

PHASE 14: CALIBRATION (calibrate)
â”œâ”€â”€ Score TRAIN data for calibration baseline
â”œâ”€â”€ Compute adaptive clip_z from P99
â”œâ”€â”€ Self-tune thresholds for target FP rate
â””â”€â”€ Write ACM_Thresholds

PHASE 15: DETECTOR FUSION (fusion.auto_tune + fusion)
â”œâ”€â”€ Auto-tune detector weights (episode separability)
â”œâ”€â”€ Compute fused_z (weighted combination)
â”œâ”€â”€ CUSUM parameter tuning (k_sigma, h_sigma)
â”œâ”€â”€ Detect anomaly episodes
â””â”€â”€ Output: fused_alert, episode markers

PHASE 16: ADAPTIVE THRESHOLDS (thresholds.adaptive)
â”œâ”€â”€ Calculate per-regime thresholds
â”œâ”€â”€ Global thresholds: alert=3.0, warn=1.5
â””â”€â”€ Write to SQL

PHASE 17: TRANSIENT DETECTION (regimes.transient_detection)
â”œâ”€â”€ Detect state transitions (startup, trip, steady)
â”œâ”€â”€ Label transient periods
â””â”€â”€ Output: Transient state per row

PHASE 18: DRIFT MONITORING (drift)
â”œâ”€â”€ Compute drift metrics (CUSUM trend)
â””â”€â”€ Classify: STABLE, DRIFTING, FAULT

PHASE 19: OUTPUT GENERATION (persist.*)
â”œâ”€â”€ write_scores_wide() -> ACM_Scores_Wide
â”œâ”€â”€ write_anomaly_events() -> ACM_Anomaly_Events
â”œâ”€â”€ write_detector_correlation() -> ACM_DetectorCorrelation
â”œâ”€â”€ write_sensor_correlation() -> ACM_SensorCorrelations
â”œâ”€â”€ write_sensor_normalized_ts() -> ACM_SensorNormalized_TS
â”œâ”€â”€ write_asset_profile() -> ACM_AssetProfiles
â””â”€â”€ write_seasonal_patterns() -> ACM_SeasonalPatterns

PHASE 20: ANALYTICS GENERATION (outputs.comprehensive_analytics)
â”œâ”€â”€ _generate_health_timeline() -> ACM_HealthTimeline
â”œâ”€â”€ _generate_regime_timeline() -> ACM_RegimeTimeline
â”œâ”€â”€ _generate_sensor_defects() -> ACM_SensorDefects
â”œâ”€â”€ _generate_sensor_hotspots() -> ACM_SensorHotspots
â””â”€â”€ Compute confidence values (v11)

PHASE 21: FORECASTING (outputs.forecasting)
â”œâ”€â”€ ForecastEngine.run_forecast()
â”‚   â”œâ”€â”€ Load health history from ACM_HealthTimeline
â”‚   â”œâ”€â”€ Fit degradation model (Holt-Winters)
â”‚   â”œâ”€â”€ Generate health forecast -> ACM_HealthForecast
â”‚   â”œâ”€â”€ Generate failure forecast -> ACM_FailureForecast
â”‚   â”œâ”€â”€ Compute RUL with Monte Carlo -> ACM_RUL
â”‚   â”œâ”€â”€ Compute confidence and reliability (v11)
â”‚   â””â”€â”€ Generate sensor forecasts -> ACM_SensorForecast
â””â”€â”€ Write forecast tables

PHASE 22: RUN FINALIZATION (sql.run_stats)
â”œâ”€â”€ Write PCA loadings -> ACM_PCA_Loadings
â”œâ”€â”€ Write run statistics -> ACM_Run_Stats
â”œâ”€â”€ Write run metadata -> ACM_Runs
â””â”€â”€ Commit all pending SQL writes
```

---

## ðŸ“¦ Module Dependency Graph

```
sql_batch_runner.py
    â””â”€â”€ subprocess calls: core/acm_main.py

core/acm_main.py (MAIN ORCHESTRATOR)
    â”œâ”€â”€ utils/config_dict.py (ConfigDict)
    â”œâ”€â”€ core/sql_client.py (SQLClient)
    â”œâ”€â”€ core/output_manager.py (OutputManager)
    â”œâ”€â”€ core/observability.py (Console, Span, Metrics, T)
    â”œâ”€â”€ core/pipeline_types.py (DataContract, PipelineMode)
    â”œâ”€â”€ core/fast_features.py (compute_all_features)
    â”œâ”€â”€ core/ar1_detector.py (AR1Detector)
    â”œâ”€â”€ core/omr.py (OMRDetector)
    â”œâ”€â”€ core/regimes.py (label, detect_transient_states)
    â”œâ”€â”€ core/fuse.py (compute_fusion, detect_episodes)
    â”œâ”€â”€ core/adaptive_thresholds.py (calculate_thresholds)
    â”œâ”€â”€ core/drift.py (compute_drift_metrics)
    â”œâ”€â”€ core/model_persistence.py (save_models, load_models)
    â”œâ”€â”€ core/model_lifecycle.py (ModelState, promote_model)
    â”œâ”€â”€ core/confidence.py (compute_*_confidence)
    â”œâ”€â”€ core/seasonality.py (SeasonalityHandler)
    â”œâ”€â”€ core/asset_similarity.py (AssetSimilarity)
    â”œâ”€â”€ core/forecast_engine.py (ForecastEngine)
    â””â”€â”€ core/health_tracker.py (HealthTracker)

core/output_manager.py
    â”œâ”€â”€ core/sql_client.py (SQLClient)
    â”œâ”€â”€ core/observability.py (Console)
    â””â”€â”€ core/confidence.py (compute_*_confidence)

core/forecast_engine.py
    â”œâ”€â”€ core/sql_client.py (SQLClient)
    â”œâ”€â”€ core/degradation_model.py (fit_degradation)
    â”œâ”€â”€ core/rul_estimator.py (estimate_rul)
    â”œâ”€â”€ core/confidence.py (compute_rul_confidence)
    â”œâ”€â”€ core/model_lifecycle.py (load_model_state_from_sql)
    â””â”€â”€ core/health_tracker.py (HealthTracker)

core/regimes.py
    â”œâ”€â”€ sklearn.mixture (GaussianMixture)  # v11.0.1: GMM for probabilistic clustering
    â”œâ”€â”€ sklearn.cluster (MiniBatchKMeans)  # fallback
    â”œâ”€â”€ sklearn.metrics (silhouette_score)
    â””â”€â”€ core/observability.py (Console)
```

---

## ï¿½ðŸ—„ï¸ SQL/T-SQL Best Practices

### CRITICAL: Use Microsoft SQL Server T-SQL Syntax

**ALWAYS use T-SQL, NEVER generic SQL:**

```sql
-- âœ… CORRECT: T-SQL patterns
SELECT TOP 10 * FROM ACM_Runs ORDER BY StartedAt DESC
SELECT DATEADD(HOUR, DATEDIFF(HOUR, 0, Timestamp), 0) AS HourStart FROM ACM_HealthTimeline
SELECT COALESCE(SUM(TotalEpisodes), 0) AS Total FROM ACM_EpisodeMetrics

-- âŒ WRONG: Generic SQL (NOT supported)
SELECT * FROM ACM_Runs ORDER BY StartedAt DESC LIMIT 10  -- LIMIT not supported!
SELECT DATE_TRUNC('hour', Timestamp) AS HourStart FROM ACM_HealthTimeline  -- DATE_TRUNC not supported!
```

### CRITICAL: Avoid Reserved Words as Aliases

**NEVER use these reserved words as column aliases:**
- `End`, `RowCount`, `Count`, `Date`, `Time`, `Order`, `Group`

**Use safe alternatives:**
- `EndTimeStr`, `TotalRows`, `TotalCount`, `DateValue`, `TimeValue`, `OrderNum`, `GroupName`

```sql
-- âŒ WRONG
SELECT COUNT(*) AS RowCount, EndTime AS End FROM ACM_Runs

-- âœ… CORRECT
SELECT COUNT(*) AS TotalRows, EndTime AS EndTimeStr FROM ACM_Runs
```

### Key ACM Tables

**Core Output Tables:**
- `ACM_Runs` - Run metadata (StartedAt, Outcome, RowsIn, RowsOut)
- `ACM_Scores_Wide` - Detector Z-scores per timestamp
- `ACM_HealthTimeline` - Health scores over time
- `ACM_RegimeTimeline` - Operating regime labels
- `ACM_Anomaly_Events` - Detected episodes with culprits
- `ACM_RUL` - RUL predictions with P10/P50/P90 bounds
- `ACM_HealthForecast` - Health projections
- `ACM_SensorDefects` - Active sensor defects

**V11 New Tables:**
- `ACM_ActiveModels` - Model lifecycle and maturity state
- `ACM_RegimeDefinitions` - Regime cluster definitions
- `ACM_DataContractValidation` - Data quality validation results
- `ACM_SeasonalPatterns` - Detected seasonal patterns
- `ACM_AssetProfiles` - Asset similarity profiles

### Common Queries

```sql
-- Check recent runs
SELECT TOP 20 RunID, EquipID, StartedAt, Outcome, RowsIn, RowsOut, DurationSec
FROM ACM_Runs ORDER BY StartedAt DESC

-- Get latest RUL prediction (CORRECT ordering!)
SELECT TOP 1 Method, RUL_Hours, P10_LowerBound, P50_Median, P90_UpperBound, Confidence
FROM ACM_RUL WHERE EquipID = 1 ORDER BY CreatedAt DESC

-- Check model lifecycle state
SELECT EquipID, Version, MaturityState, TrainingRows, SilhouetteScore
FROM ACM_ActiveModels WHERE EquipID = 1

-- Check run logs for errors
SELECT TOP 50 LoggedAt, Level, Component, Message
FROM ACM_RunLogs WHERE Level IN ('ERROR', 'WARN') ORDER BY LoggedAt DESC

-- Equipment data range
SELECT MIN(EntryDateTime) AS EarliestData, MAX(EntryDateTime) AS LatestData, COUNT(*) AS TotalRows
FROM FD_FAN_Data
```

### RUL Query Ordering (CRITICAL)

```sql
-- âœ… CORRECT: Get MOST RECENT prediction
SELECT TOP 1 * FROM ACM_RUL WHERE EquipID = 1 ORDER BY CreatedAt DESC

-- âŒ WRONG: Gets WORST-CASE from all history (misleading!)
SELECT TOP 1 * FROM ACM_RUL WHERE EquipID = 1 ORDER BY RUL_Hours ASC
```

---

## ðŸ“Š Observability Stack

### Docker Compose Stack

```powershell
# Start complete observability stack
cd install/observability; docker compose up -d

# Verify containers
docker ps --format "table {{.Names}}\t{{.Status}}"

# Expected containers:
# acm-grafana      (port 3000) - Dashboard UI, admin/admin
# acm-alloy        (port 4317, 4318) - OTLP collector
# acm-tempo        (port 3200) - Traces
# acm-loki         (port 3100) - Logs
# acm-prometheus   (port 9090) - Metrics
# acm-pyroscope    (port 4040) - Profiling

# Access Grafana
# Open http://localhost:3000 (admin/admin)

# Clean restart
docker compose down -v; docker compose up -d
```

### Console API (core/observability.py)

**ALWAYS use Console class for logging:**

```python
from core.observability import Console

# Use these methods:
Console.info("Message", component="COMP", **kwargs)    # General info â†’ Loki
Console.warn("Message", component="COMP", **kwargs)    # Warnings â†’ Loki
Console.error("Message", component="COMP", **kwargs)   # Errors â†’ Loki
Console.ok("Message", component="COMP", **kwargs)      # Success â†’ Loki
Console.status("Message")                               # Console-only (NO Loki)
Console.header("Title", char="=")                       # Section headers (NO Loki)
Console.section("Title")                                # Lighter separators (NO Loki)
```

**NEVER use:**
- `print()` - Use `Console.status()` instead
- `utils/logger.py` - Deleted in v10.3.0
- `utils/acm_logger.py` - Deleted in v10.3.0

### Trace-to-Logs/Metrics Linking

In Grafana datasources, trace attributes use `acm.` prefix:
- Span attribute: `acm.equipment`
- Query variable: `${__span.tags.equipment}` (after mapping `key: acm.equipment, value: equipment`)

---

## ðŸ“ˆ Grafana Dashboard Best Practices

### Time Series Queries

```sql
-- âœ… CORRECT: Return raw DATETIME, order ASC
SELECT Timestamp AS time, HealthScore AS value
FROM ACM_HealthTimeline
WHERE EquipID = $equipment
  AND Timestamp BETWEEN $__timeFrom() AND $__timeTo()
ORDER BY time ASC

-- âŒ WRONG: Don't use FORMAT() for time series
SELECT FORMAT(Timestamp, 'yyyy-MM-dd') AS time, HealthScore AS value  -- BREAKS time series!
```

### Panel Configuration

```json
{
  "custom": {
    "spanNulls": 3600000,      // Disconnect if gap > 1 hour (NOT true/false!)
    "lineInterpolation": "smooth"
  }
}
```

### Default Time Range

ACM dashboards should default to 5 years: `"from": "now-5y"`

---

## ðŸ”„ Model Lifecycle (V11)

### MaturityState Enum

```
COLDSTART â†’ LEARNING â†’ CONVERGED â†’ DEPRECATED
```

- **COLDSTART**: Initial model training, insufficient data
- **LEARNING**: Model accumulating data, not yet stable
- **CONVERGED**: Model meets promotion criteria, predictions reliable
- **DEPRECATED**: Model replaced by newer version

### Promotion Criteria (Configurable)

```csv
# configs/config_table.csv (v11.0.1 relaxed defaults)
0,lifecycle,promotion.min_training_days,7,int
0,lifecycle,promotion.min_silhouette_score,0.15,float
0,lifecycle,promotion.min_stability_ratio,0.6,float  # v11.0.1: relaxed from 0.8
0,lifecycle,promotion.min_consecutive_runs,3,int
0,lifecycle,promotion.min_training_rows,200,int  # v11.0.1: relaxed from 1000
```

### RUL Reliability Gating

```python
# RUL predictions are NOT_RELIABLE when:
# - Model maturity is COLDSTART or LEARNING
# - Confidence bounds are NULL
# - Health > 80% but RUL < 24h (likely false positive)
```

---

## ðŸ› Debugging Guide

### Pipeline Progress Logging

ACM uses `Console.status()` for progress messages that appear in console but NOT in Loki logs. Key progress checkpoints:

1. `[DATA] Kept N numeric columns` - Data columns validated
2. `Checking cadence and resampling...` - Cadence validation starting
3. `[DATA] SQL historian load complete` - Data loading finished
4. `Seeding baseline for EQUIP...` - Baseline seeding starting
5. `Loading baseline from ACM_BaselineBuffer...` - SQL baseline query
6. `[SEASON] Detected N seasonal patterns` - Seasonality detection complete
7. `[SEASON] Applied seasonal adjustment` - Seasonality adjustment applied
8. `[REGIME] Marked N/M points as UNKNOWN` - Regime labeling complete

If pipeline hangs after a progress message, the NEXT step is the bottleneck.

### Performance Hotspots (Common Bottlenecks)

**Top CPU-intensive operations in large batches (250K+ rows):**

| Operation | Typical Time | Cause | Solution |
|-----------|-------------|-------|----------|
| `seasonality.detect` | 30-70 min | `SeasonalityHandler.adjust_baseline` using row-by-row `.apply()` | **FIXED v11.0.1**: Vectorized implementation |
| `regimes.label` | 30-60 min | `smooth_labels` using Python for-loop | **FIXED v11.0.1**: Vectorized scipy.stats.mode |
| `outputs.comprehensive_analytics` | 10-20 min | Large SQL inserts to ACM_HealthTimeline (252K rows) | Batched inserts with commit intervals |
| `persist.write_scores` | 3-5 min | ACM_Scores_Wide inserts | Batched 5000-row inserts |

**If profiling shows these as bottlenecks, check for non-vectorized code patterns like:**
- `series.apply(lambda x: ...)` on large DataFrames
- `for idx, row in enumerate(...)` loops
- `np.unique()` called inside loops

### Common Issues

#### "Stuck after Kept N numeric columns"

**Symptom:** Pipeline logs `[DATA] Kept 9 numeric columns, dropped 0 non-numeric` then hangs.

**Causes:**
1. Slow cadence check on large score DataFrame
2. `_seed_baseline()` loading from `ACM_BaselineBuffer` (slow SQL query with 72h default window)
3. DataContract validation on large data

**Diagnosis:**
```sql
-- Check baseline buffer size
SELECT COUNT(*) AS BufferRows, MIN(Timestamp) AS Earliest, MAX(Timestamp) AS Latest
FROM ACM_BaselineBuffer WHERE EquipID = 1
```

**Solution:** 
- If buffer is huge (>100K rows), truncate old data
- Reduce `runtime.baseline.window_hours` from 72 to 24

#### "Stuck at seasonality.detect for 60+ minutes"

**Symptom:** Pipeline shows `[SEASON] Detected N seasonal patterns` then hangs for long time.

**Cause:** `SeasonalityHandler.adjust_baseline()` was using non-vectorized `Series.apply()` with `_compute_pattern_offset()` lambda.

**Solution (v11.0.1):** Now uses vectorized NumPy operations for 100x+ speedup.

#### "Stuck at regimes.label for 60+ minutes"

**Symptom:** Pipeline shows regime auto-k selection complete but then hangs.

**Cause:** `smooth_labels()` was using Python for-loop with `np.unique()` per row.

**Solution (v11.0.1):** Now uses scipy.stats.mode for vectorized mode computation.

#### "NOOP despite data existing"

**Cause:** Wrong parameter passed to stored procedure (`@EquipID` vs `@EquipmentName`).

**Solution:** Check `output_manager.py::_load_data_from_sql()` uses correct parameter name.

#### "RUL shows imminent failure (<24h) incorrectly"

**Cause:** Query using `ORDER BY RUL_Hours ASC` instead of `ORDER BY CreatedAt DESC`.

**Solution:** Always use most recent prediction: `ORDER BY CreatedAt DESC`.

### Diagnostic Queries

```sql
-- Check recent run outcomes
SELECT TOP 20 EquipID, StartedAt, Outcome, ErrorJSON
FROM ACM_Runs ORDER BY StartedAt DESC

-- Check data availability
SELECT EquipID, MIN(Timestamp), MAX(Timestamp), COUNT(*)
FROM ACM_Scores_Wide GROUP BY EquipID

-- Check model versions
SELECT EquipID, ModelType, Version, TrainedAt, TrainingRows
FROM ModelRegistry WHERE EquipID = 1 ORDER BY TrainedAt DESC
```

---

## ðŸ“ Project Structure

```
ACM/
â”œâ”€â”€ core/                 # Main codebase
â”‚   â”œâ”€â”€ acm_main.py       # Pipeline orchestrator (entry point)
â”‚   â”œâ”€â”€ output_manager.py # All CSV/PNG/SQL writes
â”‚   â”œâ”€â”€ sql_client.py     # SQL Server connectivity
â”‚   â”œâ”€â”€ observability.py  # Unified logging/traces/metrics
â”‚   â”œâ”€â”€ model_lifecycle.py # V11 maturity state management
â”‚   â”œâ”€â”€ forecast_engine.py # RUL and health forecasting
â”‚   â”œâ”€â”€ fuse.py           # Multi-detector fusion
â”‚   â”œâ”€â”€ regimes.py        # Operating regime detection
â”‚   â””â”€â”€ ...
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config_table.csv  # 238+ configuration parameters
â”‚   â””â”€â”€ sql_connection.ini # SQL credentials (gitignored)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sql_batch_runner.py # Primary batch processing
â”‚   â””â”€â”€ sql/              # SQL utilities
â”œâ”€â”€ docs/                 # All documentation
â”œâ”€â”€ grafana_dashboards/   # Grafana JSON dashboards
â”œâ”€â”€ install/observability/ # Docker Compose stack
â””â”€â”€ tests/                # pytest test suites
```

---

## âš ï¸ Common Mistakes to AVOID

| Category | âŒ Wrong | âœ… Correct |
|----------|---------|-----------|
| SQL columns | `ACM_RUL.LowerBound` | `ACM_RUL.P10_LowerBound` |
| SQL columns | `ACM_RUL.UpperBound` | `ACM_RUL.P90_UpperBound` |
| SQL columns | `ACM_Runs.StartTime` | `ACM_Runs.StartedAt` |
| SQL reserved | `AS End`, `AS RowCount` | `AS EndTimeStr`, `AS TotalRows` |
| SQL syntax | `LIMIT 10` | `TOP 10` |
| SQL syntax | `DATE_TRUNC('hour', ...)` | `DATEADD(HOUR, DATEDIFF(HOUR, 0, ...), 0)` |
| Time series | `FORMAT(time, 'yyyy-MM-dd')` | Return raw `DATETIME` |
| Time series | `ORDER BY time DESC` | `ORDER BY time ASC` |
| RUL queries | `ORDER BY RUL_Hours ASC` | `ORDER BY CreatedAt DESC` |
| Grafana | `"spanNulls": true` | `"spanNulls": 3600000` |
| PowerShell | `command1 && command2` | `command1; command2` |
| PowerShell | `tail -n 20` | `Select-Object -Last 20` |
| Logging | `print()` | `Console.status()` |
| Logging | Legacy loggers | `Console.info/warn/error` |

---

## ðŸ”§ Configuration System

### Config Loading

```python
from utils.config_dict import ConfigDict

# Load from CSV
cfg = ConfigDict.from_csv(Path("configs/config_table.csv"), equip_id=0)

# Access values
pca_components = cfg["models"]["pca"]["n_components"]  # 5
tick_minutes = cfg["runtime"]["tick_minutes"]  # 1440
```

### Key Configuration Parameters

**Data Loading:**
- `data.timestamp_col` = "EntryDateTime"
- `data.sampling_secs` = 1800 (30 min)
- `data.min_train_samples` = 200

**Detectors:**
- `models.pca.n_components` = 5
- `models.iforest.n_estimators` = 100
- `models.gmm.k_max` = 6

**Fusion:**
- `fusion.weights.ar1_z` = 0.20
- `fusion.weights.pca_spe_z` = 0.30
- `fusion.weights.pca_t2_z` = 0.20

**Forecasting:**
- `forecast.horizon_hours` = 168 (7 days)
- `forecast.alpha` = 0.30
- `forecast.failure_threshold` = 70.0

### Sync Config to SQL

After modifying `configs/config_table.csv`:
```powershell
python scripts/sql/populate_acm_config.py
```

---

## ðŸ§ª Testing

### Verify Imports

```powershell
python -c "from core import acm_main; print('OK')"
python -c "from core import model_lifecycle; print('OK')"
python -c "from core import observability; print('OK')"
```

### Verify SQL Connection

```powershell
python scripts/sql/verify_acm_connection.py
```

### Run Batch Test

```powershell
# Minimal test (2 batches)
python scripts/sql_batch_runner.py --equip FD_FAN --tick-minutes 1440 --max-batches 2 --start-from-beginning

# Watch for:
# - [SUCCESS] messages
# - "BATCH RUNNER COMPLETED SUCCESSFULLY"
# - No ERROR or WARN messages related to core functionality
```

### Run Unit Tests

```powershell
pytest tests/test_fast_features.py
pytest tests/test_observability.py
pytest tests/test_progress_tracking.py
```

---

## ðŸ“š Key Documentation

| Document | Purpose |
|----------|---------|
| `README.md` | Product overview, setup, running ACM |
| `docs/ACM_SYSTEM_OVERVIEW.md` | Architecture, module map, data flow |
| `docs/OBSERVABILITY.md` | Observability stack guide |
| `docs/sql/COMPREHENSIVE_SCHEMA_REFERENCE.md` | Authoritative SQL table definitions |
| `.github/copilot-instructions.md` | AI assistant guidelines |
| `install/observability/README.md` | Docker stack installation |

---

## ðŸ”„ Version History

| Version | Key Changes |
|---------|-------------|
| v11.0.2 | GMM replaces KMeans for regime clustering, transfer learning activation, correlation-aware detector fusion |
| v11.0.1 | Relaxed promotion criteria, vectorized seasonality/regime smoothing |
| v11.0.0 | MaturityState lifecycle, DataContract validation, seasonality detection, UNKNOWN regime |
| v10.3.0 | Unified observability (Console class), Docker Compose stack |
| v10.2.0 | Mahalanobis detector removed (redundant with PCA-TÂ²) |
| v10.0.0 | Continuous forecasting, hazard-based RUL, Monte Carlo simulations |

---

## V11.0.2 Implementation Details

### GMM Clustering for Operating Regimes

V11.0.2 replaces MiniBatchKMeans with Gaussian Mixture Models (GMM) for regime detection:

**Why GMM?**
- KMeans finds spherical density clusters, not operational modes
- GMM uses probabilistic soft assignments with confidence scores
- BIC (Bayesian Information Criterion) for optimal k selection
- Naturally supports UNKNOWN regime via low-probability assignments

**Implementation** (`core/regimes.py`):
```python
# BIC-based GMM model selection (k=1 to k_max)
from sklearn.mixture import GaussianMixture

def _fit_gmm_scaled(X_scaled, k_max=8, k_min=1, random_state=42):
    best_gmm, best_k, best_bic = None, 1, np.inf
    for k in range(k_min, k_max + 1):
        gmm = GaussianMixture(n_components=k, covariance_type="diag", random_state=random_state)
        gmm.fit(X_scaled)
        bic = gmm.bic(X_scaled)
        if bic < best_bic:
            best_gmm, best_k, best_bic = gmm, k, bic
    return best_gmm, best_k
```

**Fallback**: If GMM fails (e.g., covariance issues), KMeans is used as fallback.

### Transfer Learning Activation

V11.0.2 activates transfer learning for cold-start equipment:

**Implementation** (`core/acm_main.py` lines 4195-4265):
```python
# When detectors_missing and similar equipment found:
transfer_result = similarity_engine.transfer_baseline(
    source_id=transfer_source_id,
    target_id=equip_id,
    source_baseline=None
)
# TransferResult contains:
# - scaling_factors: Dict[str, float] per sensor
# - confidence: float 0-1
# - sensors_transferred: List[str]
```

**Logged to Console** (and Loki via observability):
- Source equipment ID
- Similarity score
- Sensor overlap count
- Transfer confidence

### Correlation-Aware Detector Fusion

V11.0.2 addresses FLAW-4 (PCA-SPE and PCA-TÂ² 80% correlated):

**Implementation** (`core/fuse.py` in `Fuser.fuse()` method):
```python
# Detect PCA detector correlation and discount weights
if "pca_spe_z" in keys and "pca_t2_z" in keys:
    corr, _ = pearsonr(spe_arr, t2_arr)
    if corr > 0.5:
        discount = min(0.5, (corr - 0.5))  # 0-50% discount
        w_raw["pca_spe_z"] *= (1 - discount)
        w_raw["pca_t2_z"] *= (1 - discount)
```

**Effect**: At 80% correlation, each PCA detector weight is reduced by ~30%, preventing PCA from dominating the fused score.
