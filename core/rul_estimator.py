"""
RUL and failure-forecast estimator
==================================

Consumes health timeline and forecast outputs and produces:
- Health forecast time series suitable for SQL (`ACM_HealthForecast_TS`)
- Failure probability over time (`ACM_FailureForecast_TS`)
- RUL time series and summary (`ACM_RUL_TS`, `ACM_RUL_Summary`)
- Sensor attribution at predicted failure time (`ACM_RUL_Attribution`)

This module is intentionally numerically simple (AR(1)-style trend + Gaussian
uncertainty); it can be replaced with richer models later without changing
the SQL/Grafana contracts.

Timestamp Policy:
-----------------
All timestamps are treated as timezone-naive local time.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd

from utils.logger import Console
from core.rul_common import norm_cdf, RULConfig


def _load_health_timeline(
    tables_dir: Path,
    sql_client: Optional[Any] = None,
    equip_id: Optional[int] = None,
    run_id: Optional[str] = None,
    output_manager: Optional[Any] = None,
) -> Optional[pd.DataFrame]:
    """
    Load health timeline for RUL estimation.

    RUL-01: Now supports artifact cache for SQL-only mode.
    
    Priority:
    1. Try artifact cache (output_manager) if available - SQL-only mode
    2. If sql_client and identifiers are available, read from ACM_HealthTimeline
    3. Fallback to health_timeline.csv in tables_dir (legacy file mode)
    """
    # RUL-01: First try artifact cache (SQL-only mode support)
    if output_manager is not None:
        df = output_manager.get_cached_table("health_timeline.csv")
        if df is not None:
            Console.info(f"[RUL] Using cached health_timeline.csv from OutputManager ({len(df)} rows)")
            # Normalize column names
            if "Timestamp" in df.columns:
                ts_col = "Timestamp"
            elif "timestamp" in df.columns:
                ts_col = "timestamp"
            else:
                ts_col = df.columns[0]
            # TIME-01: Ensure naive timestamps
            df[ts_col] = pd.to_datetime(df[ts_col], errors="coerce")
            if df[ts_col].dt.tz is not None:
                df[ts_col] = df[ts_col].dt.tz_localize(None)
            
            df = df.dropna(subset=[ts_col]).sort_values(ts_col)
            df = df.rename(columns={ts_col: "Timestamp"})
            return df
    
    # Try SQL when possible (SQL-only mode without cache)
    if sql_client is not None and equip_id is not None and run_id:
        try:
            cur = sql_client.cursor()
            cur.execute(
                """
                SELECT Timestamp, HealthIndex
                FROM dbo.ACM_HealthTimeline
                WHERE EquipID = ? AND RunID = ?
                ORDER BY Timestamp
                """,
                (equip_id, run_id),
            )
            rows = cur.fetchall()
            cur.close()
            if rows:
                df = pd.DataFrame.from_records(rows, columns=["Timestamp", "HealthIndex"])
                # TIME-01: Ensure naive timestamps
                df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors="coerce")
                if df["Timestamp"].dt.tz is not None:
                    df["Timestamp"] = df["Timestamp"].dt.tz_localize(None)
                
                df = df.dropna(subset=["Timestamp"]).sort_values("Timestamp")
                Console.info(
                    f"[RUL] Loaded {len(df)} health points from SQL for EquipID={equip_id}, RunID={run_id}"
                )
                return df
            else:
                Console.warn(
                    f"[RUL] No rows in ACM_HealthTimeline for EquipID={equip_id}, RunID={run_id}"
                )
        except Exception as e:
            Console.warn(f"[RUL] Failed to load health timeline from SQL: {e}")

    # Fallback: legacy CSV path (file mode)
    p = tables_dir / "health_timeline.csv"
    if not p.exists():
        Console.warn(f"[RUL] health_timeline.csv not found in {tables_dir} (no cache, no SQL, no file)")
        return None
    df = pd.read_csv(p)
    Console.info(f"[RUL] Loaded health_timeline.csv from file ({len(df)} rows)")
    if "Timestamp" in df.columns:
        ts_col = "Timestamp"
    elif "timestamp" in df.columns:
        ts_col = "timestamp"
    else:
        ts_col = df.columns[0]
    # TIME-01: Ensure naive timestamps
    df[ts_col] = pd.to_datetime(df[ts_col], errors="coerce")
    if df[ts_col].dt.tz is not None:
        df[ts_col] = df[ts_col].dt.tz_localize(None)
        
    df = df.sort_values(ts_col).dropna(subset=[ts_col])
    df = df.rename(columns={ts_col: "Timestamp"})
    return df


def _simple_ar1_forecast(
    ts: pd.Series, cfg: RULConfig
) -> Tuple[pd.Series, np.ndarray, np.ndarray]:
    """
    Fit a simple AR(1) on the last window of HealthIndex and forecast forward with
    growing uncertainty. Returns (forecast_series, forecast_std, horizons_hours).
    """
    y = ts.dropna().astype(float)
    if y.size < cfg.min_points:
        Console.warn(f"[RUL] Only {y.size} health points available; skipping RUL.")
        return pd.Series(dtype=float), np.array([]), np.array([])

    # Use last window (all for now)
    x = y.to_numpy(copy=False)
    mu = float(np.nanmean(x))
    if not np.isfinite(mu):
        mu = 0.0
    xc = x - mu
    var_xc = float(np.var(xc)) if xc.size else 0.0
    phi = 0.0
    if np.isfinite(var_xc) and var_xc >= 1e-8:
        num = float(np.dot(xc[1:], xc[:-1]))
        den = float(np.dot(xc[:-1], xc[:-1]))
        if abs(den) >= 1e-9:
            phi = num / den
    # Stabilize AR(1) to avoid near-unit-root explosions
    if not np.isfinite(phi):
        phi = 0.0
    phi = float(np.clip(phi, -0.99, 0.99))

    # TRAIN residual std
    x_shift = np.empty_like(x, dtype=np.float64)
    x_shift[0] = mu
    x_shift[1:] = x[:-1]
    pred_train = (x_shift - mu) * phi + mu
    resid = x - pred_train
    resid_for_sd = resid[1:] if resid.size > 1 else resid
    sd_train = float(np.std(resid_for_sd)) or 1.0
    if not np.isfinite(sd_train) or sd_train <= 0:
        sd_train = 1.0

    # infer cadence
    idx = y.index
    if not isinstance(idx, pd.DatetimeIndex):
        Console.warn("[RUL] HealthIndex index is not DateTimeIndex; cannot forecast.")
        return pd.Series(dtype=float), np.array([]), np.array([])
    idx = idx.sort_values()
    if len(idx) < 2:
        return pd.Series(dtype=float), np.array([]), np.array([])
    deltas = np.diff(idx.values.astype("int64")) / 1e9  # seconds
    step_sec = float(np.median(deltas))
    if step_sec <= 0:
        step_sec = 60.0
    samples_per_hour = 3600.0 / step_sec
    max_steps = int(np.ceil(cfg.max_forecast_hours * samples_per_hour))

    # build forecast index
    last_ts = pd.Timestamp(idx[-1])  # Ensure Timestamp type for arithmetic
    freq = pd.to_timedelta(step_sec, unit="s")
    idx_fore = pd.date_range(last_ts + freq, periods=max_steps, freq=freq)
    h_steps = np.arange(1, len(idx_fore) + 1, dtype=int)
    h_hours = h_steps * (step_sec / 3600.0)

    last_val = float(x[-1])
    
    # Detect if series is stationary (low drift)
    # Calculate drift from recent data
    recent_window = min(len(x), 50)
    x_recent = x[-recent_window:]
    t_recent = np.arange(len(x_recent))
    if len(x_recent) > 1:
        drift_slope = np.polyfit(t_recent, x_recent, 1)[0]
    else:
        drift_slope = 0.0
    is_stationary = abs(drift_slope) < 0.01  # Minimal drift threshold
    
    if abs(phi) > 1e-9:
        log_phi = np.log(abs(phi))
        sign_sequence = np.where(h_steps % 2 == 0, 1.0, np.sign(phi))
        phi_powers = sign_sequence * np.exp(log_phi * h_steps.astype(float))
    else:
        phi_powers = np.zeros_like(h_steps, dtype=float)
    
    base_forecast = mu + phi_powers * (last_val - mu)
    
    # Add realistic variation for stationary series to prevent flat visual lines
    if is_stationary:
        # Small sinusoidal variation (±0.5% of range or ±1.0 absolute minimum)
        data_range = np.ptp(x) if len(x) > 1 else 10.0
        variation_amplitude = max(data_range * 0.005, 1.0)
        variation = variation_amplitude * np.sin(h_steps / 6.0)
        yhat_values = base_forecast + variation
    else:
        yhat_values = base_forecast

    # growing variance
    if abs(phi) < 0.9999:
        phi_squared = phi ** 2
        phi_sq_powers = np.power(phi_squared, h_steps)
        var_ratio = (1 - phi_sq_powers) / (1 - phi_squared + 1e-9)
    else:
        var_ratio = h_steps.astype(float)
    var_ratio = np.clip(var_ratio, 1.0, 100.0)
    forecast_std = sd_train * np.sqrt(var_ratio)

    forecast = pd.Series(yhat_values, index=idx_fore, name="ForecastHealth")
    return forecast, forecast_std, h_hours


def estimate_rul_and_failure(
    tables_dir: Path,
    equip_id: Optional[int],
    run_id: Optional[str],
    health_threshold: float = 70.0,
    cfg: Optional[RULConfig] = None,
    sql_client: Optional[Any] = None,
    output_manager: Optional[Any] = None,
) -> Dict[str, pd.DataFrame]:
    """
    Main entrypoint used by acm_main.

    RUL-01: Now accepts output_manager for artifact cache support in SQL-only mode.

    Returns a dict of DataFrames keyed by SQL table name:
    - ACM_HealthForecast_TS
    - ACM_FailureForecast_TS
    - ACM_RUL_TS
    - ACM_RUL_Summary
    - ACM_RUL_Attribution
    - ACM_MaintenanceRecommendation
    """
    cfg = cfg or RULConfig(health_threshold=health_threshold)
    
    # Cleanup old forecast data to prevent RunID overlap in charts (SQL mode only)
    if sql_client is not None and equip_id is not None:
        try:
            import os
            try:
                keep_runs = int(os.getenv("ACM_FORECAST_RUNS_RETAIN", "2"))
            except Exception:
                keep_runs = 2
            keep_runs = max(1, min(int(keep_runs), 50))
            cur = sql_client.cursor()
            # Keep only the 2 most recent RunIDs to preserve some history while reducing clutter
            cur.execute("""
                WITH RankedRuns AS (
                    SELECT DISTINCT RunID, 
                           ROW_NUMBER() OVER (ORDER BY MAX(CreatedAt) DESC) AS rn
                    FROM dbo.ACM_HealthForecast_TS
                    WHERE EquipID = ?
                    GROUP BY RunID
                )
                DELETE FROM dbo.ACM_HealthForecast_TS
                WHERE EquipID = ? 
                  AND RunID IN (SELECT RunID FROM RankedRuns WHERE rn > ?)
            """, (equip_id, equip_id, keep_runs))
            
            cur.execute("""
                WITH RankedRuns AS (
                    SELECT DISTINCT RunID, 
                           ROW_NUMBER() OVER (ORDER BY MAX(CreatedAt) DESC) AS rn
                    FROM dbo.ACM_FailureForecast_TS
                    WHERE EquipID = ?
                    GROUP BY RunID
                )
                DELETE FROM dbo.ACM_FailureForecast_TS
                WHERE EquipID = ? 
                  AND RunID IN (SELECT RunID FROM RankedRuns WHERE rn > ?)
            """, (equip_id, equip_id, keep_runs))
            
            if not sql_client.conn.autocommit:
                sql_client.conn.commit()
            Console.info(f"[RUL] Cleaned old forecast data for EquipID={equip_id} (kept {keep_runs} RunIDs)")
        except Exception as e:
            Console.warn(f"[RUL] Failed to cleanup old forecasts: {e}")
            # Non-fatal, continue with RUL estimation
    
    health_df = _load_health_timeline(
        tables_dir, sql_client=sql_client, equip_id=equip_id, run_id=run_id,
        output_manager=output_manager
    )
    if health_df is None:
        Console.warn("[RUL] Health timeline not available; skipping RUL/forecast outputs.")
        return {}
    if "HealthIndex" not in health_df.columns or "Timestamp" not in health_df.columns:
        Console.warn(
            f"[RUL] Health timeline missing required columns (have={list(health_df.columns)})"
        )
        return {}

    # Prepare health time series
    hi = health_df["HealthIndex"].astype(float).clip(lower=0.0, upper=100.0)
    # TIME-01: Ensure naive timestamps
    ts_idx = pd.to_datetime(health_df["Timestamp"], errors="coerce")
    if ts_idx.dt.tz is not None:
        ts_idx = ts_idx.dt.tz_localize(None)
    hi.index = pd.DatetimeIndex(ts_idx)
    hi = hi.sort_index()
    if hi.size < cfg.min_points:
        Console.warn(
            f"[RUL] Not enough health points ({hi.size}, min={cfg.min_points}) for robust AR(1) RUL; "
            "falling back to naive flat forecast."
        )

    # Forecast health
    forecast, forecast_std, h_hours = _simple_ar1_forecast(hi, cfg)
    if forecast.empty:
        Console.warn("[RUL] Forecast series is empty; falling back to naive flat forecast.")
        # Naive fallback: hold last value flat over the forecast horizon
        last_ts = hi.index[-1]
        last_val = float(hi.iloc[-1])
        step_hours = 1.0
        max_h = max(float(cfg.max_forecast_hours), step_hours)
        h_hours = np.arange(step_hours, max_h + step_hours, step_hours, dtype=float)
        idx_fore = last_ts + pd.to_timedelta(h_hours, unit="h")
        forecast_values = np.full_like(h_hours, last_val, dtype=float)
        # Use empirical std of history as uncertainty; default to 1.0 if degenerate
        hist_std = float(np.nanstd(hi.values)) if hi.size > 1 else 1.0
        if not np.isfinite(hist_std) or hist_std <= 0:
            hist_std = 1.0
        forecast_std = np.full_like(h_hours, hist_std, dtype=float)
        forecast = pd.Series(forecast_values, index=idx_fore, name="ForecastHealth")
    # Ensure strictly positive, finite std
    forecast_std = np.asarray(forecast_std, dtype=float)
    forecast_std = np.where(np.isfinite(forecast_std), forecast_std, 1.0)
    forecast_std = np.clip(forecast_std, 1e-6, None)

    ci_k = 1.96
    ci_lower = forecast - ci_k * forecast_std
    ci_upper = forecast + ci_k * forecast_std

    # Failure probability by horizon (prob HealthIndex <= threshold)
    thr = float(np.clip(health_threshold, 0.0, 100.0))
    z = (thr - forecast.values) / (forecast_std + 1e-9)
    failure_prob = np.clip(norm_cdf(z), 0.0, 1.0)
    # NOTE: Removed np.maximum.accumulate - it was forcing all probabilities to be identical
    # when forecasts were similar. Let failure probability reflect actual forecast trajectory.
    # For improving health, failure prob should decrease; for degrading health, it increases naturally.

    # Determine RUL: earliest horizon where central forecast crosses threshold
    below = forecast.values <= thr
    if below.any():
        first_idx = int(np.argmax(below))
        rul_hours = float(h_hours[first_idx])
        failure_time = forecast.index[first_idx]
    else:
        # no crossing in horizon -> treat as large RUL
        rul_hours = float(h_hours[-1])
        failure_time = forecast.index[-1]

    # Bounds: use CI crossing points
    lower_cross = ci_lower.values <= thr
    upper_cross = ci_upper.values <= thr
    lower_bound_hours = float(h_hours[np.argmax(lower_cross)]) if lower_cross.any() else float(h_hours[-1])
    upper_bound_hours = float(h_hours[np.argmax(upper_cross)]) if upper_cross.any() else float(h_hours[-1])
    if lower_bound_hours > upper_bound_hours:
        lower_bound_hours, upper_bound_hours = upper_bound_hours, lower_bound_hours

    # Confidence: narrower CI => higher confidence
    ci_width_norm = (ci_upper.values - ci_lower.values) / (thr + 1e-9)
    conf = float(np.clip(1.0 - np.nanmean(ci_width_norm), 0.0, 1.0))

    # Build DataFrames for SQL
    run_id_val = run_id or ""
    equip_id_val = int(equip_id) if equip_id is not None else None

    def _insert_ids(df: pd.DataFrame) -> pd.DataFrame:
        if run_id_val:
            df.insert(0, "RunID", run_id_val)
        if equip_id_val is not None:
            df.insert(1 if run_id_val else 0, "EquipID", equip_id_val)
        return df

    # Health forecast TS
    # Clamp health and CI into valid [0, 100] domain for visualization stability
    forecast = forecast.clip(lower=0.0, upper=100.0)
    ci_lower = ci_lower.clip(lower=0.0, upper=100.0)
    ci_upper = ci_upper.clip(lower=0.0, upper=100.0)

    # CONTINUITY FIX: Include recent historical health data to bridge gap between history and forecast
    # This prevents stepped appearance in dashboard by providing smooth transition
    lookback_hours = 24.0  # Include last 24h of history
    last_forecast_ts = forecast.index[-1]
    cutoff_ts = forecast.index[0] - pd.Timedelta(hours=lookback_hours)
    recent_history = hi[hi.index >= cutoff_ts].copy()
    
    # Build forecast dataframe
    forecast_df = pd.DataFrame(
        {
            "Timestamp": forecast.index,
            "ForecastHealth": forecast.values,
            "CiLower": ci_lower.values,
            "CiUpper": ci_upper.values,
            "ForecastStd": forecast_std,
            "Method": "AR1_Health",
        }
    )
    
    # Build historical dataframe for recent data
    if len(recent_history) > 0:
        # For historical data, use actual health value and narrow CI based on measurement uncertainty
        hist_uncertainty = 2.0  # Small fixed uncertainty for historical data
        history_df = pd.DataFrame({
            "Timestamp": recent_history.index,
            "ForecastHealth": recent_history.values,
            "CiLower": np.clip(recent_history.values - hist_uncertainty, 0.0, 100.0),
            "CiUpper": np.clip(recent_history.values + hist_uncertainty, 0.0, 100.0),
            "ForecastStd": hist_uncertainty,
            "Method": "Historical",
        })
        # Concatenate history + forecast for seamless timeline
        health_forecast_df = pd.concat([history_df, forecast_df], ignore_index=True)
    else:
        health_forecast_df = forecast_df
    
    health_forecast_df = _insert_ids(health_forecast_df)

    # Failure probability TS
    failure_forecast_df = pd.DataFrame(
        {
            "Timestamp": forecast.index,
            "FailureProb": failure_prob,
            "ThresholdUsed": thr,
            "Method": "AR1_Health",
        }
    )
    
    # CONTINUITY FIX: Include historical failure probability (near zero for healthy history)
    if len(recent_history) > 0:
        # For historical data, calculate failure probability based on actual health vs threshold
        z_hist = (thr - recent_history.values) / (hist_uncertainty + 1e-9)
        from scipy.stats import norm as norm_dist
        failure_prob_hist = np.clip(norm_dist.cdf(z_hist), 0.0, 1.0)
        history_fail_df = pd.DataFrame({
            "Timestamp": recent_history.index,
            "FailureProb": failure_prob_hist,
            "ThresholdUsed": thr,
            "Method": "Historical",
        })
        failure_ts_df = pd.concat([history_fail_df, failure_forecast_df], ignore_index=True)
    else:
        failure_ts_df = failure_forecast_df
    
    failure_ts_df = _insert_ids(failure_ts_df)

    # RUL TS: from each forecast point, remaining hours until predicted failure
    remaining_hours = np.maximum(rul_hours - h_hours, 0.0)
    rul_ts_df = pd.DataFrame(
        {
            "Timestamp": forecast.index,
            "RUL_Hours": remaining_hours,
            "LowerBound": np.maximum(lower_bound_hours - h_hours, 0.0),
            "UpperBound": np.maximum(upper_bound_hours - h_hours, 0.0),
            "Confidence": conf,
            "Method": "AR1_Health",
        }
    )
    rul_ts_df = _insert_ids(rul_ts_df)

    # RUL summary
    now_ts = hi.index[-1]
    rul_summary_df = pd.DataFrame(
        [
            {
                "RUL_Hours": rul_hours,
                "LowerBound": lower_bound_hours,
                "UpperBound": upper_bound_hours,
                "Confidence": conf,
                "Method": "AR1_Health",
                "LastUpdate": now_ts,
            }
        ]
    )
    rul_summary_df = _insert_ids(rul_summary_df)

    # Sensor attribution at predicted failure time: use current hotspots as proxy
    attribution_df = _build_sensor_attribution(
        tables_dir=tables_dir,
        equip_id=equip_id_val,
        run_id=run_id_val,
        failure_time=failure_time,
        sql_client=sql_client,
    )

    # Maintenance recommendation window based on failure probability curve
    maint_df = _build_maintenance_recommendation(
        forecast_index=forecast.index,
        failure_prob=failure_prob,
        cfg=cfg,
        run_id=run_id_val,
        equip_id=equip_id_val,
    )

    return {
        "ACM_HealthForecast_TS": health_forecast_df,
        "ACM_FailureForecast_TS": failure_ts_df,
        "ACM_RUL_TS": rul_ts_df,
        "ACM_RUL_Summary": rul_summary_df,
        "ACM_RUL_Attribution": attribution_df,
        "ACM_MaintenanceRecommendation": maint_df,
    }


def _build_sensor_attribution(
    tables_dir: Path,
    equip_id: Optional[int],
    run_id: str,
    failure_time: pd.Timestamp,
    sql_client: Optional[Any] = None,
) -> pd.DataFrame:
    """
    Heuristic sensor attribution: use current SensorHotspots as proxy for which
    sensors are most likely to be responsible at predicted failure time.
    """
    # Prefer SQL when available
    if sql_client is not None and equip_id is not None and run_id:
        try:
            cur = sql_client.cursor()
            cur.execute(
                """
                SELECT TOP 10 SensorName, LatestAbsZ, AboveAlertCount
                FROM dbo.ACM_SensorHotspots
                WHERE EquipID = ? AND RunID = ?
                ORDER BY LatestAbsZ DESC
                """,
                (equip_id, run_id),
            )
            rows = cur.fetchall()
            cur.close()
            if rows:
                df = pd.DataFrame.from_records(rows, columns=["SensorName", "Z", "AlertCount"])
                df["Z"] = pd.to_numeric(df["Z"], errors="coerce").clip(lower=0.0)
                df = df.dropna(subset=["Z"])
                if df["Z"].sum() > 0:
                    df["FailureContribution"] = df["Z"] / df["Z"].sum()
                    df = df.sort_values("FailureContribution", ascending=False).head(10)
                else:
                    df = pd.DataFrame(columns=["SensorName", "FailureContribution", "Z", "AlertCount"])
            else:
                df = pd.DataFrame(columns=["SensorName", "FailureContribution", "Z", "AlertCount"])
        except Exception as e:
            Console.warn(f"[RUL] Failed to load sensor hotspots from SQL: {e}")
            df = pd.DataFrame(columns=["SensorName", "FailureContribution", "Z", "AlertCount"])
    else:
        # Fallback to CSV (file mode)
        p = tables_dir / "sensor_hotspots.csv"
        if not p.exists():
            Console.warn(f"[RUL] sensor_hotspots.csv not found in {tables_dir}")
            return pd.DataFrame(columns=["RunID", "EquipID", "FailureTime", "SensorName",
                                         "FailureContribution", "ZScoreAtFailure", "AlertCount", "Comment"])
        df = pd.read_csv(p)
        if "SensorName" not in df.columns or "LatestAbsZ" not in df.columns:
            return pd.DataFrame(columns=["RunID", "EquipID", "FailureTime", "SensorName",
                                         "FailureContribution", "ZScoreAtFailure", "AlertCount", "Comment"])
        df = df.rename(columns={"LatestAbsZ": "Z"})
        df["Z"] = df["Z"].astype(float).clip(lower=0.0)
        if df["Z"].sum() <= 0:
            return pd.DataFrame(columns=["RunID", "EquipID", "FailureTime", "SensorName",
                                         "FailureContribution", "ZScoreAtFailure", "AlertCount", "Comment"])
        df["FailureContribution"] = df["Z"] / df["Z"].sum()
        df = df.sort_values("FailureContribution", ascending=False).head(10)

    result = pd.DataFrame(
        {
            "FailureTime": failure_time,
            "SensorName": df["SensorName"],
            "FailureContribution": df["FailureContribution"],
            "ZScoreAtFailure": df["Z"],
            "AlertCount": df.get("AboveAlertCount", pd.Series([None] * len(df))),
            "Comment": [None] * len(df),
        }
    )
    if run_id:
        result.insert(0, "RunID", run_id)
    if equip_id is not None:
        result.insert(1 if run_id else 0, "EquipID", equip_id)
    return result


def _build_maintenance_recommendation(
    forecast_index: pd.DatetimeIndex,
    failure_prob: np.ndarray,
    cfg: RULConfig,
    run_id: str,
    equip_id: Optional[int],
) -> pd.DataFrame:
    """
    Construct a simple maintenance window recommendation from the failure
    probability time series.

    Heuristic:
    - EarliestMaintenance: first time failure probability exceeds maintenance_risk_low.
    - PreferredWindowStart: same as EarliestMaintenance.
    - PreferredWindowEnd: first time failure probability exceeds maintenance_risk_high
      (or the last forecast point if never exceeded).
    """
    if forecast_index.empty or failure_prob.size == 0:
        return pd.DataFrame(
            columns=[
                "RunID",
                "EquipID",
                "EarliestMaintenance",
                "PreferredWindowStart",
                "PreferredWindowEnd",
                "FailureProbAtWindowEnd",
                "Comment",
            ]
        )

    fp = np.asarray(failure_prob, dtype=float)
    times = forecast_index

    low_mask = fp >= float(cfg.maintenance_risk_low)
    high_mask = fp >= float(cfg.maintenance_risk_high)

    if low_mask.any():
        first_low_idx = int(np.argmax(low_mask))
    else:
        first_low_idx = fp.size - 1

    if high_mask.any():
        first_high_idx = int(np.argmax(high_mask))
    else:
        first_high_idx = fp.size - 1

    earliest_ts = times[first_low_idx]
    window_start = earliest_ts
    window_end = times[first_high_idx]
    prob_at_end = float(fp[first_high_idx])

    df = pd.DataFrame(
        [
            {
                "EarliestMaintenance": earliest_ts,
                "PreferredWindowStart": window_start,
                "PreferredWindowEnd": window_end,
                "FailureProbAtWindowEnd": prob_at_end,
                "Comment": None,
            }
        ]
    )
    if run_id:
        df.insert(0, "RunID", run_id)
    if equip_id is not None:
        df.insert(1 if run_id else 0, "EquipID", equip_id)
    return df
