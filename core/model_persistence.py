"""
Model Versioning & Persistence Module
=====================================
Manages trained model storage, versioning, and loading with cold-start resolution.

Architecture:
- SQL-ONLY MODE: All models stored in dbo.ModelRegistry table
- No filesystem artifacts - models serialized to VARBINARY(MAX) in SQL
- Config tracks active model version
- Auto-increments version on retraining
- Loads cached models on cold-start from SQL

Models persisted:
- AR1: fitted parameters (alpha, sigma)
- PCA: fitted model (sklearn.decomposition.PCA)
- IForest: fitted model (sklearn.ensemble.IsolationForest)
- GMM: fitted model (sklearn.mixture.GaussianMixture)
- Regimes: KMeans model (sklearn.cluster.KMeans)
- Scalers: StandardScaler for each detector
- Feature medians: Imputation values

Manifest structure:
{
    "version": 1,
    "created_at": "2025-10-27T16:04:37Z",
    "equip": "FD_FAN",
    "config_signature": "2ede5b7b43512a27",
    "train_rows": 10770,
    "train_sensors": ["sensor1", "sensor2", ...],
    "models": {
        "ar1": {"params": {...}, "quality": {...}},
        "pca": {"n_components": 5, "variance_ratio": 0.85, ...},
        "iforest": {"contamination": 0.001, "n_estimators": 100, ...},
        "gmm": {"n_components": 5, "bic": 12345.67, ...},
        "regimes": {"n_clusters": 4, "silhouette": 0.42, ...}
    },
    "feature_stats": {
        "n_features": 72,
        "imputation_medians": {...}
    }
}
"""

import joblib
import json
import os
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timezone
from io import BytesIO
from dataclasses import dataclass, asdict
import pandas as pd
import numpy as np
from utils.logger import Console


# ============================================================================
# Forecast State Persistence (FORECAST-STATE-01)
# ============================================================================

@dataclass
class ForecastState:
    """
    Persistent state for continuous forecasting between batches.
    Enables temporal continuity and conditional retraining.
    
    Attributes:
        equip_id: Equipment ID
        state_version: Incremental version number
        model_type: Forecasting model type (AR1, ARIMA, ETS)
        model_params: **DEPRECATED** - Kept for schema compatibility, always empty dict {}
        residual_variance: **DEPRECATED** - Kept for schema compatibility, always 0.0
        last_forecast_horizon_json: JSON string of last forecast DataFrame
        hazard_baseline: EWMA smoothed hazard rate for probability continuity
        last_retrain_time: Timestamp of last full retrain
        training_data_hash: Hash of training window for change detection
        training_window_hours: Length of training window in hours
        forecast_quality: Dict with rmse, mae, mape metrics
        
    Note:
        model_params and residual_variance are retained only for SQL schema compatibility
        with ACM_ForecastState table. They are not populated or used in runtime logic.
        Consider removing in future schema migration.
    """
    equip_id: int
    state_version: int
    model_type: str
    model_params: Dict[str, Any]  # DEPRECATED: always {}
    residual_variance: float  # DEPRECATED: always 0.0
    last_forecast_horizon_json: str  # JSON array of {Timestamp, ForecastHealth, CI_Lower, CI_Upper}
    hazard_baseline: float
    last_retrain_time: str  # ISO format datetime string
    training_data_hash: str
    training_window_hours: int
    forecast_quality: Dict[str, float]  # {rmse, mae, mape}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ForecastState":
        """Create ForecastState from dictionary."""
        return cls(**data)
    
    def get_last_forecast_horizon(self) -> pd.DataFrame:
        """Deserialize forecast horizon from JSON string."""
        try:
            import json
            horizon_data = json.loads(self.last_forecast_horizon_json)
            df = pd.DataFrame(horizon_data)
            if not df.empty and "Timestamp" in df.columns:
                df["Timestamp"] = pd.to_datetime(df["Timestamp"])
            return df
        except Exception as e:
            Console.warn(f"[FORECAST_STATE] Failed to deserialize forecast horizon: {e}")
            return pd.DataFrame()
    
    @staticmethod
    def serialize_forecast_horizon(df: pd.DataFrame) -> str:
        """Serialize forecast horizon DataFrame to JSON string."""
        try:
            import json
            if df.empty:
                return "[]"
            # Convert Timestamp to ISO string for JSON compatibility
            df_copy = df.copy()
            if "Timestamp" in df_copy.columns:
                df_copy["Timestamp"] = df_copy["Timestamp"].dt.strftime("%Y-%m-%dT%H:%M:%S")
            return json.dumps(df_copy.to_dict(orient="records"))
        except Exception as e:
            Console.warn(f"[FORECAST_STATE] Failed to serialize forecast horizon: {e}")
            return "[]"


def save_forecast_state(state: ForecastState, equip: str, sql_client) -> None:
    """
    Save ForecastState to SQL (SQL-ONLY MODE).
    
    Args:
        state: ForecastState object to persist
        equip: Equipment name (for logging)
        sql_client: SQL client for persistence
    """
    if sql_client is None:
        Console.error("[FORECAST_STATE] SQL client required for SQL-only mode")
        return
    
    try:
        cur = sql_client.cursor()
        
        # Convert dicts to JSON strings for SQL storage
        model_params_json = json.dumps(state.model_params)
        forecast_quality_json = json.dumps(state.forecast_quality)
        
        # Upsert into ACM_ForecastState
        cur.execute("""
            MERGE INTO dbo.ACM_ForecastState AS target
            USING (SELECT ? AS EquipID, ? AS StateVersion) AS source
            ON target.EquipID = source.EquipID AND target.StateVersion = source.StateVersion
            WHEN MATCHED THEN
                UPDATE SET
                    ModelType = ?,
                    ModelParamsJson = ?,
                    ResidualVariance = ?,
                    LastForecastHorizonJson = ?,
                    HazardBaseline = ?,
                    LastRetrainTime = ?,
                    TrainingDataHash = ?,
                    TrainingWindowHours = ?,
                    ForecastQualityJson = ?
            WHEN NOT MATCHED THEN
                INSERT (EquipID, StateVersion, ModelType, ModelParamsJson, ResidualVariance,
                        LastForecastHorizonJson, HazardBaseline, LastRetrainTime,
                        TrainingDataHash, TrainingWindowHours, ForecastQualityJson)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
        """, (
            state.equip_id, state.state_version,  # MERGE match
            state.model_type, model_params_json, state.residual_variance,
            state.last_forecast_horizon_json, state.hazard_baseline,
            state.last_retrain_time, state.training_data_hash,
            state.training_window_hours, forecast_quality_json,  # UPDATE values
            state.equip_id, state.state_version, state.model_type,
            model_params_json, state.residual_variance,
            state.last_forecast_horizon_json, state.hazard_baseline,
            state.last_retrain_time, state.training_data_hash,
            state.training_window_hours, forecast_quality_json  # INSERT values
        ))
        
        if not sql_client.conn.autocommit:
            sql_client.conn.commit()
        
        Console.info(f"[FORECAST_STATE] Saved state v{state.state_version} to ACM_ForecastState (EquipID={state.equip_id})")
    except Exception as e:
        Console.error(f"[FORECAST_STATE] Failed to save state to SQL: {e}")


def load_forecast_state(equip: str, equip_id: int, sql_client) -> Optional[ForecastState]:
    """
    Load latest ForecastState from SQL (SQL-ONLY MODE).
    
    Args:
        equip: Equipment name (for logging)
        equip_id: Equipment ID (required)
        sql_client: SQL client to load from database
    
    Returns:
        ForecastState object or None if not found
    """
    if sql_client is None:
        Console.error("[FORECAST_STATE] SQL client required for SQL-only mode")
        return None
    
    if equip_id is None:
        Console.error("[FORECAST_STATE] equip_id required for SQL-only mode")
        return None
    
    try:
        cur = sql_client.cursor()
        cur.execute("""
            SELECT TOP 1
                EquipID, StateVersion, ModelType, ModelParamsJson, ResidualVariance,
                LastForecastHorizonJson, HazardBaseline, LastRetrainTime,
                TrainingDataHash, TrainingWindowHours, ForecastQualityJson
            FROM dbo.ACM_ForecastState
            WHERE EquipID = ?
            ORDER BY StateVersion DESC
        """, (equip_id,))
        
        row = cur.fetchone()
        cur.close()
        
        if row:
            state = ForecastState(
                equip_id=row[0],
                state_version=row[1],
                model_type=row[2],
                model_params=json.loads(row[3]) if row[3] else {},
                residual_variance=float(row[4]) if row[4] is not None else 0.0,
                last_forecast_horizon_json=row[5] or "[]",
                hazard_baseline=float(row[6]) if row[6] is not None else 0.0,
                last_retrain_time=row[7].isoformat() if row[7] else datetime.now(timezone.utc).isoformat(),
                training_data_hash=row[8] or "",
                training_window_hours=int(row[9]) if row[9] is not None else 72,
                forecast_quality=json.loads(row[10]) if row[10] else {}
            )
            Console.info(f"[FORECAST_STATE] Loaded state v{state.state_version} from SQL (EquipID={equip_id})")
            return state
        else:
            Console.info(f"[FORECAST_STATE] No prior forecast state found for EquipID={equip_id}")
            return None
    except Exception as e:
        Console.error(f"[FORECAST_STATE] Failed to load state from SQL: {e}")
        return None


# ============================================================================
# Regime State Persistence (REGIME-STATE-01)
# ============================================================================

@dataclass
class RegimeState:
    """
    Persistent state for regime clustering between batches.
    Enables label continuity and conditional retraining.
    
    Attributes:
        equip_id: Equipment ID
        state_version: Incremental version number
        n_clusters: Number of regime clusters
        cluster_centers_json: JSON string of cluster centers (serialized numpy array)
        scaler_mean_json: JSON string of scaler mean values
        scaler_scale_json: JSON string of scaler scale values
        pca_components_json: JSON string of PCA components (if used)
        pca_explained_variance_json: JSON string of PCA explained variance ratios
        n_pca_components: Number of PCA components (0 if not used)
        silhouette_score: Clustering quality metric
        quality_ok: Boolean indicating if regime model passes quality checks
        last_trained_time: Timestamp of last training
        config_hash: Hash of regime config for change detection
        regime_basis_hash: Hash of regime basis features for change detection
    """
    equip_id: int
    state_version: int
    n_clusters: int
    cluster_centers_json: str
    scaler_mean_json: str
    scaler_scale_json: str
    pca_components_json: str
    pca_explained_variance_json: str
    n_pca_components: int
    silhouette_score: float
    quality_ok: bool
    last_trained_time: str  # ISO format datetime string
    config_hash: str
    regime_basis_hash: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "RegimeState":
        """Create RegimeState from dictionary."""
        return cls(**data)
    
    def get_cluster_centers(self) -> np.ndarray:
        """Deserialize cluster centers from JSON string."""
        try:
            centers = json.loads(self.cluster_centers_json)
            return np.array(centers)
        except Exception as e:
            Console.warn(f"[REGIME_STATE] Failed to deserialize cluster centers: {e}")
            return np.array([])
    
    def get_scaler_params(self) -> Tuple[np.ndarray, np.ndarray]:
        """Deserialize scaler mean and scale from JSON strings."""
        try:
            mean = np.array(json.loads(self.scaler_mean_json))
            scale = np.array(json.loads(self.scaler_scale_json))
            return mean, scale
        except Exception as e:
            Console.warn(f"[REGIME_STATE] Failed to deserialize scaler params: {e}")
            return np.array([]), np.array([])
    
    def get_pca_params(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """Deserialize PCA components and explained variance."""
        if self.n_pca_components == 0:
            return None, None
        try:
            components = np.array(json.loads(self.pca_components_json))
            explained_var = np.array(json.loads(self.pca_explained_variance_json))
            return components, explained_var
        except Exception as e:
            Console.warn(f"[REGIME_STATE] Failed to deserialize PCA params: {e}")
            return None, None
    
    @staticmethod
    def serialize_array(arr: np.ndarray) -> str:
        """Serialize numpy array to JSON string."""
        try:
            return json.dumps(arr.tolist())
        except Exception as e:
            Console.warn(f"[REGIME_STATE] Failed to serialize array: {e}")
            return "[]"


def save_regime_state(state: RegimeState, artifact_root: Path, equip: str, sql_client=None) -> None:
    """
    Save RegimeState to SQL (SQL-ONLY MODE).
    
    Args:
        state: RegimeState object to persist
        artifact_root: IGNORED - kept for API compatibility
        equip: Equipment name (for logging)
        sql_client: SQL client (REQUIRED)
    """
    if sql_client is None:
        Console.error("[REGIME_STATE] SQL client required for SQL-only mode")
        return
    
    try:
        cur = sql_client.cursor()
        
        # Upsert into ACM_RegimeState
        cur.execute("""
            MERGE INTO dbo.ACM_RegimeState AS target
            USING (SELECT ? AS EquipID, ? AS StateVersion) AS source
            ON target.EquipID = source.EquipID AND target.StateVersion = source.StateVersion
            WHEN MATCHED THEN
                UPDATE SET
                    NumClusters = ?,
                    ClusterCentersJson = ?,
                    ScalerMeanJson = ?,
                    ScalerScaleJson = ?,
                    PCAComponentsJson = ?,
                    PCAExplainedVarianceJson = ?,
                    NumPCAComponents = ?,
                    SilhouetteScore = ?,
                    QualityOk = ?,
                    LastTrainedTime = ?,
                    ConfigHash = ?,
                    RegimeBasisHash = ?
            WHEN NOT MATCHED THEN
                INSERT (EquipID, StateVersion, NumClusters, ClusterCentersJson,
                        ScalerMeanJson, ScalerScaleJson, PCAComponentsJson,
                        PCAExplainedVarianceJson, NumPCAComponents, SilhouetteScore,
                        QualityOk, LastTrainedTime, ConfigHash, RegimeBasisHash)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
        """, (
            state.equip_id, state.state_version,  # MERGE match
            state.n_clusters, state.cluster_centers_json,
            state.scaler_mean_json, state.scaler_scale_json,
            state.pca_components_json, state.pca_explained_variance_json,
            state.n_pca_components, state.silhouette_score, state.quality_ok,
            state.last_trained_time, state.config_hash, state.regime_basis_hash,  # UPDATE values
            state.equip_id, state.state_version, state.n_clusters,
            state.cluster_centers_json, state.scaler_mean_json,
            state.scaler_scale_json, state.pca_components_json,
            state.pca_explained_variance_json, state.n_pca_components,
            state.silhouette_score, state.quality_ok,
            state.last_trained_time, state.config_hash, state.regime_basis_hash  # INSERT values
        ))
        
        if not sql_client.conn.autocommit:
            sql_client.conn.commit()
        
        Console.info(f"[REGIME_STATE] Saved state v{state.state_version} to ACM_RegimeState (EquipID={state.equip_id})")
    except Exception as e:
        Console.warn(f"[REGIME_STATE] Failed to save state to SQL: {e}")


def load_regime_state(artifact_root: Path, equip: str, equip_id: Optional[int] = None, sql_client=None) -> Optional[RegimeState]:
    """
    Load latest RegimeState from SQL (SQL-ONLY MODE).
    
    Args:
        artifact_root: IGNORED - kept for API compatibility
        equip: Equipment name (for logging)
        equip_id: Equipment ID (REQUIRED)
        sql_client: SQL client (REQUIRED)
    
    Returns:
        RegimeState object or None if not found
    """
    if sql_client is None or equip_id is None:
        Console.error("[REGIME_STATE] SQL client and equip_id required for SQL-only mode")
        return None
    
    try:
        cur = sql_client.cursor()
        cur.execute("""
            SELECT TOP 1
                EquipID, StateVersion, NumClusters, ClusterCentersJson,
                ScalerMeanJson, ScalerScaleJson, PCAComponentsJson,
                PCAExplainedVarianceJson, NumPCAComponents, SilhouetteScore,
                QualityOk, LastTrainedTime, ConfigHash, RegimeBasisHash
            FROM dbo.ACM_RegimeState
            WHERE EquipID = ?
            ORDER BY StateVersion DESC
        """, (equip_id,))
        
        row = cur.fetchone()
        cur.close()
        
        if row:
            state = RegimeState(
                equip_id=row[0],
                state_version=row[1],
                n_clusters=int(row[2]) if row[2] is not None else 0,
                cluster_centers_json=row[3] or "[]",
                scaler_mean_json=row[4] or "[]",
                scaler_scale_json=row[5] or "[]",
                pca_components_json=row[6] or "[]",
                pca_explained_variance_json=row[7] or "[]",
                n_pca_components=int(row[8]) if row[8] is not None else 0,
                silhouette_score=float(row[9]) if row[9] is not None else 0.0,
                quality_ok=bool(row[10]) if row[10] is not None else False,
                last_trained_time=row[11].isoformat() if row[11] else datetime.now(timezone.utc).isoformat(),
                config_hash=row[12] or "",
                regime_basis_hash=row[13] or ""
            )
            Console.info(f"[REGIME_STATE] Loaded state v{state.state_version} from SQL (EquipID={equip_id})")
            return state
        else:
            Console.info(f"[REGIME_STATE] No existing state found in SQL for EquipID={equip_id}")
            return None
    except Exception as e:
        Console.warn(f"[REGIME_STATE] Failed to load state from SQL: {e}")
        return None


# ============================================================================
# Model Version Manager
# ============================================================================

class ModelVersionManager:
    """Manages model versioning, persistence, and loading (SQL-ONLY MODE)."""
    
    def __init__(self, equip: str, artifact_root: Path, sql_client=None, equip_id: Optional[int] = None):
        """
        Initialize model version manager (SQL-ONLY MODE).
        
        Args:
            equip: Equipment name (e.g., "FD_FAN")
            artifact_root: IGNORED - kept for API compatibility only
            sql_client: SQL client for model storage (REQUIRED)
            equip_id: Equipment ID for SQL operations (REQUIRED)
        """
        self.equip = equip
        self.sql_client = sql_client
        self.equip_id = equip_id
        
        if not sql_client or equip_id is None:
            Console.error("[MODEL] SQL client and equip_id required for SQL-only mode")
    
    def get_latest_version(self) -> Optional[int]:
        """Get the latest model version number from SQL ModelRegistry."""
        if not self.sql_client or self.equip_id is None:
            Console.warn("[MODEL] Cannot get latest version - SQL client/equip_id missing")
            return None
        
        return self._get_latest_version_from_sql()
    
    def get_next_version(self) -> int:
        """Get the next version number (latest + 1, or 1 if none exist)."""
        latest = self.get_latest_version()
        return 1 if latest is None else latest + 1
    
    def save_models(
        self,
        models: Dict[str, Any],
        metadata: Dict[str, Any],
        version: Optional[int] = None
    ) -> int:
        """
        Save trained models to SQL ModelRegistry (SQL-ONLY MODE).
        
        Args:
            models: Dictionary of model artifacts to save
            metadata: Model metadata
            version: Explicit version number, or None to auto-increment
        
        Returns:
            Version number used
        """
        if not self.sql_client or self.equip_id is None:
            Console.error("[MODEL] Cannot save models - SQL client/equip_id missing")
            raise ValueError("SQL client and equip_id required for model persistence")
        
        # Determine version
        if version is None:
            version = self.get_next_version()
        
        Console.info(f"[MODEL] Saving models to SQL ModelRegistry v{version}")
        
        # Create manifest
        manifest = {
            "version": version,
            "created_at": datetime.now().isoformat(),
            "equip": self.equip,
            "saved_models": list(models.keys()),
            **metadata
        }
        
        # Save to SQL
        try:
            self._save_models_to_sql(models, metadata, version)
            Console.info(f"[MODEL] Saved {len(models)} models to SQL ModelRegistry v{version}")
        except Exception as e:
            Console.error(f"[MODEL-SQL] Failed to save models to SQL: {e}")
            raise
        
        return version
    
    def _save_models_to_sql(self, models: Dict[str, Any], metadata: Dict[str, Any], version: int):
        """
        Save models to SQL ModelRegistry table with atomic transaction handling.
        
        SQL-20 Implementation:
        - Serializes all model types to binary using joblib
        - Stores comprehensive metadata as JSON
        - Uses atomic transactions (rollback on any failure)
        - Handles special model types (ar1_params, omr_model)
        - Note: mhal_params removed v9.1.0 (MHAL deprecated)
        """
        Console.info(f"[MODEL-SQL] Saving models to SQL ModelRegistry v{version}...")
        
        if not self.sql_client.conn:
            Console.warn("[MODEL-SQL] SQL connection not available")
            return
        
        cursor = self.sql_client.conn.cursor()
        saved_count = 0
        errors = []
        
        try:
            # Begin transaction - will rollback if any model fails
            # (SQL Server auto-starts transaction on first command)
            
            # Delete existing models for this version if they exist (replace strategy)
            delete_sql = "DELETE FROM ModelRegistry WHERE EquipID = ? AND Version = ?"
            cursor.execute(delete_sql, (self.equip_id, version))
            deleted_count = cursor.rowcount
            if deleted_count > 0:
                Console.info(f"[MODEL-SQL] Replaced {deleted_count} existing models for v{version}")
            
            for model_name, model_obj in models.items():
                if model_obj is None:
                    Console.debug(f"[MODEL-SQL]   - Skipping None model: {model_name}")
                    continue
                
                try:
                    # Serialize model to bytes using joblib
                    buffer = BytesIO()
                    joblib.dump(model_obj, buffer)
                    model_bytes = buffer.getvalue()
                    
                    # Extract model-specific metadata
                    model_meta = metadata.get("models", {}).get(model_name.replace("_params", "").replace("_model", ""), {})
                    params_json = json.dumps(model_meta) if model_meta else None
                    
                    # Get overall stats (include full metadata for reconstruction)
                    stats_meta = {
                        "train_rows": metadata.get("train_rows"),
                        "train_sensors": metadata.get("train_sensors"),
                        "config_signature": metadata.get("config_signature"),
                        "created_at": metadata.get("created_at"),
                        "training_duration_s": metadata.get("training_duration_s"),
                        "data_stats": metadata.get("data_stats"),
                        "feature_stats": metadata.get("feature_stats")
                    }
                    stats_json = json.dumps(stats_meta)
                    
                    # Insert into ModelRegistry
                    insert_sql = """
                    INSERT INTO ModelRegistry 
                    (ModelType, EquipID, Version, EntryDateTime, ParamsJSON, StatsJSON, ModelBytes, RunID)
                    VALUES (?, ?, ?, SYSUTCDATETIME(), ?, ?, ?, NULL)
                    """
                    
                    cursor.execute(insert_sql, (
                        model_name,
                        self.equip_id,
                        version,
                        params_json,
                        stats_json,
                        model_bytes
                    ))
                    
                    saved_count += 1
                    Console.info(f"[MODEL-SQL]   - Saved {model_name} ({len(model_bytes):,} bytes)")
                    
                except Exception as e:
                    error_msg = f"Failed to save {model_name}: {e}"
                    errors.append(error_msg)
                    Console.warn(f"[MODEL-SQL]   - {error_msg}")
                    # Don't break - try to save other models, but will rollback all if any fail
            
            # Commit transaction if all successful
            if errors:
                Console.warn(f"[MODEL-SQL] Rolling back transaction due to {len(errors)} error(s)")
                self.sql_client.conn.rollback()
                Console.warn(f"[MODEL-SQL] Transaction rolled back - no models saved")
            else:
                self.sql_client.conn.commit()
                Console.info(f"[MODEL-SQL] OK Committed {saved_count}/{len(models)} models to SQL ModelRegistry v{version}")
                
        except Exception as e:
            Console.warn(f"[MODEL-SQL] Critical error during save, rolling back: {e}")
            try:
                self.sql_client.conn.rollback()
            except Exception:
                pass
            raise

    def _get_latest_version_from_sql(self) -> Optional[int]:
        """Fetch the latest Version for this EquipID from ModelRegistry."""
        try:
            cur = self.sql_client.cursor()
            cur.execute(
                "SELECT MAX(Version) FROM ModelRegistry WHERE EquipID = ?",
                (self.equip_id,)
            )
            row = cur.fetchone()
            cur.close()
            if row and row[0] is not None:
                return int(row[0])
            return None
        except Exception as e:
            Console.warn(f"[MODEL] Failed to get latest version from SQL: {e}")
            return None
    
    def _load_models_from_sql(self, version: int) -> Optional[Tuple[Dict[str, Any], Dict[str, Any]]]:
        """
        Load models from SQL ModelRegistry table with metadata reconstruction.
        
        SQL-21 Implementation:
        - Retrieves all model types for equipment + version
        - Deserializes binary model data using joblib
        - Reconstructs manifest from StatsJSON and ParamsJSON
        - Returns (models_dict, manifest_dict) tuple
        
        Returns:
            Tuple of (models_dict, manifest_dict) or None if not found
        """
        Console.info(f"[MODEL-SQL] Loading models from SQL ModelRegistry v{version}...")
        
        if not self.sql_client or not self.sql_client.conn:
            Console.warn("[MODEL-SQL] SQL connection not available")
            return None
        
        try:
            cursor = self.sql_client.conn.cursor()
            
            # Query to get all models and their metadata
            sql = """
            SELECT ModelType, ModelBytes, ParamsJSON, StatsJSON, EntryDateTime
            FROM ModelRegistry 
            WHERE EquipID = ? AND Version = ?
            ORDER BY ModelType
            """
            
            cursor.execute(sql, (self.equip_id, version))
            rows = cursor.fetchall()
            
            if not rows:
                Console.info(f"[MODEL-SQL] No models found for EquipID={self.equip_id} Version={version}")
                return None
            
            models = {}
            all_params = {}
            all_stats = {}
            entry_datetime = None
            
            for row in rows:
                model_type = row[0]
                model_bytes = row[1]
                params_json = row[2]
                stats_json = row[3]
                entry_dt = row[4]
                
                if entry_datetime is None and entry_dt:
                    entry_datetime = entry_dt
                
                # Deserialize model bytes
                if model_bytes:
                    try:
                        buffer = BytesIO(model_bytes)
                        model_obj = joblib.load(buffer)
                        models[model_type] = model_obj
                        Console.info(f"[MODEL-SQL]   - Loaded {model_type} ({len(model_bytes):,} bytes)")
                    except Exception as e:
                        Console.warn(f"[MODEL-SQL]   - Failed to deserialize {model_type}: {e}")
                        continue
                
                # Collect metadata
                if params_json:
                    try:
                        params = json.loads(params_json)
                        # Store with normalized key (remove _params/_model suffix)
                        key = model_type.replace("_params", "").replace("_model", "")
                        all_params[key] = params
                    except Exception as e:
                        Console.warn(f"[MODEL-SQL]   - Failed to parse ParamsJSON for {model_type}: {e}")
                
                if stats_json:
                    try:
                        stats = json.loads(stats_json)
                        all_stats.update(stats)  # Merge stats from all models
                    except Exception as e:
                        Console.warn(f"[MODEL-SQL]   - Failed to parse StatsJSON for {model_type}: {e}")
            
            if not models:
                Console.warn(f"[MODEL-SQL] No models successfully deserialized")
                return None
            
            # Reconstruct manifest from SQL metadata
            manifest = {
                "version": version,
                "source": "sql",
                "equip": self.equip,
                "saved_models": list(models.keys()),
                "loaded_from_sql": True,
                "models": all_params,
                **all_stats  # Include train_rows, config_signature, created_at, etc.
            }
            
            if entry_datetime:
                manifest["entry_datetime"] = str(entry_datetime)
            
            Console.info(f"[MODEL-SQL] ✓ Loaded {len(models)}/{len(rows)} models from SQL ModelRegistry v{version}")
            return models, manifest
            
        except Exception as e:
            Console.warn(f"[MODEL-SQL] Failed to load models from SQL: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_models(
        self,
        version: Optional[int] = None,
        prefer_sql: bool = True
    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """
        Load models from a specific version (SQL first, then filesystem fallback).
        
        Args:
            version: Version to load, or None to load latest
            prefer_sql: If True and SQL client available, try SQL first
        
        Returns:
            Tuple of (models_dict, manifest_dict), or (None, None) if not found
        """
        # Determine version to load
        if version is None:
            version = self.get_latest_version()
            if version is None:
                Console.info("[MODEL] No cached models found - will train from scratch")
                return None, None
        
        # SQL-ONLY MODE: Load from SQL ModelRegistry only
        if not self.sql_client or self.equip_id is None:
            Console.warn("[MODEL] Cannot load models - SQL client/equip_id missing")
            return None, None
        
        result = self._load_models_from_sql(version)
        if result:
            sql_models, sql_manifest = result
            Console.info(f"[MODEL] ✓ Loaded from SQL ModelRegistry successfully")
            return sql_models, sql_manifest
        else:
            Console.warn(f"[MODEL] Failed to load models from SQL ModelRegistry")
            return None, None
    
    def update_models_incremental(
        self,
        models: Dict[str, Any],
        new_data: pd.DataFrame,
        version: Optional[int] = None
    ) -> Tuple[Dict[str, Any], int]:
        """
        Update models incrementally with new data using partial_fit where available.
        
        This method supports incremental learning for compatible models:
        - PCA: Not directly supported, but can be approximated with IncrementalPCA
        - IsolationForest: Not supported (batch-only)
        - GMM: Not supported (batch-only)
        - Scalers: Can be updated with partial_fit
        
        For unsupported models, this returns the original models unchanged.
        
        Args:
            models: Dictionary of current model artifacts
            new_data: New data batch for incremental update
            version: Version to save updated models, or None to auto-increment
        
        Returns:
            Tuple of (updated_models, version_number)
        """
        from sklearn.preprocessing import StandardScaler
        
        updated_models = models.copy()
        updated_count = 0
        
        Console.info(f"[MODEL] Updating models incrementally with {len(new_data)} new samples")
        
        # Update scalers if present and new data is available
        if "scalers" in models and models["scalers"] is not None and not new_data.empty:
            scalers = models["scalers"]
            updated_scalers = {}
            
            for detector_name, scaler in scalers.items():
                if isinstance(scaler, StandardScaler):
                    try:
                        # Extract relevant features for this scaler
                        # Assume scaler was fitted on all columns or subset
                        available_features = [col for col in new_data.columns if col in scaler.feature_names_in_]
                        
                        if available_features:
                            scaler_data = new_data[available_features].dropna()
                            if not scaler_data.empty:
                                scaler.partial_fit(scaler_data)
                                updated_scalers[detector_name] = scaler
                                updated_count += 1
                                Console.info(f"[MODEL]   - Updated scaler for {detector_name}")
                        else:
                            updated_scalers[detector_name] = scaler
                    except Exception as e:
                        Console.warn(f"[MODEL]   - Failed to update scaler for {detector_name}: {e}")
                        updated_scalers[detector_name] = scaler
                else:
                    updated_scalers[detector_name] = scaler
            
            updated_models["scalers"] = updated_scalers
        
        # Note: PCA, IsolationForest, GMM, and KMeans don't support incremental updates
        # They would require full retraining or replacement with incremental variants:
        # - IncrementalPCA for PCA
        # - MiniBatchKMeans for KMeans
        # - No incremental equivalent for IsolationForest or GMM
        
        if updated_count > 0:
            Console.info(f"[MODEL] Successfully updated {updated_count} model components")
            
            # Save updated models to new version
            metadata = {
                "update_type": "incremental",
                "updated_components": updated_count,
                "new_samples": len(new_data),
                "updated_at": datetime.now().isoformat()
            }
            
            if version is None:
                version = self.get_next_version()
            
            version = self.save_models(updated_models, metadata, version=version)
            
            return updated_models, version
        else:
            Console.info("[MODEL] No models support incremental updates, returning original models")
            return models, self.get_latest_version() or 1
    
    def check_model_validity(
        self,
        manifest: Dict[str, Any],
        current_config_signature: str,
        current_sensors: List[str]
    ) -> Tuple[bool, List[str]]:
        """
        Check if cached models are valid for current run.
        Task 5: Extended with temporal validation (model age threshold).
        
        Args:
            manifest: Loaded manifest dictionary
            current_config_signature: Current config signature
            current_sensors: Current sensor list
        
        Returns:
            Tuple of (is_valid, reasons_for_invalidity)
        """
        reasons = []
        
        # Check config signature
        cached_sig = manifest.get("config_signature", "")
        if cached_sig != current_config_signature:
            reasons.append(f"Config changed (cached: {cached_sig[:8]}, current: {current_config_signature[:8]})")
        
        # Check sensors
        cached_sensors = manifest.get("train_sensors", [])
        if set(cached_sensors) != set(current_sensors):
            reasons.append(f"Sensor list changed (cached: {len(cached_sensors)}, current: {len(current_sensors)})")
        
        # Task 5: Temporal validation - reject models exceeding max_model_age_days
        from datetime import datetime, timedelta
        created_at_str = manifest.get("created_at")
        if created_at_str:
            try:
                created_at = datetime.fromisoformat(created_at_str)
                age_days = (datetime.now() - created_at).total_seconds() / 86400
                max_age_days = manifest.get("max_model_age_days", 30)  # Default 30 days
                if age_days > max_age_days:
                    reasons.append(f"Model too old: {age_days:.1f}d > {max_age_days}d threshold")
            except Exception:
                pass
        
        is_valid = len(reasons) == 0
        return is_valid, reasons
    
    def list_versions(self) -> List[Dict[str, Any]]:
        """
        List all available model versions with metadata from SQL ModelRegistry.
        
        Returns:
            List of version metadata dicts
        """
        versions = []
        
        if not self.sql_client or self.equip_id is None:
            Console.warn("[MODEL] Cannot list versions - SQL client/equip_id missing")
            return versions
        
        try:
            cur = self.sql_client.cursor()
            cur.execute("""
                SELECT DISTINCT Version, MIN(EntryDateTime) AS FirstSaved, COUNT(*) AS ModelCount
                FROM ModelRegistry 
                WHERE EquipID = ?
                GROUP BY Version
                ORDER BY Version DESC
            """, (self.equip_id,))
            
            rows = cur.fetchall()
            cur.close()
            
            for row in rows:
                versions.append({
                    "version": int(row[0]),
                    "saved_at": str(row[1]) if row[1] else None,
                    "model_count": int(row[2]),
                    "source": "sql"
                })
            
            Console.info(f"[MODEL] Found {len(versions)} versions in SQL ModelRegistry")
        except Exception as e:
            Console.warn(f"[MODEL] Failed to list versions from SQL: {e}")
        
        return versions


def create_model_metadata(
    config_signature: str,
    train_data: pd.DataFrame,
    models_dict: Dict[str, Any],
    regime_quality: Optional[Dict[str, float]] = None,
    training_duration_s: Optional[float] = None
) -> Dict[str, Any]:
    """
    Create enhanced metadata dictionary for model manifest.
    Task 5: Extended with temporal fields (train_start, train_end, train_hash).
    
    Args:
        config_signature: Config signature hash
        train_data: Training dataframe (for stats)
        models_dict: Dictionary of trained models
        regime_quality: Optional regime quality metrics
        training_duration_s: Total training time in seconds
    
    Returns:
        Enhanced metadata dictionary with training duration, quality metrics, and data stats
    """
    import hashlib
    
    # Base metadata
    metadata = {
        "config_signature": config_signature,
        "train_rows": len(train_data),
        "train_sensors": train_data.columns.tolist() if hasattr(train_data, 'columns') else [],
        "models": {}
    }
    
    # Task 5: Add temporal fields for drift/age validation
    if hasattr(train_data, 'index') and len(train_data):
        try:
            metadata["train_start"] = str(train_data.index.min())
            metadata["train_end"] = str(train_data.index.max())
        except Exception:
            pass
    
    # Task 5: Add train_hash for data fingerprint validation
    if hasattr(train_data, 'values'):
        try:
            data_bytes = train_data.values.tobytes()
            metadata["train_hash"] = hashlib.sha256(data_bytes).hexdigest()[:16]
        except Exception:
            pass
    
    # Training duration
    if training_duration_s is not None:
        metadata["training_duration_s"] = round(training_duration_s, 2)
        metadata["training_duration_minutes"] = round(training_duration_s / 60, 2)
    
    # Data quality statistics
    if hasattr(train_data, 'values'):
        try:
            values = train_data.values
            metadata["data_stats"] = {
                "n_rows": len(train_data),
                "n_columns": len(train_data.columns) if hasattr(train_data, 'columns') else 0,
                "nan_count": int(np.isnan(values).sum()),
                "nan_percentage": round(float(np.isnan(values).sum() / values.size * 100), 2) if values.size > 0 else 0,
                "inf_count": int(np.isinf(values).sum()),
                "mean": round(float(np.nanmean(values)), 4),
                "std": round(float(np.nanstd(values)), 4),
                "min": round(float(np.nanmin(values)), 4),
                "max": round(float(np.nanmax(values)), 4)
            }
        except Exception as e:
            Console.warn(f"[META] Failed to compute data stats: {e}")
            metadata["data_stats"] = {"error": str(e)}
    
    # AR1 metadata
    if "ar1_params" in models_dict and models_dict["ar1_params"]:
        params = models_dict["ar1_params"]
        # ar1_params is {"phimap": {sensor: phi, ...}, "sdmap": {sensor: sd, ...}}
        phimap = params.get("phimap", {})
        sdmap = params.get("sdmap", {})
        metadata["models"]["ar1"] = {
            "n_sensors": len(phimap) if phimap else len(sdmap) if sdmap else 0,
            "mean_autocorr": round(float(np.mean(list(phimap.values()))), 4) if phimap else 0.0,
            "mean_residual_std": round(float(np.mean(list(sdmap.values()))), 4) if sdmap else 0.0,
            "params_count": len(phimap) + len(sdmap) if phimap or sdmap else 0
        }
    
    # PCA metadata with enhanced quality metrics
    if "pca_model" in models_dict and models_dict["pca_model"]:
        pca = models_dict["pca_model"]
        explained_var_ratio = pca.explained_variance_ratio_
        metadata["models"]["pca"] = {
            "n_components": pca.n_components_,
            "variance_ratio_sum": round(float(explained_var_ratio.sum()), 4),
            "variance_ratio_first_component": round(float(explained_var_ratio[0]), 4),
            "variance_ratio_top3": round(float(explained_var_ratio[:3].sum()), 4) if len(explained_var_ratio) >= 3 else round(float(explained_var_ratio.sum()), 4),
            "explained_variance": [round(float(v), 4) for v in pca.explained_variance_],
            "singular_values": [round(float(v), 4) for v in pca.singular_values_]
        }
    
    # IForest metadata
    if "iforest_model" in models_dict and models_dict["iforest_model"]:
        iforest = models_dict["iforest_model"]
        metadata["models"]["iforest"] = {
            "n_estimators": iforest.n_estimators,
            "contamination": float(iforest.contamination) if hasattr(iforest, 'contamination') else None,
            "max_features": iforest.max_features if hasattr(iforest, 'max_features') else None,
            "max_samples": iforest.max_samples if hasattr(iforest, 'max_samples') else None
        }
    
    # GMM metadata with BIC and AIC
    if "gmm_model" in models_dict and models_dict["gmm_model"]:
        gmm = models_dict["gmm_model"]
        gmm_meta = {
            "n_components": gmm.n_components,
            "covariance_type": gmm.covariance_type
        }
        
        # Compute BIC and AIC if possible
        if hasattr(train_data, 'values'):
            try:
                gmm_meta["bic"] = round(float(gmm.bic(train_data.values)), 2)
                gmm_meta["aic"] = round(float(gmm.aic(train_data.values)), 2)
                gmm_meta["lower_bound"] = round(float(gmm.lower_bound_), 2)
            except Exception as e:
                Console.warn(f"[META] Failed to compute GMM quality metrics: {e}")
        
        metadata["models"]["gmm"] = gmm_meta
    
    # Regime metadata with enhanced quality metrics
    if "regime_model" in models_dict and models_dict["regime_model"]:
        regime = models_dict["regime_model"]
        regime_meta = {
            "n_clusters": regime.n_clusters,
            "quality": regime_quality or {}
        }
        
        # Add silhouette scores if available in quality metrics
        if regime_quality:
            if "silhouette_score" in regime_quality:
                regime_meta["silhouette_score"] = round(float(regime_quality["silhouette_score"]), 4)
            if "calinski_harabasz_score" in regime_quality:
                regime_meta["calinski_harabasz_score"] = round(float(regime_quality["calinski_harabasz_score"]), 2)
            if "davies_bouldin_score" in regime_quality:
                regime_meta["davies_bouldin_score"] = round(float(regime_quality["davies_bouldin_score"]), 4)
        
        # Add inertia
        if hasattr(regime, 'inertia_'):
            regime_meta["inertia"] = round(float(regime.inertia_), 2)
        
        # Add iterations
        if hasattr(regime, 'n_iter_'):
            regime_meta["n_iterations"] = int(regime.n_iter_)
        
        metadata["models"]["regimes"] = regime_meta
    
    # Feature stats
    if "feature_medians" in models_dict and models_dict["feature_medians"] is not None:
        medians = models_dict["feature_medians"]
        metadata["feature_stats"] = {
            "n_features": len(medians) if hasattr(medians, '__len__') else 0
        }
        
        # Add median statistics if it's a pandas Series
        if hasattr(medians, 'values'):
            try:
                median_vals = medians.values
                metadata["feature_stats"]["imputation_median_mean"] = round(float(np.nanmean(median_vals)), 4)
                metadata["feature_stats"]["imputation_median_std"] = round(float(np.nanstd(median_vals)), 4)
                metadata["feature_stats"]["imputation_zero_count"] = int(np.sum(median_vals == 0))
            except Exception:
                pass
    
    return metadata


