"""
Autonomous Model Re-evaluation Module
=====================================
Monitors model quality and triggers retraining when performance degrades.

Quality Metrics Monitored:
1. Detector Saturation: % of z-scores hitting clip limits
2. Anomaly Rate: % of fused_z > threshold
3. Regime Quality: Silhouette score, stability metrics
4. Episode Validity: Duration, frequency, coverage
5. Sensor Coverage: % of sensors contributing to detections

Retraining Triggers:
- Saturation > 5% → Model underfitting, recalibrate
- Anomaly rate > 10% or < 0.01% → Miscalibration
- Regime silhouette < 0.15 → Poor clustering
- Episode coverage > 80% → Excessive false positives
- Config signature changed → Parameter update

Actions:
- Auto-retrains models when degradation detected
- Increments model version
- Logs reasoning to manifest
- Falls back to cached model if retraining fails
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Tuple, List
from pathlib import Path
from datetime import datetime, timezone
from core.observability import Console


class ModelQualityMonitor:
    """Monitors model performance and triggers retraining decisions."""
    
    def __init__(self, cfg: Dict[str, Any]):
        """
        Initialize quality monitor.
        
        Args:
            cfg: Configuration dictionary
        """
        self.cfg = cfg
        self.thresholds = {
            "max_saturation": 0.05,  # 5% saturation triggers retraining
            "max_anomaly_rate": 0.10,  # 10% anomaly rate (too many FPs)
            "min_anomaly_rate": 0.0001,  # 0.01% anomaly rate (too few detections)
            "min_silhouette": 0.15,  # Minimum acceptable regime quality
            "max_episode_coverage": 0.80,  # 80% of data in episodes is suspicious
            "max_episode_duration_days": 30,  # Episodes > 30 days are suspicious
        }
        
        # Override thresholds from config
        if "model_quality" in cfg:
            self.thresholds.update(cfg["model_quality"])
    
    def assess_detector_quality(
        self,
        scores: pd.DataFrame,
        detector_names: List[str],
        clip_z: float = 12.0
    ) -> Dict[str, Any]:
        """
        Assess detector quality by measuring saturation.
        
        Args:
            scores: DataFrame with detector z-scores
            detector_names: List of detector column names
            clip_z: Clipping limit used
        
        Returns:
            Dictionary with saturation metrics
        """
        saturation = {}
        
        for det_name in detector_names:
            z_col = f"{det_name}_z"
            if z_col not in scores.columns:
                continue
            
            z_vals = scores[z_col].values
            # Count values at clip limits
            saturated = (np.abs(z_vals) >= (clip_z * 0.95)).sum()  # 95% of clip limit
            total = len(z_vals)
            saturation_pct = (saturated / total) * 100 if total > 0 else 0.0
            
            saturation[det_name] = {
                "saturated_count": int(saturated),
                "total_count": int(total),
                "saturation_pct": float(saturation_pct)
            }
        
        # Overall saturation
        max_saturation_pct = max([s["saturation_pct"] for s in saturation.values()], default=0.0)
        
        return {
            "per_detector": saturation,
            "max_saturation_pct": max_saturation_pct,
            "is_acceptable": max_saturation_pct < (self.thresholds["max_saturation"] * 100)
        }
    
    def assess_anomaly_rate(
        self,
        scores: pd.DataFrame,
        threshold_col: str = "fused"
    ) -> Dict[str, Any]:
        """
        Assess anomaly detection rate.
        
        Args:
            scores: DataFrame with fused z-scores
            threshold_col: Column name with fused scores
        
        Returns:
            Dictionary with anomaly rate metrics
        """
        active_col = threshold_col
        if active_col not in scores.columns:
            # Backward compatibility: prior builds used "fused_z"
            fallback_col = "fused_z"
            if fallback_col in scores.columns:
                active_col = fallback_col
            else:
                return {
                    "anomaly_rate": 0.0,
                    "is_acceptable": False,
                    "reason": "Missing fused score column"
                }
        
        fused_series = pd.to_numeric(scores[active_col], errors="coerce")
        fused_values = fused_series.to_numpy(dtype=float, copy=False)
        valid_mask = ~np.isnan(fused_values)
        if not valid_mask.any():
            return {
                "anomaly_rate": 0.0,
                "is_acceptable": False,
                "reason": "Fused score column has no numeric values"
            }

        fused_z = fused_values[valid_mask]
        threshold = 1.0  # Assuming z > 1.0 is anomalous
        
        anomalies = (fused_z > threshold).sum()
        total = len(fused_z)
        anomaly_rate = (anomalies / total) if total > 0 else 0.0
        
        is_acceptable = (
            anomaly_rate >= self.thresholds["min_anomaly_rate"] and
            anomaly_rate <= self.thresholds["max_anomaly_rate"]
        )
        
        return {
            "anomaly_count": int(anomalies),
            "total_count": int(total),
            "anomaly_rate": float(anomaly_rate),
            "is_acceptable": is_acceptable,
            "reason": (
                "Anomaly rate too high" if anomaly_rate > self.thresholds["max_anomaly_rate"] else
                "Anomaly rate too low" if anomaly_rate < self.thresholds["min_anomaly_rate"] else
                "OK"
            )
        }
    
    def assess_regime_quality(
        self,
        regime_quality: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Assess regime clustering quality.
        
        Args:
            regime_quality: Regime quality metrics from regimes module
        
        Returns:
            Dictionary with regime quality assessment
        """
        silhouette = regime_quality.get("silhouette", 0.0)
        is_acceptable = silhouette >= self.thresholds["min_silhouette"]
        
        return {
            "silhouette": float(silhouette),
            "min_threshold": float(self.thresholds["min_silhouette"]),
            "is_acceptable": is_acceptable,
            "reason": "Silhouette score too low" if not is_acceptable else "OK"
        }
    
    def assess_episode_quality(
        self,
        episodes: pd.DataFrame,
        total_rows: int
    ) -> Dict[str, Any]:
        """
        Assess episode detection quality.
        
        Args:
            episodes: DataFrame with detected episodes
            total_rows: Total number of timesteps
        
        Returns:
            Dictionary with episode quality metrics
        """
        if episodes.empty:
            return {
                "episode_count": 0,
                "coverage": 0.0,
                "max_duration_days": 0.0,
                "is_acceptable": True,
                "reason": "No episodes detected"
            }
        
        # Calculate coverage
        episode_rows = episodes["duration"].sum() if "duration" in episodes.columns else 0
        coverage = (episode_rows / total_rows) if total_rows > 0 else 0.0
        
        # Calculate max duration (in days if timestamps available)
        if "start_dt" in episodes.columns and "end_dt" in episodes.columns:
            durations = (episodes["end_dt"] - episodes["start_dt"]).dt.total_seconds() / 86400
            max_duration_days = float(durations.max())
        else:
            max_duration_days = 0.0
        
        is_acceptable = (
            coverage <= self.thresholds["max_episode_coverage"] and
            max_duration_days <= self.thresholds["max_episode_duration_days"]
        )
        
        return {
            "episode_count": len(episodes),
            "coverage": float(coverage),
            "max_duration_days": float(max_duration_days),
            "is_acceptable": is_acceptable,
            "reason": (
                "Episode coverage too high" if coverage > self.thresholds["max_episode_coverage"] else
                f"Episode duration too long ({max_duration_days:.1f} days)" if max_duration_days > self.thresholds["max_episode_duration_days"] else
                "OK"
            )
        }
    
    def should_retrain(
        self,
        quality_metrics: Dict[str, Any],
        config_changed: bool = False
    ) -> Tuple[bool, List[str]]:
        """
        Decide if models should be retrained based on quality metrics.
        
        Args:
            quality_metrics: Aggregated quality metrics
            config_changed: Whether configuration signature changed
        
        Returns:
            Tuple of (should_retrain, reasons)
        """
        reasons = []
        
        # Check config change
        if config_changed:
            reasons.append("Configuration changed")
        
        # Check detector saturation
        detector_quality = quality_metrics.get("detector_quality", {})
        if not detector_quality.get("is_acceptable", True):
            reasons.append(
                f"Detector saturation too high: {detector_quality.get('max_saturation_pct', 0):.1f}%"
            )
        
        # Check anomaly rate
        anomaly_metrics = quality_metrics.get("anomaly_metrics", {})
        if not anomaly_metrics.get("is_acceptable", True):
            reasons.append(anomaly_metrics.get("reason", "Anomaly rate issue"))
        
        # Check regime quality
        regime_metrics = quality_metrics.get("regime_metrics", {})
        if not regime_metrics.get("is_acceptable", True):
            reasons.append(regime_metrics.get("reason", "Regime quality issue"))
        
        # Check episode quality
        episode_metrics = quality_metrics.get("episode_metrics", {})
        if not episode_metrics.get("is_acceptable", True):
            reasons.append(episode_metrics.get("reason", "Episode quality issue"))
        
        should_retrain = len(reasons) > 0
        
        return should_retrain, reasons
    
    def create_quality_report(
        self,
        quality_metrics: Dict[str, Any],
        should_retrain: bool,
        reasons: List[str]
    ) -> Dict[str, Any]:
        """
        Create comprehensive quality report for logging.
        
        Args:
            quality_metrics: All quality metrics
            should_retrain: Retraining decision
            reasons: Reasons for retraining
        
        Returns:
            Quality report dictionary
        """
        return {
            "timestamp": datetime.now().isoformat(),
            "should_retrain": should_retrain,
            "retraining_reasons": reasons,
            "metrics": quality_metrics,
            "thresholds": self.thresholds
        }


def assess_model_quality(
    scores: pd.DataFrame,
    episodes: pd.DataFrame,
    regime_quality: Dict[str, Any],
    cfg: Dict[str, Any],
    cached_manifest: Optional[Dict[str, Any]] = None
) -> Tuple[bool, List[str], Dict[str, Any]]:
    """
    Main entry point for model quality assessment.
    
    Args:
        scores: Scores dataframe with detector outputs
        episodes: Episodes dataframe
        regime_quality: Regime quality metrics
        cfg: Configuration dictionary
        cached_manifest: Cached model manifest (for config comparison)
    
    Returns:
        Tuple of (should_retrain, reasons, quality_report)
    """
    monitor = ModelQualityMonitor(cfg)
    
    # Assess detector quality
    detector_names = ["ar1", "pca_spe", "pca_t2", "mhal", "iforest", "gmm"]
    clip_z = cfg.get("thresholds", {}).get("self_tune", {}).get("clip_z", 12.0)
    detector_quality = monitor.assess_detector_quality(scores, detector_names, clip_z)
    
    # Assess anomaly rate
    anomaly_metrics = monitor.assess_anomaly_rate(scores)
    
    # Assess regime quality
    regime_metrics = monitor.assess_regime_quality(regime_quality)
    
    # Assess episode quality
    episode_metrics = monitor.assess_episode_quality(episodes, len(scores))
    
    # Check if config changed
    config_changed = False
    if cached_manifest:
        cached_sig = cached_manifest.get("config_signature", "")
        current_sig = cfg.get("_signature", "unknown")
        config_changed = (cached_sig != current_sig)
    
    # Aggregate metrics
    quality_metrics = {
        "detector_quality": detector_quality,
        "anomaly_metrics": anomaly_metrics,
        "regime_metrics": regime_metrics,
        "episode_metrics": episode_metrics
    }
    
    # Decide on retraining
    should_retrain, reasons = monitor.should_retrain(quality_metrics, config_changed)
    
    # Create report
    quality_report = monitor.create_quality_report(quality_metrics, should_retrain, reasons)
    
    return should_retrain, reasons, quality_report


def auto_tune_parameters(
    frame: pd.DataFrame,
    episodes: pd.DataFrame,
    score_out: Dict[str, Any],
    regime_quality_ok: bool,
    cfg: Dict[str, Any],
    sql_client: Any,
    run_id: Optional[str],
    equip_id: int,
    equip: str,
    output_manager: Optional[Any] = None,
    cached_manifest: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Perform autonomous parameter tuning based on model quality assessment.
    
    Evaluates model quality metrics (anomaly rate, drift score, regime quality) and
    proposes parameter adjustments. Records tuning actions to ACM_ConfigHistory and
    creates refit requests in ACM_RefitRequests when quality degrades.
    
    Args:
        frame: Scored DataFrame with detector z-scores and fused output
        episodes: Detected anomaly episodes
        score_out: Score output dict with silhouette score etc.
        regime_quality_ok: Whether regime clustering met quality threshold
        cfg: Configuration dictionary
        sql_client: SQL client for database writes
        run_id: Current run identifier
        equip_id: Equipment ID
        equip: Equipment name for logging
        output_manager: Optional OutputManager for refit request persistence
        cached_manifest: Optional cached model manifest for age checks
        
    v11.5.0: CRITICAL FIX - Refit Request Guard
    ============================================
    In ONLINE mode (score-only), do NOT write refit requests. These requests
    created a feedback loop during historical batch processing:
    
    1. Batch N scores data, metrics look poor (expected during calibration)
    2. auto_tune writes refit request
    3. Batch N+1 checks refit_request, triggers full refit
    4. Models change, thresholds shift
    5. Repeat - models never stabilize
    
    Correct behavior:
    - ONLINE mode: Score only, log quality metrics, but never trigger refit
    - OFFLINE mode: Can request refit if quality truly degraded
    - Scheduled refresh: Use separate mechanism (model age, scheduled jobs)
    """
    if not cfg.get("models", {}).get("auto_tune", True):
        return
    
    # v11.6.0 FIX #3: Skip refit evaluation entirely for CONVERGED models
    # CONVERGED models are stable and should NOT trigger refit requests.
    # This prevents 170+ spurious refit requests for stable equipment.
    model_maturity = cfg.get("runtime", {}).get("model_maturity_state", "LEARNING")
    if model_maturity == "CONVERGED":
        Console.info(
            "Auto-tune: Skipping refit evaluation - model is CONVERGED (stable)",
            component="AUTO-TUNE", equip=equip, maturity=model_maturity
        )
        return
    
    # v11.5.0: Check pipeline mode - do NOT write refit requests in ONLINE mode
    # Refit requests during historical batch processing cause infinite refit loops
    pipeline_mode = cfg.get("runtime", {}).get("pipeline_mode", "offline")
    allow_refit_requests = (pipeline_mode != "online")
    
    if not allow_refit_requests:
        Console.info(
            "Auto-tune: ONLINE mode - quality assessment only (no refit requests)",
            component="AUTO-TUNE", equip=equip, pipeline_mode=pipeline_mode
        )
    
    try:
        from core.config_history_writer import log_auto_tune_changes
        
        # Build regime quality metrics
        regime_quality_metrics = {
            "silhouette": score_out.get("silhouette", 0.0),
            "quality_ok": regime_quality_ok
        }
        
        # Perform full quality assessment
        should_retrain, reasons, quality_report = assess_model_quality(
            scores=frame,
            episodes=episodes,
            regime_quality=regime_quality_metrics,
            cfg=cfg,
            cached_manifest=cached_manifest
        )
        
        # Extract metrics for additional retrain triggers
        auto_retrain_cfg = cfg.get("models", {}).get("auto_retrain", {})
        if isinstance(auto_retrain_cfg, bool):
            auto_retrain_cfg = {}
        
        # Check anomaly rate trigger
        anomaly_rate_trigger = False
        anomaly_metrics = quality_report.get("metrics", {}).get("anomaly_metrics", {})
        current_anomaly_rate = anomaly_metrics.get("anomaly_rate", 0.0)
        max_anomaly_rate = auto_retrain_cfg.get("max_anomaly_rate", 0.25)
        if current_anomaly_rate > max_anomaly_rate:
            anomaly_rate_trigger = True
            if not should_retrain:
                reasons = []
            reasons.append(f"anomaly_rate={current_anomaly_rate:.2%} > {max_anomaly_rate:.2%}")
            Console.warn(f"Anomaly rate {current_anomaly_rate:.2%} exceeds threshold {max_anomaly_rate:.2%}", 
                        component="RETRAIN-TRIGGER", equip=equip, 
                        anomaly_rate=round(current_anomaly_rate, 4), threshold=max_anomaly_rate)
        
        # Check drift score trigger
        drift_score_trigger = False
        drift_score = quality_report.get("metrics", {}).get("drift_score", 0.0)
        max_drift_score = auto_retrain_cfg.get("max_drift_score", 2.0)
        if drift_score > max_drift_score:
            drift_score_trigger = True
            if not should_retrain and not anomaly_rate_trigger:
                reasons = []
            reasons.append(f"drift_score={drift_score:.2f} > {max_drift_score:.2f}")
        
        # Aggregate all retrain triggers
        needs_retraining = should_retrain or anomaly_rate_trigger or drift_score_trigger
        
        if not needs_retraining:
            return
        
        # Auto-tune parameters based on specific issues
        tuning_actions = []
        
        # Issue 1: High detector saturation - Increase clip_z
        detector_quality = quality_report.get("metrics", {}).get("detector_quality", {})
        if detector_quality.get("max_saturation_pct", 0) > 5.0:
            self_tune_cfg = cfg.get("thresholds", {}).get("self_tune", {})
            raw_clip_z = self_tune_cfg.get("clip_z", 12.0)
            try:
                current_clip_z = float(raw_clip_z)
            except (TypeError, ValueError):
                current_clip_z = 12.0
            
            clip_caps = [
                self_tune_cfg.get("max_clip_z"),
                cfg.get("model_quality", {}).get("max_clip_z"),
                50.0,
            ]
            clip_cap = max((float(c) for c in clip_caps if c is not None), default=50.0)
            clip_cap = max(clip_cap, current_clip_z, 20.0)
            
            proposed_clip = round(current_clip_z * 1.2, 2)
            if proposed_clip <= current_clip_z + 0.05:
                proposed_clip = current_clip_z + 2.0
            new_clip_z = min(proposed_clip, clip_cap)
            
            if new_clip_z > current_clip_z + 0.05:
                tuning_actions.append(f"clip_z: {current_clip_z:.2f}->{new_clip_z:.2f}")
        
        # Issue 2: High anomaly rate - Increase k_sigma
        if anomaly_metrics.get("anomaly_rate", 0) > 0.10:
            raw_k_sigma = cfg.get("episodes", {}).get("cpd", {}).get("k_sigma", 2.0)
            try:
                current_k = float(raw_k_sigma)
            except (TypeError, ValueError):
                current_k = 2.0
            new_k = min(round(current_k * 1.1, 3), 4.0)
            if new_k > current_k + 0.05:
                tuning_actions.append(f"k_sigma: {current_k:.3f}->{new_k:.3f}")
        
        # Issue 3: Low regime quality - Increase k_max
        regime_metrics = quality_report.get("metrics", {}).get("regime_metrics", {})
        if regime_metrics.get("silhouette", 1.0) < 0.15:
            auto_k_cfg = cfg.get("regimes", {}).get("auto_k", {})
            raw_k_max = auto_k_cfg.get("k_max", cfg.get("regimes", {}).get("k_max", 8))
            try:
                current_k_max = int(raw_k_max)
            except (TypeError, ValueError):
                current_k_max = 8
            new_k_max = min(current_k_max + 2, 12)
            if new_k_max > current_k_max:
                tuning_actions.append(f"k_max: {current_k_max}->{int(new_k_max)}")
        
        if tuning_actions:
            # Log config changes to ACM_ConfigHistory
            refit_triggered = False
            try:
                if sql_client and run_id:
                    # v11.5.0: Only trigger refit if allowed by pipeline mode
                    trigger_refit_on_tune = auto_retrain_cfg.get("on_tuning_change", False) and allow_refit_requests
                    log_auto_tune_changes(
                        sql_client=sql_client,
                        equip_id=int(equip_id),
                        tuning_actions=tuning_actions,
                        run_id=run_id,
                        trigger_refit=trigger_refit_on_tune
                    )
                    refit_triggered = trigger_refit_on_tune
            except Exception as log_err:
                Console.warn(f"Failed to log auto-tune changes: {log_err}", component="CONFIG_HIST",
                            equip=equip, error=str(log_err)[:200])
            
            # Consolidated auto-tune log
            mode_note = " (ONLINE - refit blocked)" if not allow_refit_requests else ""
            Console.info(f"Auto-tune: {len(tuning_actions)} adjustments ({', '.join(tuning_actions)}) | refit={'triggered' if refit_triggered else 'next_run'}{mode_note}", component="AUTO-TUNE")
        
        # v11.5.0: Only persist refit request if pipeline mode allows it
        # In ONLINE mode, we assess quality but do NOT request refit to prevent loops
        if output_manager and allow_refit_requests and needs_retraining:
            output_manager.write_refit_request(
                reasons=reasons,
                anomaly_rate=current_anomaly_rate if anomaly_rate_trigger else None,
                drift_score=drift_score if drift_score_trigger else None,
                regime_quality=regime_metrics.get("silhouette", 0.0),
            )
    
    except Exception as e:
        Console.warn(f"Autonomous tuning failed: {e}", component="AUTO-TUNE",
                    equip=equip, error_type=type(e).__name__, error=str(e)[:200])
