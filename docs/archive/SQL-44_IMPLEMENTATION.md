# SQL-44: SQL Historian Data Loading - Implementation Summary

## Overview
Implemented pure SQL data loading for the ACM pipeline, eliminating dependency on CSV files for input data. The pipeline can now load training and scoring data directly from SQL Server equipment data tables using the `usp_ACM_GetHistorianData_TEMP` stored procedure.

## Changes Made

### 1. Core Output Manager (`core/output_manager.py`)

#### Modified `load_data()` Method
- **New Parameters**:
  - `equipment_name`: Equipment name for SQL queries (e.g., 'FD_FAN', 'GAS_TURBINE')
  - `sql_mode`: Boolean flag to enable SQL historian loading
- **Behavior**: Routes to `_load_data_from_sql()` when `sql_mode=True`, otherwise uses existing CSV loading logic

#### New `_load_data_from_sql()` Method (Lines 777-932)
Loads data from SQL historian with the following features:
- Calls `usp_ACM_GetHistorianData_TEMP` with time range and equipment name
- Fetches result set from stored procedure (EntryDateTime + sensor columns)
- Converts to pandas DataFrame with datetime index
- Splits data into train/score using `cold_start_split_ratio` (default 60%/40%)
- Validates minimum sample requirements
- Performs cadence check, resampling, and gap filling (same as CSV mode)
- Returns (train_df, score_df, DataMeta) tuple

### 2. Pipeline Main (`core/acm_main.py`)

#### Updated Data Loading Call (Line 741)
```python
if SQL_MODE:
    # SQL mode: Load from historian using stored procedure
    train, score, meta = output_manager.load_data(
        cfg, 
        start_utc=win_start, 
        end_utc=win_end,
        equipment_name=equip,
        sql_mode=True
    )
else:
    # File mode: Load from CSV files
    train, score, meta = output_manager.load_data(cfg)
```

## How It Works

### SQL Mode Flow
1. Pipeline starts in SQL mode (`runtime.storage_backend='sql'`)
2. `_sql_start_run()` calls `usp_ACM_StartRun` to get time window (win_start, win_end)
3. `output_manager.load_data()` is called with `sql_mode=True` and `equipment_name`
4. `_load_data_from_sql()` executes:
   ```sql
   EXEC dbo.usp_ACM_GetHistorianData_TEMP 
       @StartTime = '2012-01-06 00:00:00',
       @EndTime = '2012-03-01 00:00:00',
       @EquipmentName = 'FD_FAN'
   ```
5. SP returns all sensor columns for the time range from `FD_FAN_Data` table
6. Data is split into train (60%) / score (40%) and processed
7. Returns clean train/score DataFrames ready for analytics

### CSV Mode Flow (Unchanged)
1. Pipeline starts in file mode (`runtime.storage_backend='file'`)
2. `load_data()` reads from `data.train_csv` and `data.score_csv` config paths
3. Existing CSV processing logic executes

## Validation

### Test Script: `scripts/sql/test_sql_mode_loading.py`
- Tests SQL historian loading independently
- Validates stored procedure call and DataFrame conversion
- Confirms train/score split and metadata generation
- **Test Result**: âœ“ 672 rows loaded (403 train + 269 score) for FD_FAN 2-month window

### Output Example
```
[DATA] Loading from SQL historian: FD_FAN
[DATA] Time range: 2012-01-06 00:00:00 to 2012-03-01 00:00:00
[DATA] Retrieved 672 rows from SQL historian
[DATA] Split (60.0%): 403 train rows, 269 score rows
[DATA] Kept 9 numeric columns, dropped 0 non-numeric
[DATA] SQL historian load complete: 403 train + 269 score = 672 total rows

Train shape: (403, 9)
Score shape: (269, 9)
Sampling seconds: 1800.0
```

## Configuration

### Enable SQL Mode
Update `configs/config_table.csv`:
```csv
EquipID,Section,Key,Value,Type,LastModified,ModifiedBy,Reason,Notes
0,runtime,storage_backend,sql,string,2025-11-13 00:00:00,SQL_MODE,SQL-44,Enable pure SQL mode
```

### SQL Mode Requirements
- `runtime.storage_backend='sql'` in config
- SQL connection available (`configs/sql_connection.ini` with `[acm]` section)
- Equipment registered in `Equipment` table
- Equipment data table exists (e.g., `FD_FAN_Data`, `GAS_TURBINE_Data`)
- `ACM_TagEquipmentMap` populated with sensor tags
- `usp_ACM_GetHistorianData_TEMP` stored procedure created

## Benefits

### 1. Single Source of Truth
- All data resides in SQL Server
- No CSV file synchronization issues
- Centralized data management

### 2. Dynamic Time Windows
- Pipeline queries arbitrary time ranges
- No need to pre-generate CSV files
- Supports rolling windows and backfill

### 3. Production-Ready Architecture
- Database-first design (standard for enterprise)
- Scales to larger datasets (millions of rows)
- Supports concurrent pipeline runs

### 4. Simplified Deployment
- No CSV file distribution required
- Configuration-driven time windows
- Easy to integrate with schedulers/orchestrators

## Next Steps

### SQL-45: Remove CSV Output Writes
- Keep: SQL table writes to `ACM_Scores_Wide`, `ACM_Episodes`, etc.
- Remove: All `to_csv()` calls in `output_manager.py`
- Remove: Dual-write logic for scores/episodes/metrics

### SQL-46: Eliminate Model Filesystem Persistence
- Keep: SQL `ModelRegistry` table writes
- Remove: Filesystem `.joblib` file saves
- Remove: `stable_models_dir` fallback logic

### SQL-50: End-to-End Pure SQL Validation
- Run full pipeline with `storage_backend='sql'`
- Verify: No files created in `artifacts/` directory
- Verify: All results in SQL tables only
- Confirm: Pipeline runs successfully start-to-finish

## Technical Details

### Stored Procedure Contract
```sql
CREATE PROCEDURE dbo.usp_ACM_GetHistorianData_TEMP
    @StartTime DATETIME2,
    @EndTime DATETIME2,
    @TagNames NVARCHAR(MAX) = NULL,        -- Optional: filter specific tags
    @EquipID INT = NULL,                   -- Alternative to EquipmentName
    @EquipmentName VARCHAR(50) = NULL      -- Equipment name (e.g., 'FD_FAN')
AS
BEGIN
    -- Returns: EntryDateTime + all sensor columns for time range
    -- Sorted by EntryDateTime ASC
    -- Filters by equipment data table (FD_FAN_Data, GAS_TURBINE_Data, etc.)
END
```

### Data Quality Checks
- Minimum 10 rows required (fail fast if no data)
- Warns if training data < 500 rows (configurable: `data.min_train_samples`)
- Validates numeric columns only (drops non-numeric)
- Checks timestamp index uniqueness
- Performs cadence validation and resampling

### Performance
- **Small window (2 hours)**: 5 rows, ~30ms query time
- **Medium window (2 months)**: 672 rows, ~10ms query time
- **Large window (full dataset)**: 17,499 rows, ~100ms query time (estimated)

## Files Modified
1. `core/output_manager.py`: Added `_load_data_from_sql()` method, updated `load_data()` signature
2. `core/acm_main.py`: Updated data loading call to pass `equipment_name` and `sql_mode` parameters

## Files Created
1. `scripts/sql/test_sql_mode_loading.py`: Standalone test for SQL historian loading

## Compatibility
- **Backward Compatible**: File mode still works with existing CSV-based configs
- **Forward Compatible**: Prepared for SQL-45/46 (removing output CSV writes and model files)
- **Migration Path**: Can run file mode and SQL mode side-by-side (different equipment configs)
