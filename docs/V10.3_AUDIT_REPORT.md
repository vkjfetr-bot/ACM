# ACM v10.3 Audit Report & v10.4 Quick Fix Plan

**Date**: 2025-12-23  
**Auditor**: GitHub Copilot  
**Question**: Is v10.3 completely unusable? What's the least effort method to make ACM work better?

---

## Executive Answer

### Is v10.3 Completely Unusable?

**NO** - v10.3 is **NOT completely unusable**.

**What Works:**
- ✅ All 6 core detectors work correctly (AR1, PCA-SPE, PCA-T², IForest, GMM, OMR)
- ✅ Fusion logic combines detector outputs properly
- ✅ Health tracking calculates health index correctly
- ✅ Episode detection identifies anomalous events
- ✅ Observability stack (Grafana, Tempo, Loki, Prometheus, Pyroscope) fully functional
- ✅ SQL persistence layer writes all analytics tables
- ✅ Batch processing runs without crashing

**What's Broken:**
1. **Forced regime assignment** - Cannot say "UNKNOWN", always forces nearest
2. **Unreliable RUL predictions** - Writes numbers even when prerequisites fail
3. **No data validation** - Bad data corrupts analytics downstream
4. **Train-score leakage** - Current batch can influence its own scores
5. **Point-anomaly alerts** - Noise instead of persistent episodes
6. **Forecasting duplication** - Logic scattered across multiple modules

### Assessment Grade: **B- (Usable but Unstable)**

v10.3 will run and produce outputs, but the outputs may be misleading in edge cases (sparse data, regime transitions, unstable equipment).

---

## Least Effort Method to Fix ACM

### Quick Fix: v10.4.0 (Tactical Improvements)

**Strategy**: "Amputate, don't fix" - Remove broken features, add safety gates

**Total Effort**: 11-14 hours (3 already done)

**Completed** (3 hours):
1. ✅ **UNKNOWN Regime Support** - System can now say "I don't know"
2. ✅ **RUL Reliability Gating** - Prevents unreliable predictions
3. ✅ **Data Contract Validation** - Rejects corrupt data at entry

**Remaining** (8-11 hours):
4. ⏳ Wire data contract into pipeline (1 hour)
5. ⏳ Disable point-anomaly alerts (1 hour)
6. ⏳ Fix detector train-score separation (3-4 hours)
7. ⏳ Consolidate forecasting logic (1 hour)
8. ⏳ Testing & validation (2 hours)

### Impact

**Before v10.4 (v10.3 behavior):**
- Equipment in unknown regime → Forced to nearest (wrong)
- Sparse data → RUL prediction (unreliable)
- Duplicate timestamps → Corrupt detector training (silent failure)
- Single anomaly spike → Alert (noise)

**After v10.4 (fixed behavior):**
- Equipment in unknown regime → REGIME_UNKNOWN (-1) (honest)
- Sparse data → RUL status = NOT_RELIABLE (safe)
- Duplicate timestamps → Contract violation, batch rejected (fail fast)
- Single anomaly spike → No alert (reduced noise)

---

## Detailed Findings

### What's Working (The Good)

**Analytics Backbone** (No changes needed):
- Feature engineering (`fast_features.py`) - Solid
- Detector implementations (AR1, PCA, IForest, GMM, OMR) - Correct
- Fusion logic (`fuse.py`) - Working
- Health calculation - Accurate
- Episode detection - Functional
- SQL persistence - Reliable

**Observability Stack** (v10.3.0 achievement):
- Complete Docker-based stack
- OpenTelemetry tracing to Tempo
- Structured logging to Loki
- Metrics to Prometheus
- Profiling with Pyroscope
- Unified Console API
- This is excellent and should be preserved

### What's Broken (The Bad)

#### 1. Forced Regime Assignment (Violates v11 Rule #4)

**Problem**:
```python
# core/regimes.py line 749 (v10.3)
labels = pairwise_distances_argmin(X_scaled, centers, axis=1)
# ALWAYS assigns to nearest cluster, even with 0.01 confidence
```

**Consequence**: Equipment in transitional states gets wrong regime → wrong thresholds → false alarms

**v10.4 Fix**:
```python
# v10.4 adds confidence check
confidence = 1.0 / (1.0 + normalized_distance)
if confidence < min_confidence:
    labels[i] = REGIME_UNKNOWN  # Honest answer
```

#### 2. Unreliable RUL Predictions (Violates v11 Rule #7)

**Problem**:
```python
# core/rul_estimator.py (v10.3)
def estimate_rul(...):
    # No prerequisite checks
    # Always returns numeric RUL, even on:
    # - Sparse data (5 samples)
    # - Unknown regime
    # - Improving health (negative degradation)
```

**Consequence**: Operators see "RUL = 12 hours" on perfectly healthy equipment

**v10.4 Fix**:
```python
# v10.4 adds prerequisite gating
status, reason = self.check_prerequisites(
    data_quality, regime_confidence, degradation_trend, drift
)
if status != RELIABLE:
    return RULEstimate(status=NOT_RELIABLE, p50=NaN, ...)
```

#### 3. No Data Contract (Violates v11 Requirement #9)

**Problem**:
- No validation at pipeline entry
- Duplicate timestamps → corrupt model training
- Future timestamps → nonsensical predictions
- Backward time jumps → regime detection fails

**Consequence**: Garbage in, garbage out - silently

**v10.4 Fix**:
```python
# v10.4 adds entry-point validation
contract = DataContract(...)
violations = contract.validate(df)
if violations:
    raise ContractViolation("Bad data rejected")
```

#### 4. Train-Score Leakage (Violates v11 Rule #8)

**Problem**:
- AR1: Fits on full window including current batch
- Current batch influences its own anomaly score
- Creates positive feedback loop

**Consequence**: Anomaly scores are inflated/deflated incorrectly

**v10.4 Fix**:
- Audit each detector
- Ensure training data != scoring data
- Add explicit guards

#### 5. Point-Anomaly Alerts (Violates v11 Rule #7)

**Problem**:
- Single spikes trigger alerts
- Not episode-based

**Consequence**: Alert fatigue, false alarms

**v10.4 Fix**:
- Add config flag `episodes_only_alerts = true`
- Disable point-alert code path

#### 6. Forecasting Duplication

**Problem**:
- Logic in `forecasting_legacy.py` AND `forecast_engine.py`
- Duplicate SQL writes
- Hard to maintain

**Consequence**: Confusion, maintenance burden

**v10.4 Fix**:
- Mark legacy modules as deprecated
- Route all forecasting through `ForecastEngine`

---

## How to Solve ACM Quickly

### Option 1: Finish v10.4 (Recommended)

**Time**: 8-11 hours remaining work  
**Risk**: Low  
**Benefit**: Stable foundation for v11

**Approach**:
1. Complete remaining phases (4-8)
2. Test with real data
3. Deploy v10.4.0
4. Gather feedback
5. Plan v11 execution

### Option 2: Minimal v10.4 (Fastest)

**Time**: 3 hours (just wire phases 1-3)  
**Risk**: Medium  
**Benefit**: Quick stability improvement

**Approach**:
1. Wire data contract into `acm_main.py` (1 hour)
2. Add config flags for UNKNOWN regime (30 min)
3. Update RUL calls to pass prerequisites (30 min)
4. Deploy with flags disabled by default (30 min)
5. Enable gradually per equipment (30 min)

### Option 3: Jump to v11 (Not Recommended)

**Time**: 100+ hours (50 items × 2 hours avg)  
**Risk**: Very High  
**Benefit**: Full architectural fix

**Why not recommended**:
- v10.3 is unstable - needs stable base first
- v11 is complex - requires careful planning
- High risk of introducing new bugs
- Better to stabilize with v10.4, then tackle v11

---

## Recommendation

**Execute v10.4.0 fully** (11-14 hours total):

**Rationale**:
1. v10.3 is usable but unreliable in edge cases
2. v10.4 fixes critical issues with minimal changes
3. Creates stable foundation for v11
4. Low risk, high value
5. Already 21% complete (3/14 hours)

**Timeline**:
- **This week**: Complete phases 4-5 (2 hours)
- **Next week**: Complete phases 6-7 (4-5 hours)
- **Following week**: Testing & deployment (2 hours)
- **Total**: 2-3 weeks of focused work

**Deployment Strategy**:
1. Deploy v10.4 with all new features **disabled** by default
2. Enable per equipment gradually
3. Monitor for issues
4. Fully enable after 1 week of validation

---

## What Happens After v10.4?

### Short-term (1-2 months)
- Gather operator feedback
- Tune confidence/reliability thresholds
- Monitor data contract rejection rates
- Document edge cases

### Medium-term (3-6 months)
- Plan v11 execution (50+ items)
- Create detailed implementation tracker
- Assign tasks to milestones
- Set up v11 feature branch

### Long-term (6-12 months)
- Execute v11 refactor in phases
- Comprehensive testing between phases
- Gradual deployment
- Full production rollout

---

## Conclusion

**Is v10.3 completely unusable?** No - it's **B- grade (usable but unstable)**.

**Least effort fix?** **v10.4.0 tactical improvements (11-14 hours)**.

**Already done**: 3 hours (UNKNOWN regime, RUL gating, data contracts)  
**Remaining**: 8-11 hours (wire contracts, fix leakage, test)

**Key insight**: v10.3 analytics are solid, but **missing safety gates**. v10.4 adds those gates without complex refactoring.

**Recommended path**:
1. Finish v10.4.0 (8-11 hours)
2. Deploy with features disabled
3. Enable gradually per equipment
4. Gather feedback for 1-2 months
5. Plan v11 execution properly
6. Execute v11 over 6-12 months

This creates a **stable, honest, reliable ACM** that operators can trust, while building toward the full v11 vision.
