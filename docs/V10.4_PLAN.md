# ACM v10.4.0 - Tactical Stability Plan

**Created**: 2025-12-23  
**Target Version**: 10.4.0  
**Status**: In Progress

---

## Executive Summary

v10.3.0 is **NOT completely broken** - the core analytics (detectors, fusion, health tracking) work correctly. However, several critical issues violate the v11 architectural principles and cause instability:

### Critical Issues Found

1. **Forced Regime Assignment** (Violates v11 Rule #4)
   - System forces nearest-regime assignment even when confidence is low
   - Cannot express "UNKNOWN" or "EMERGING" regimes
   - Result: False confidence in operating context

2. **Train-Score Leakage** (Violates v11 Rule #8)
   - Current batch can influence its own anomaly scores
   - No strict separation between training and scoring data
   - Result: Inflated or deflated anomaly detection

3. **Point-Anomaly Alerts** (Violates v11 Rule #7)
   - Alerts triggered by individual point anomalies
   - Episodes are not the only alerting primitive
   - Result: Alert fatigue, false alarms

4. **No Data Contract** (Missing v11 Requirement #9)
   - No validation at pipeline entry
   - Bad data (duplicates, future timestamps, disorder) corrupts analytics
   - Result: Garbage in, garbage out

5. **Unreliable RUL Outputs** (Violates v11 Rule #7)
   - System writes numeric RUL even when prerequisites fail
   - No "RUL_NOT_RELIABLE" status
   - Result: Misleading predictions

6. **Forecasting Duplication**
   - Logic scattered across multiple modules
   - Duplicate SQL writes
   - Result: Maintenance burden, inconsistency risk

---

## v10.4.0 Strategy

**Philosophy**: "Amputate, don't fix"

Instead of attempting complex refactoring, we'll make **minimal surgical changes** to:
1. Remove broken features (forced assignment, point alerts)
2. Add safety gates (data contracts, RUL reliability checks)
3. Consolidate duplicated logic (forecasting)
4. Document what's deferred to v11

This creates a **stable foundation** for the full v11 refactor.

---

## Implementation Plan

### Phase 1: UNKNOWN Regime Support (Fix Forced Assignment)

**Goal**: Allow regime detection to say "I don't know" instead of forcing assignment

**Files**:
- `core/regimes.py`

**Changes**:
1. Add `UNKNOWN = -1` and `EMERGING = -2` to regime labels
2. Add confidence threshold for regime assignment (default: 0.5)
3. When confidence < threshold, assign UNKNOWN instead of nearest
4. Update `ACM_RegimeTimeline` writes to include confidence
5. Update downstream consumers to handle UNKNOWN gracefully

**Testing**:
- Unit test: regime assignment with low-confidence data
- Integration test: batch run with sparse data produces UNKNOWN regimes

**Success Criteria**:
- Regime detection returns -1 for low-confidence assignments
- No forced nearest-regime when confidence is low
- Grafana dashboards show "UNKNOWN" label

---

### Phase 2: RUL Reliability Gating (Prevent Bad Predictions)

**Goal**: Add explicit "NOT_RELIABLE" status to RUL predictions

**Files**:
- `core/rul_estimator.py`
- `core/forecast_engine.py`
- `core/output_manager.py`

**Changes**:
1. Add `RULStatus` enum: `RELIABLE`, `NOT_RELIABLE`, `INSUFFICIENT_DATA`
2. Implement prerequisite checks in `RULEstimator`:
   - Minimum data quality (not SPARSE/FLAT)
   - Stable regime (not UNKNOWN, confidence > 0.6)
   - Persistent degradation trend (slope < 0 for >3 batches)
   - Low drift/novelty
3. When prerequisites fail, return status="NOT_RELIABLE", RUL_Hours=NULL
4. Update `ACM_RUL` table to include `Status` column
5. Update Grafana queries to show status

**Testing**:
- Unit test: RUL estimation with good vs bad data quality
- Integration test: verify NOT_RELIABLE on sparse data

**Success Criteria**:
- RUL writes NULL instead of numbers when unreliable
- Grafana shows reliability status
- No misleading low-RUL predictions on healthy equipment

---

### Phase 3: Data Contract Validation (Fail Fast)

**Goal**: Catch bad data at pipeline entry before it corrupts analytics

**Files**:
- `core/data_contract.py` (new)
- `core/acm_main.py`

**Changes**:
1. Create `DataContract` class with validation methods:
   - `validate_timestamp_order()` - monotonic increasing
   - `validate_no_duplicates()` - unique (timestamp, sensor)
   - `validate_no_future_rows()` - all timestamps <= now
   - `validate_cadence()` - median gap within expected range
2. Add `ContractViolation` exception class
3. Wire into `acm_main.py` at pipeline entry (before feature engineering)
4. On violation: log error, write to `ACM_RunLogs`, return NOOP
5. Add config flag `strict_data_contracts` (default: True)

**Testing**:
- Unit tests: each validation rule with good/bad data
- Integration test: batch with duplicates is rejected

**Success Criteria**:
- Bad data rejected with clear error message
- No analytics corruption from garbage inputs
- Operators see validation failures in logs

---

### Phase 4: Disable Point-Anomaly Alerts

**Goal**: Make episodes the only alerting primitive

**Files**:
- `core/acm_main.py`
- Config table

**Changes**:
1. Add config flag `episodes_only_alerts` (default: True)
2. When True, disable any point-anomaly alert logic
3. Ensure episode detection still runs
4. Document that point alerts are deprecated

**Testing**:
- Integration test: verify no alerts on isolated spikes
- Integration test: verify alerts on persistent episodes

**Success Criteria**:
- No alerts from single-point anomalies
- Episodes still detected and alerted correctly
- Reduced alert noise

---

### Phase 5: Detector Train-Score Separation

**Goal**: Ensure current batch cannot influence its own anomaly scores

**Files**:
- `core/ar1_detector.py`
- `core/outliers.py` (PCA, IForest, GMM)
- `core/omr.py`
- `core/correlation.py`

**Changes**:
1. Audit each detector's `fit()` and `score()` methods
2. Add explicit guards: training data != scoring data
3. Document separation contract in docstrings
4. Add assertion checks in development mode

**Known Issues**:
- AR1: Currently fits on full window including current batch
- PCA: Uses sliding window that includes current batch
- Fix: Ensure training uses only data BEFORE current batch

**Testing**:
- Unit test: fit(X_train), score(X_test) with overlapping data raises error
- Integration test: batch scores don't change when batch added to training

**Success Criteria**:
- Each detector enforces train != score
- No self-influence in anomaly detection
- Consistent scores across reruns

---

### Phase 6: Consolidate Forecasting

**Goal**: Remove duplicate forecasting logic, single source of truth

**Files**:
- Mark as deprecated: `forecasting_legacy.py`, `rul_engine_legacy.py`
- Primary: `core/forecast_engine.py`

**Changes**:
1. Verify all forecasting routes through `ForecastEngine`
2. Remove duplicate SQL write paths
3. Add deprecation warnings to legacy modules
4. Update documentation to reference only ForecastEngine

**Testing**:
- Verify forecasts work in batch mode
- Check no duplicate table writes

**Success Criteria**:
- Single forecasting orchestrator
- No confusion about which module is active
- Reduced maintenance burden

---

## What's Deferred to v11

The following v11 requirements are **NOT** addressed in v10.4:

- **Online/Offline Mode Split** (v11 Item #1) - Too invasive
- **ACM_ActiveModels Pointer** (v11 Item #2) - Requires new table
- **Maturity State Gating** (v11 Item #13) - Complex state machine
- **Offline Historical Replay** (v11 Item #14) - New runner script
- **Unified Baseline Normalization** (v11 Item #20) - Cross-module refactor
- **Calibrated Fusion** (v11 Item #22) - Algorithm change
- **Episode-Only Alerting** (v11 Item #6) - Partially done (disable points)
- **Drift/Novelty Control Plane** (v11 Item #10) - New subsystem

These require the full v11 architectural refactor.

---

## Testing Strategy

### Unit Tests
- Data contract validation rules
- RUL reliability checks
- Regime UNKNOWN assignment
- Detector train-score separation

### Integration Tests
- Full batch run with sparse data → UNKNOWN regimes
- Full batch run with low-quality health → RUL NOT_RELIABLE
- Batch with duplicate timestamps → contract violation
- Batch with good data → all features work normally

### Regression Tests
- Verify v10.3 functionality still works
- No breaking changes to existing dashboards
- Backward compatible with existing config

---

## Success Criteria

v10.4.0 is successful when:

1. ✅ System can output "UNKNOWN" regime (no forced assignment)
2. ✅ RUL includes reliability status (no misleading predictions)
3. ✅ Data contract violations rejected at pipeline entry
4. ✅ Point-anomaly alerts disabled (episodes-only)
5. ✅ Detectors enforce train-score separation
6. ✅ Forecasting consolidated (no duplication)
7. ✅ All existing tests pass
8. ✅ No regressions in core analytics
9. ✅ Documentation updated
10. ✅ Clear roadmap to v11

---

## Risk Mitigation

### Risk: Breaking existing dashboards
**Mitigation**: Add compatibility flags, update Grafana queries incrementally

### Risk: Performance degradation
**Mitigation**: Data contract checks are O(n), minimal overhead

### Risk: False UNKNOWN regimes
**Mitigation**: Tune confidence threshold (default 0.5, configurable)

### Risk: Too many NOT_RELIABLE RUL
**Mitigation**: Tune prerequisites, document requirements clearly

---

## Rollback Plan

If v10.4.0 causes issues:

1. Revert to v10.3.0 tag
2. Redeploy previous version
3. No database schema changes (backward compatible)
4. Config flags allow gradual feature adoption

---

## Timeline

- **Phase 1-2** (UNKNOWN regime, RUL gating): 2-3 hours
- **Phase 3** (Data contracts): 2 hours
- **Phase 4** (Disable point alerts): 1 hour
- **Phase 5** (Train-score separation): 3-4 hours
- **Phase 6** (Consolidate forecasting): 1 hour
- **Testing & validation**: 2 hours

**Total**: 11-14 hours of focused work

---

## Post-Release Actions

After v10.4.0 is stable:

1. Gather operator feedback on UNKNOWN regimes
2. Tune reliability gates based on real-world data
3. Monitor data contract rejection rates
4. Document lessons learned for v11 planning
5. Create v11 refactor tracker (already exists)

---

## Conclusion

v10.4.0 is a **tactical release** to fix critical issues without attempting the full v11 refactor. It makes ACM more honest, more stable, and better prepared for v11.

Key principle: **Remove broken features rather than trying to fix them halfway**.

The full v11 refactor remains the long-term goal, with 50+ items across 5 phases. v10.4.0 creates breathing room to execute v11 properly.
