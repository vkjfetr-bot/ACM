"""
Unit Tests for V11.2.1 Analytical Fixes

Tests the 6 critical analytical flaw fixes implemented in v11.2.1:
- FLAW #2: Prediction confidence horizon adjustment
- FLAW #3: Model lifecycle forecast quality gates
- FLAW #4: RUL reliability drift check
- FLAW #5: Health confidence detector agreement
- FLAW #6: Episode confidence temporal coherence
- FLAW #8: Data quality sigmoid confidence

Author: ACM Development Team
Date: 2026-01-04
Version: v11.2.1
"""

import pytest
import numpy as np
from datetime import datetime
from typing import List

from core.confidence import (
    compute_data_quality_confidence,
    compute_prediction_confidence,
    compute_health_confidence,
    compute_episode_confidence,
    check_rul_reliability,
    compute_rul_confidence,
    ReliabilityStatus,
    ConfidenceFactors,
)
from core.model_lifecycle import (
    MaturityState,
    ModelState,
    PromotionCriteria,
    check_promotion_eligibility,
    update_model_state_from_run,
)


# =============================================================================
# FLAW #8: Data Quality Sigmoid Confidence
# =============================================================================

class TestDataQualitySigmoid:
    """Test sigmoid confidence scaling vs old linear approach."""
    
    def test_boundary_values(self):
        """Test that min and max sample counts work correctly."""
        # Minimum samples should give 0.1
        conf_min = compute_data_quality_confidence(100, min_samples=100, optimal_samples=1000)
        assert abs(conf_min - 0.1) < 0.01
        
        # Optimal samples should give 1.0
        conf_opt = compute_data_quality_confidence(1000, min_samples=100, optimal_samples=1000)
        assert abs(conf_opt - 1.0) < 0.01
    
    def test_midpoint_reasonable(self):
        """Test that midpoint (550) gives reasonable confidence ~0.5."""
        conf_mid = compute_data_quality_confidence(550, min_samples=100, optimal_samples=1000)
        assert 0.4 < conf_mid < 0.6, f"Expected ~0.5, got {conf_mid}"
    
    def test_sigmoid_smooth(self):
        """Test that sigmoid is smooth (no jumps)."""
        counts = [100, 200, 400, 550, 700, 900, 1000]
        confidences = [
            compute_data_quality_confidence(c, min_samples=100, optimal_samples=1000)
            for c in counts
        ]
        
        # Check monotonically increasing
        for i in range(len(confidences) - 1):
            assert confidences[i] < confidences[i + 1], \
                f"Not monotonic: {confidences[i]} >= {confidences[i+1]}"
        
        # Check no large jumps (max delta < 0.2)
        deltas = [confidences[i+1] - confidences[i] for i in range(len(confidences) - 1)]
        assert max(deltas) < 0.2, f"Large jump detected: {max(deltas)}"
    
    def test_coverage_ratio(self):
        """Test that coverage_ratio scales confidence properly."""
        conf_full = compute_data_quality_confidence(500, coverage_ratio=1.0)
        conf_half = compute_data_quality_confidence(500, coverage_ratio=0.5)
        
        assert conf_half < conf_full
        assert abs(conf_half / conf_full - 0.5) < 0.1  # Approximately halved


# =============================================================================
# FLAW #2: Prediction Confidence Horizon Adjustment
# =============================================================================

class TestPredictionHorizonDecay:
    """Test time-to-horizon confidence decay."""
    
    def test_no_horizon_no_decay(self):
        """Test that horizon=0 gives base confidence with no decay."""
        base = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=0.0
        )
        assert base > 0.5  # Reasonable spread should give good confidence
    
    def test_horizon_decay(self):
        """Test that confidence decays with increasing horizon."""
        base = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=0.0
        )
        week = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=168.0  # 7 days
        )
        two_weeks = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=336.0  # 14 days
        )
        
        # Check decay
        assert week < base, "7-day horizon should reduce confidence"
        assert two_weeks < week, "14-day horizon should reduce further"
        
        # Check exponential relationship (7 days â‰ˆ 63% decay)
        assert 0.55 < week / base < 0.7, f"Expected ~0.63, got {week/base:.2f}"
    
    def test_characteristic_horizon(self):
        """Test that tau parameter controls decay rate."""
        # Fast decay (tau = 84h = 3.5 days)
        fast = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=168.0,
            characteristic_horizon=84.0
        )
        
        # Slow decay (tau = 336h = 14 days)
        slow = compute_prediction_confidence(
            p10=10, p50=50, p90=90,
            prediction_horizon_hours=168.0,
            characteristic_horizon=336.0
        )
        
        assert fast < slow, "Shorter tau should decay faster"


# =============================================================================
# FLAW #5: Health Confidence Detector Agreement
# =============================================================================

class TestDetectorAgreement:
    """Test inter-detector agreement factor."""
    
    def test_high_agreement(self):
        """Test that similar detector scores give high confidence."""
        conf_agree = compute_health_confidence(
            fused_z=3.0,
            detector_zscores=[2.8, 3.1, 2.9, 3.2, 2.95, 3.05],
            maturity_state="CONVERGED",
            sample_count=1000
        )
        assert conf_agree > 0.7, "High agreement should give high confidence"
    
    def test_low_agreement(self):
        """Test that disagreeing detectors reduce confidence."""
        conf_disagree = compute_health_confidence(
            fused_z=3.0,
            detector_zscores=[0, 2, 8, 1, 0.5, 3],
            maturity_state="CONVERGED",
            sample_count=1000
        )
        assert conf_disagree < 0.6, "High disagreement should reduce confidence"
    
    def test_agreement_penalty(self):
        """Test that disagreement penalty is significant (>20%)."""
        conf_agree = compute_health_confidence(
            fused_z=3.0,
            detector_zscores=[2.8, 3.1, 2.9, 3.2],
            maturity_state="CONVERGED",
            sample_count=1000
        )
        conf_disagree = compute_health_confidence(
            fused_z=3.0,
            detector_zscores=[0, 2, 8, 1],
            maturity_state="CONVERGED",
            sample_count=1000
        )
        
        penalty = (conf_agree - conf_disagree) / conf_agree
        assert penalty > 0.2, f"Expected >20% penalty, got {penalty:.1%}"
    
    def test_no_detectors_no_penalty(self):
        """Test that None detector_zscores defaults to agreement=1.0."""
        conf = compute_health_confidence(
            fused_z=3.0,
            detector_zscores=None,  # No detector info
            maturity_state="CONVERGED",
            sample_count=1000
        )
        # Should use prediction_factor=1.0 (agreement_factor)
        assert conf > 0.7


# =============================================================================
# FLAW #6: Episode Confidence Temporal Coherence
# =============================================================================

class TestEpisodeTemporalCoherence:
    """Test rise time factor for episode boundary sharpness."""
    
    def test_sharp_onset(self):
        """Test that sharp onset (rise < 10% duration) gives full confidence."""
        conf_sharp = compute_episode_confidence(
            episode_duration_seconds=100.0,
            peak_z=5.0,
            rise_time_seconds=5.0,  # 5% of duration
            maturity_state="CONVERGED"
        )
        # Should have rise_factor = 1.0
        assert conf_sharp > 0.7
    
    def test_slow_onset(self):
        """Test that slow onset (rise > 50% duration) reduces confidence."""
        conf_slow = compute_episode_confidence(
            episode_duration_seconds=100.0,
            peak_z=5.0,
            rise_time_seconds=60.0,  # 60% of duration
            maturity_state="CONVERGED"
        )
        # Should have rise_factor = 0.5
        assert conf_slow < 0.6
    
    def test_onset_speed_impact(self):
        """Test that onset speed significantly affects confidence."""
        conf_sharp = compute_episode_confidence(
            episode_duration_seconds=100.0,
            peak_z=5.0,
            rise_time_seconds=5.0,
            maturity_state="CONVERGED"
        )
        conf_slow = compute_episode_confidence(
            episode_duration_seconds=100.0,
            peak_z=5.0,
            rise_time_seconds=60.0,
            maturity_state="CONVERGED"
        )
        
        ratio = conf_sharp / conf_slow
        assert ratio > 1.3, f"Expected >30% difference, got {ratio:.2f}x"
    
    def test_no_rise_time_assumes_sharp(self):
        """Test that None rise_time defaults to rise_factor=1.0."""
        conf = compute_episode_confidence(
            episode_duration_seconds=100.0,
            peak_z=5.0,
            rise_time_seconds=None,  # Assume sharp
            maturity_state="CONVERGED"
        )
        # Should assume sharp boundary
        assert conf > 0.7


# =============================================================================
# FLAW #4: RUL Reliability Drift Check
# =============================================================================

class TestRULReliabilityDrift:
    """Test drift check in reliability gate."""
    
    def test_no_drift_reliable(self):
        """Test that low drift allows RELIABLE status."""
        status, reason = check_rul_reliability(
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            health_history_days=5.0,
            drift_z=1.5  # Below threshold
        )
        assert status == ReliabilityStatus.RELIABLE
    
    def test_high_drift_not_reliable(self):
        """Test that high drift blocks reliability."""
        status, reason = check_rul_reliability(
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            health_history_days=5.0,
            drift_z=4.0  # Above threshold
        )
        assert status == ReliabilityStatus.NOT_RELIABLE
        assert "drift" in reason.lower()
    
    def test_drift_threshold(self):
        """Test that drift_threshold parameter works."""
        # At threshold
        status_at, _ = check_rul_reliability(
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            health_history_days=5.0,
            drift_z=3.0,
            drift_threshold=3.0
        )
        # Just above threshold
        status_above, _ = check_rul_reliability(
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            health_history_days=5.0,
            drift_z=3.1,
            drift_threshold=3.0
        )
        
        assert status_at == ReliabilityStatus.RELIABLE
        assert status_above == ReliabilityStatus.NOT_RELIABLE
    
    def test_no_drift_check_if_none(self):
        """Test that drift_z=None skips drift check."""
        status, _ = check_rul_reliability(
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            health_history_days=5.0,
            drift_z=None  # No drift monitoring
        )
        assert status == ReliabilityStatus.RELIABLE


# =============================================================================
# FLAW #3: Model Lifecycle Forecast Quality Gates
# =============================================================================

class TestModelLifecycleForecastQuality:
    """Test forecast quality gates in promotion criteria."""
    
    def test_good_forecast_quality_passes(self):
        """Test that good forecast quality passes promotion."""
        state = ModelState(
            equip_id=1,
            version=1,
            maturity=MaturityState.LEARNING,
            created_at=datetime.now(),
            training_days=10.0,
            silhouette_score=0.3,
            stability_ratio=0.7,
            consecutive_runs=5,
            training_rows=500,
            forecast_mape=30.0,  # Good (< 50%)
            forecast_rmse=10.0   # Good (< 15)
        )
        
        eligible, reasons = check_promotion_eligibility(state)
        assert eligible, f"Should be eligible: {reasons}"
    
    def test_poor_mape_blocks_promotion(self):
        """Test that high MAPE blocks promotion."""
        state = ModelState(
            equip_id=1,
            version=1,
            maturity=MaturityState.LEARNING,
            created_at=datetime.now(),
            training_days=10.0,
            silhouette_score=0.3,
            stability_ratio=0.7,
            consecutive_runs=5,
            training_rows=500,
            forecast_mape=60.0,  # Poor (> 50%)
            forecast_rmse=10.0
        )
        
        eligible, reasons = check_promotion_eligibility(state)
        assert not eligible
        assert any("mape" in r.lower() for r in reasons)
    
    def test_poor_rmse_blocks_promotion(self):
        """Test that high RMSE blocks promotion."""
        state = ModelState(
            equip_id=1,
            version=1,
            maturity=MaturityState.LEARNING,
            created_at=datetime.now(),
            training_days=10.0,
            silhouette_score=0.3,
            stability_ratio=0.7,
            consecutive_runs=5,
            training_rows=500,
            forecast_mape=30.0,
            forecast_rmse=20.0  # Poor (> 15)
        )
        
        eligible, reasons = check_promotion_eligibility(state)
        assert not eligible
        assert any("rmse" in r.lower() for r in reasons)
    
    def test_update_forecast_metrics(self):
        """Test that update_model_state_from_run updates forecast metrics."""
        state = ModelState(
            equip_id=1,
            version=1,
            maturity=MaturityState.LEARNING,
            created_at=datetime.now(),
            training_days=5.0,
            training_rows=300,
            consecutive_runs=2
        )
        
        updated = update_model_state_from_run(
            state=state,
            run_id="run_123",
            run_success=True,
            forecast_mape=35.0,
            forecast_rmse=12.0
        )
        
        assert updated.forecast_mape == 35.0
        assert updated.forecast_rmse == 12.0
    
    def test_custom_criteria_thresholds(self):
        """Test that custom criteria thresholds work."""
        strict_criteria = PromotionCriteria(
            max_forecast_mape=30.0,  # Stricter than default 50%
            max_forecast_rmse=10.0   # Stricter than default 15
        )
        
        state = ModelState(
            equip_id=1,
            version=1,
            maturity=MaturityState.LEARNING,
            created_at=datetime.now(),
            training_days=10.0,
            silhouette_score=0.3,
            stability_ratio=0.7,
            consecutive_runs=5,
            training_rows=500,
            forecast_mape=40.0,  # Passes default but fails strict
            forecast_rmse=12.0   # Passes default but fails strict
        )
        
        # Default criteria should pass
        eligible_default, _ = check_promotion_eligibility(state)
        assert eligible_default
        
        # Strict criteria should fail
        eligible_strict, _ = check_promotion_eligibility(state, strict_criteria)
        assert not eligible_strict


# =============================================================================
# Integration Tests
# =============================================================================

class TestConfidenceIntegration:
    """Integration tests for complete confidence pipeline."""
    
    def test_rul_confidence_with_drift_and_horizon(self):
        """Test RUL confidence with both drift check and horizon decay."""
        # Good case: low drift, short horizon
        conf_good, status_good, _ = compute_rul_confidence(
            p10=10, p50=50, p90=90,
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            drift_z=1.0,
            prediction_horizon_hours=50.0
        )
        
        # Bad case: high drift, long horizon
        conf_bad, status_bad, _ = compute_rul_confidence(
            p10=10, p50=50, p90=90,
            maturity_state="CONVERGED",
            training_rows=1000,
            training_days=10.0,
            drift_z=4.0,  # High drift
            prediction_horizon_hours=500.0  # Long horizon
        )
        
        assert status_good == ReliabilityStatus.RELIABLE
        assert status_bad == ReliabilityStatus.NOT_RELIABLE
        assert conf_good > 0.4
        assert conf_bad < 0.3


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
