"""SQL Batch Runner - Continuous ACM processing from SQL historian with smart coldstart.

This script runs ACM continuously from SQL mode, handling:
1. Cold start - repeatedly calls ACM until coldstart completes successfully
2. Batch processing - processes all available data in tick-sized windows
3. Progress tracking - resumes from last successful batch

Usage examples:
    # Process single equipment until all data analyzed
    python scripts/sql_batch_runner.py --equip FD_FAN
    
    # Process multiple equipment in parallel
    python scripts/sql_batch_runner.py --equip FD_FAN GAS_TURBINE --max-workers 2
    
    # Resume from last successful run
    python scripts/sql_batch_runner.py --equip FD_FAN --resume
    
    # Dry run to see what would be processed
    python scripts/sql_batch_runner.py --equip FD_FAN --dry-run
"""

from __future__ import annotations

import argparse
import json
import subprocess
import sys
import textwrap
import os
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import pyodbc

# Ensure project root is on sys.path so `core` imports work when running as a script
project_root = Path(__file__).resolve().parents[1]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from core.output_manager import ALLOWED_TABLES
from core.observability import Console, init as init_observability, shutdown as shutdown_observability


class SQLBatchRunner:
    """Manages continuous batch processing from SQL historian."""
    
    def __init__(self, 
                 sql_conn_string: str,
                 artifact_root: Path,
                 tick_minutes: int = 240,
                 max_coldstart_attempts: int = 10,
                 max_batches: Optional[int] = None,
                 start_from_beginning: bool = False):
        """Initialize batch runner.
        
        Args:
            sql_conn_string: SQL Server connection string
            artifact_root: Root directory for artifacts
            tick_minutes: Window size in minutes (default: 30)
            max_coldstart_attempts: Max attempts to complete coldstart (default: 10)
        """
        self.sql_conn_string = sql_conn_string
        self.artifact_root = artifact_root
        self.tick_minutes = tick_minutes
        self.max_coldstart_attempts = max_coldstart_attempts
        self.progress_file = artifact_root / ".sql_batch_progress.json"
        self.max_batches = max_batches
        self.start_from_beginning = start_from_beginning

    def _log_historian_overview(self, equip_name: str) -> bool:
        """Preflight: Log historian table coverage and return True when data exists.

        This helps quickly diagnose cases where batch runs appear to succeed
        but no outputs are written because the historian query returns no rows.
        """
        try:
            table_name = f"{equip_name}_Data"
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                cur.execute(f"SELECT MIN(EntryDateTime), MAX(EntryDateTime), COUNT(*) FROM {table_name}")
                row = cur.fetchone()
                if not row:
                    Console.warn(f"[PRECHECK] {equip_name}: Historian table query returned no result", equipment=equip_name)
                    return False
                min_ts, max_ts, total_rows = row[0], row[1], int(row[2]) if row[2] is not None else 0
                if not min_ts or not max_ts:
                    Console.warn(f"[PRECHECK] {equip_name}: Historian table has no min/max timestamps", equipment=equip_name)
                    return False
                if total_rows <= 0:
                    Console.warn(
                        f"[PRECHECK] {equip_name}: Historian table has 0 rows; skipping processing",
                        equipment=equip_name,
                    )
                    Console.info(
                        f"[PRECHECK] {equip_name}: Ensure raw table {table_name} is populated for the expected window",
                        equipment=equip_name,
                        table=table_name,
                    )
                    return False
                Console.info(
                    f"[PRECHECK] {equip_name}: Historian coverage OK — range=[{min_ts},{max_ts}], rows={total_rows}",
                    equipment=equip_name,
                    min_timestamp=min_ts,
                    max_timestamp=max_ts,
                    total_rows=total_rows,
                )
                return True
        except Exception as e:
            Console.warn(f"[PRECHECK] {equip_name}: Historian overview failed: {e}", equipment=equip_name, error=str(e))
            return False
    
    def _get_sql_connection(self) -> pyodbc.Connection:
        """Create SQL connection with a short timeout."""
        # Use the pyodbc timeout parameter instead of a custom
        # connection-string attribute to avoid driver errors.
        return pyodbc.connect(self.sql_conn_string, timeout=10)

    def _test_sql_connection(self) -> bool:
        """Quick sanity check that SQL is reachable.

        This is used once per equipment run so that connection issues are
        clearly reported up front instead of being hit repeatedly inside the
        coldstart loop.
        """
        try:
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT 1")
                cur.fetchone()
            Console.info("Connection test OK", component="SQL")
            return True
        except Exception as exc:
            Console.error(f"SQL connection test failed: {exc}", error=str(exc))
            return False

    # ------------------------
    # SQL helpers (config/progress)
    # ------------------------
    def _get_equip_id(self, equip_name: str) -> Optional[int]:
        """Resolve or register EquipID for a given equipment code.

        Prefers calling dbo.usp_ACM_RegisterEquipment to create/return a stable ID.
        Falls back to reading from dbo.Equipment when the procedure isn't available.
        """
        try:
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                # Try registration procedure first (preferred)
                try:
                    tsql = (
                        "DECLARE @EID INT;\n"
                        "EXEC dbo.usp_ACM_RegisterEquipment @EquipCode = ?, @EquipID = @EID OUTPUT;\n"
                        "SELECT @EID;"
                    )
                    cur.execute(tsql, (equip_name,))
                    row = cur.fetchone()
                    if row and row[0] is not None:
                        eid = int(row[0])
                        Console.info(f"[ID] Registered/Resolved EquipID={eid} for {equip_name}", equipment=equip_name, equip_id=eid)
                        return eid
                except Exception:
                    # Fall back to direct lookup when SP missing or errors
                    pass

                # Fallback: direct lookup
                try:
                    cur.execute("SELECT EquipID FROM dbo.Equipment WHERE EquipCode = ?", (equip_name,))
                    row = cur.fetchone()
                    return int(row[0]) if row else None
                except Exception:
                    return None
        except Exception as e:
            Console.warn(f"Could not resolve EquipID for {equip_name}: {e}", error=str(e))
            return None

    def _get_config_int(self, equip_id: int, param_path: str, default_value: int) -> int:
        """Fetch integer config value from ACM_Config for an equipment, with default fallback."""
        try:
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    "SELECT TOP 1 ParamValue FROM dbo.ACM_Config WHERE EquipID = ? AND ParamPath = ? ORDER BY UpdatedAt DESC",
                    (equip_id, param_path)
                )
                row = cur.fetchone()
                if row and row[0] is not None:
                    try:
                        return int(row[0])
                    except ValueError:
                        return default_value
        except Exception as e:
            Console.warn(f"Could not read config {param_path} for EquipID={equip_id}: {e}", error=str(e))
        return default_value

    def _set_tick_minutes(self, equip_id: int, minutes: int) -> None:
        """Upsert runtime.tick_minutes in ACM_Config for the equipment (patched: no Category/ChangeReason)."""
        try:
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                # Try update first
                cur.execute(
                    "UPDATE dbo.ACM_Config SET ParamValue = ?, UpdatedAt = SYSUTCDATETIME() "
                    "WHERE EquipID = ? AND ParamPath = 'runtime.tick_minutes'",
                    (str(minutes), equip_id)
                )
                if cur.rowcount == 0:
                    # Insert (patched: only valid columns)
                    cur.execute(
                        "INSERT INTO dbo.ACM_Config (EquipID, ParamPath, ParamValue, ValueType, UpdatedBy, UpdatedAt) "
                        "VALUES (?, 'runtime.tick_minutes', ?, 'int', 'sql_batch_runner', SYSUTCDATETIME())",
                        (equip_id, str(minutes))
                    )
                conn.commit()
                Console.info(f"[CFG] Set runtime.tick_minutes={minutes} for EquipID={equip_id}", tick_minutes=minutes, equip_id=equip_id)
        except Exception as e:
            Console.warn(f"Could not set runtime.tick_minutes for EquipID={equip_id}: {e}", error=str(e))

    def _infer_tick_minutes_from_raw(self, equip_name: str, target_rows_per_batch: int = 5000) -> int:
        """Infer a reasonable tick size (minutes) from historian stats."""
        try:
            table_name = f"{equip_name}_Data"
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    f"SELECT MIN(EntryDateTime), MAX(EntryDateTime), COUNT(*) FROM {table_name}"
                )
                row = cur.fetchone()
                cur.close()
            if not row or not row[0] or not row[1] or not row[2]:
                return self.tick_minutes

            min_ts, max_ts, total_rows = row[0], row[1], int(row[2])
            total_minutes = max((max_ts - min_ts).total_seconds() / 60.0, 1.0)
            rows_per_minute = total_rows / total_minutes if total_minutes > 0 else 0.0
            if rows_per_minute <= 0:
                return self.tick_minutes

            # Require a small but non-zero sample per batch so SQL loads don't NOOP.
            min_rows_per_batch = 12  # prevents ACM from bailing on <10-row windows
            cadence_minutes = 1.0 / rows_per_minute if rows_per_minute > 0 else 30.0
            min_tick = int(max(5, math.ceil(min_rows_per_batch * cadence_minutes)))

            inferred = int(max(1, round(target_rows_per_batch / rows_per_minute)))
            max_tick = int(os.getenv("ACM_SQL_MAX_TICK_MINUTES", "1440"))  # allow up to 24h windows
            inferred = max(min_tick, min(inferred, max_tick))

            Console.info(
                f"[CONFIG] Inferred tick_minutes={inferred} for {equip_name} "
                f"(rows={total_rows}, minutes={total_minutes:.1f}, cadence={cadence_minutes:.2f}m)",
                tick_minutes=inferred, equipment=equip_name, total_rows=total_rows
            )
            if inferred == max_tick:
                Console.warn("[CONFIG] Clamped by ACM_SQL_MAX_TICK_MINUTES; override env var to expand further", max_tick=max_tick)
            return inferred
        except Exception as e:
            Console.warn(f"Could not infer tick_minutes from raw table for {equip_name}: {e}", error=str(e))
            return self.tick_minutes

    def _truncate_outputs_for_equip(self, equip_id: int) -> None:
        """
        Development helper: delete existing outputs for an equipment from ACM
        analytical tables so a dev batch run starts from a clean slate.
        
        Uses batched deletes for large tables to avoid transaction log bloat.
        """
        try:
            tables_list = sorted(ALLOWED_TABLES)
            total_tables = len(tables_list)
            Console.info(f"[RESET] Truncating {total_tables} ACM output tables for EquipID={equip_id}...", equip_id=equip_id, total_tables=total_tables)
            
            # Large tables that need batched deletion (can have millions of rows)
            large_tables = {
                'ACM_BaselineBuffer', 'ACM_SensorNormalized_TS', 'ACM_OMRContributionsLong',
                'ACM_PCA_Loadings', 'ACM_Scores_Long', 'ACM_ContributionTimeline',
                'ACM_RunLogs', 'ACM_SensorHotspotTimeline', 'ACM_HealthForecast',
                'ACM_FailureForecast', 'ACM_Scores_Wide'
            }
            
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                deleted_count = 0
                for idx, table in enumerate(tables_list, 1):
                    try:
                        # Check if table exists and has EquipID column
                        cur.execute(
                            f"SELECT CASE WHEN OBJECT_ID('dbo.{table}', 'U') IS NOT NULL "
                            f"AND COL_LENGTH('dbo.{table}', 'EquipID') IS NOT NULL THEN 1 ELSE 0 END"
                        )
                        can_delete = cur.fetchone()[0]
                        if not can_delete:
                            continue
                        
                        # For large tables, use batched delete to avoid massive transaction log
                        if table in large_tables:
                            batch_size = 50000
                            total_deleted = 0
                            while True:
                                cur.execute(
                                    f"DELETE TOP ({batch_size}) FROM dbo.{table} WHERE EquipID = ?",
                                    (equip_id,),
                                )
                                rows = cur.rowcount
                                total_deleted += rows
                                conn.commit()  # Commit each batch to release transaction log
                                if rows < batch_size:
                                    break
                            if total_deleted > 0:
                                deleted_count += 1
                                Console.info(f"[RESET] Deleted {total_deleted:,} rows from {table}", table=table, rows=total_deleted)
                        else:
                            # Small tables - single delete
                            cur.execute(
                                f"DELETE FROM dbo.{table} WHERE EquipID = ?",
                                (equip_id,),
                            )
                            rows_deleted = cur.rowcount
                            if rows_deleted > 0:
                                deleted_count += 1
                            conn.commit()
                    except Exception as tbl_err:
                        Console.warn(f"Failed to truncate {table} for EquipID={equip_id}: {tbl_err}", table=table, error=str(tbl_err))
                    # Progress indicator every 10 tables
                    if idx % 10 == 0:
                        Console.info(f"[RESET] Truncated {idx}/{total_tables} tables...", progress=idx, total=total_tables)
            Console.info(f"[RESET] Truncated {deleted_count} tables with data for EquipID={equip_id}", equip_id=equip_id, tables_truncated=deleted_count)
        except Exception as e:
            Console.warn(f"Failed to truncate outputs for EquipID={equip_id}: {e}", error=str(e))

    def _delete_models_for_equip(self, equip_id: int) -> None:
        """
        Delete existing models for an equipment from SQL ModelRegistry
        so coldstart truly rebuilds from scratch (SQL-ONLY MODE).
        """
        try:
            # Delete from SQL ModelRegistry
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                cur.execute(
                    "IF OBJECT_ID('dbo.ModelRegistry', 'U') IS NOT NULL "
                    "DELETE FROM dbo.ModelRegistry WHERE EquipID = ?",
                    (equip_id,),
                )
                deleted_count = cur.rowcount
                conn.commit()
            Console.info(f"[RESET] Deleted {deleted_count} models from SQL ModelRegistry for EquipID={equip_id}", equip_id=equip_id, deleted=deleted_count)
                
        except Exception as e:
            Console.warn(f"Failed to delete models for EquipID={equip_id}: {e}", error=str(e))

    def _inspect_last_run_outputs(self, equip_name: str) -> None:
        """
        Lightweight QA: after a batch run, report row counts in key tables for
        the last RunID for this equipment so a dev can spot anomalies.
        """
        try:
            equip_id = self._get_equip_id(equip_name)
            if not equip_id:
                Console.warn(f"[QA] EquipID not found for {equip_name}, skipping output inspection", equipment=equip_name)
                return
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                # Prefer deriving the latest RunID from freshly written forecast tables
                # to avoid mismatches when ACM_Runs ordering differs.
                run_id = None
                run_source = None
                # Try ACM_HealthForecast_TS first
                try:
                    cur.execute(
                        """
                        IF OBJECT_ID('dbo.ACM_HealthForecast_TS','U') IS NOT NULL
                        BEGIN
                          IF EXISTS(SELECT 1 FROM dbo.ACM_HealthForecast_TS WHERE EquipID = ?)
                          BEGIN
                            IF COL_LENGTH('dbo.ACM_HealthForecast_TS','CreatedAt') IS NOT NULL
                              SELECT TOP 1 RunID FROM dbo.ACM_HealthForecast_TS WHERE EquipID = ? GROUP BY RunID ORDER BY MAX(CreatedAt) DESC;
                            ELSE
                              SELECT TOP 1 RunID FROM dbo.ACM_HealthForecast_TS WHERE EquipID = ? GROUP BY RunID ORDER BY MAX([Timestamp]) DESC;
                          END
                          ELSE SELECT CAST(NULL AS UNIQUEIDENTIFIER);
                        END
                        ELSE SELECT CAST(NULL AS UNIQUEIDENTIFIER);
                        """,
                        (equip_id, equip_id, equip_id),
                    )
                    r = cur.fetchone()
                    if r and r[0]:
                        run_id = r[0]
                        run_source = "ACM_HealthForecast_TS"
                except Exception:
                    pass

                # Fallback: derive from ACM_FailureForecast_TS
                if run_id is None:
                    try:
                        cur.execute(
                            """
                            IF OBJECT_ID('dbo.ACM_FailureForecast_TS','U') IS NOT NULL
                            BEGIN
                              IF EXISTS(SELECT 1 FROM dbo.ACM_FailureForecast_TS WHERE EquipID = ?)
                              BEGIN
                                IF COL_LENGTH('dbo.ACM_FailureForecast_TS','CreatedAt') IS NOT NULL
                                  SELECT TOP 1 RunID FROM dbo.ACM_FailureForecast_TS WHERE EquipID = ? GROUP BY RunID ORDER BY MAX(CreatedAt) DESC;
                                ELSE
                                  SELECT TOP 1 RunID FROM dbo.ACM_FailureForecast_TS WHERE EquipID = ? GROUP BY RunID ORDER BY MAX([Timestamp]) DESC;
                              END
                              ELSE SELECT CAST(NULL AS UNIQUEIDENTIFIER);
                            END
                            ELSE SELECT CAST(NULL AS UNIQUEIDENTIFIER);
                            """,
                            (equip_id, equip_id, equip_id),
                        )
                        r = cur.fetchone()
                        if r and r[0]:
                            run_id = r[0]
                            run_source = "ACM_FailureForecast_TS"
                    except Exception:
                        pass

                # Final fallback: latest in ACM_Runs by StartedAt
                started_at = None
                completed_at = None
                if run_id is None:
                    cur.execute(
                        "SELECT TOP 1 RunID, StartedAt, CompletedAt FROM dbo.ACM_Runs WHERE EquipID = ? ORDER BY StartedAt DESC",
                        (equip_id,),
                    )
                    row = cur.fetchone()
                    if not row:
                        Console.warn(f"[QA] No ACM_Runs entry found for EquipID={equip_id}, skipping inspection", equip_id=equip_id)
                        return
                    run_id, started_at, completed_at = row[0], row[1], row[2]
                    run_source = "ACM_Runs"

                # If we derived from forecast tables, try to enrich with window from ACM_Runs
                if started_at is None or completed_at is None:
                    try:
                        cur.execute(
                            "SELECT TOP 1 StartedAt, CompletedAt FROM dbo.ACM_Runs WHERE RunID = ?",
                            (run_id,),
                        )
                        rw = cur.fetchone()
                        if rw:
                            started_at, completed_at = rw[0], rw[1]
                    except Exception:
                        pass

                Console.info(
                    f"[QA] Inspecting outputs for EquipID={equip_id}, RunID={run_id} (from {run_source}), "
                    f"window=[{started_at},{completed_at})",
                    equip_id=equip_id, run_id=str(run_id)
                )
                tables_to_check: List[Tuple[str, bool]] = [
                    ("ACM_HealthTimeline", True),
                    ("ACM_SensorHotspots", True),
                    ("ACM_DefectTimeline", True),
                    ("ACM_HealthForecast", True),
                    ("ACM_FailureForecast", True),
                    ("ACM_RUL", True),
                    ("ACM_EpisodeMetrics", True),
                ]
                for table_name, has_run in tables_to_check:
                    try:
                        if has_run:
                            cur.execute(
                                f"IF OBJECT_ID('dbo.{table_name}', 'U') IS NOT NULL "
                                f"SELECT COUNT(*) FROM dbo.{table_name} WHERE EquipID = ? AND RunID = ? "
                                f"ELSE SELECT 0",
                                (equip_id, run_id),
                            )
                        else:
                            cur.execute(
                                f"IF OBJECT_ID('dbo.{table_name}', 'U') IS NOT NULL "
                                f"SELECT COUNT(*) FROM dbo.{table_name} WHERE EquipID = ? "
                                f"ELSE SELECT 0",
                                (equip_id,),
                            )
                        cnt_row = cur.fetchone()
                        count_val = int(cnt_row[0]) if cnt_row else 0
                        Console.info(
                            f"[QA] {table_name}: {count_val} row(s) for EquipID={equip_id} "
                            f"{'(RunID scoped)' if has_run else ''}",
                            table=table_name, count=count_val
                        )
                    except Exception as tbl_err:
                        Console.warn(f"[QA] Skipped {table_name}: {tbl_err}", table=table_name, error=str(tbl_err))
        except Exception as e:
            Console.error(f"[QA] Output inspection failed for {equip_name}: {e}", equipment=equip_name, error=str(e))

    def _reset_progress_to_beginning(self, equip_id: int) -> None:
        """Optional: Clear Runs and Coldstart state to force start from earliest EntryDateTime."""
        try:
            with self._get_sql_connection() as conn:
                cur = conn.cursor()
                # Clear coldstart and runs for this equipment
                cur.execute("SET QUOTED_IDENTIFIER ON;")
                cur.execute("DELETE FROM dbo.ACM_ColdstartState WHERE EquipID = ?", (equip_id,))
                cur.execute("DELETE FROM dbo.ACM_Runs WHERE EquipID = ?", (equip_id,))
                conn.commit()
                Console.info(f"[RESET] Cleared ACM_Runs and Coldstart for EquipID={equip_id}", equip_id=equip_id)
        except Exception as e:
            Console.warn(f"Could not reset progress for EquipID={equip_id}: {e}", error=str(e))
    
    def _load_progress(self) -> Dict[str, Dict]:
        """Load progress tracking state.
        
        Returns:
            Dictionary with equipment progress: {
                'FD_FAN': {
                    'coldstart_complete': True,
                    'last_batch_end': '2012-01-10 00:00:00',
                    'batches_completed': 15
                }
            }
        """
        if not self.progress_file.exists():
            return {}
        
        try:
            with open(self.progress_file, "r") as f:
                data = json.load(f)
                return data
        except (json.JSONDecodeError, OSError) as exc:
            Console.warn(f"Could not load progress file: {exc}", error=str(exc))
            return {}
    
    def _save_progress(self, progress: Dict[str, Dict]) -> None:
        """Save progress tracking state."""
        self.artifact_root.mkdir(parents=True, exist_ok=True)
        try:
            with open(self.progress_file, "w") as f:
                json.dump(progress, f, indent=2, default=str)
        except OSError as exc:
            Console.warn(f"Could not save progress file: {exc}", error=str(exc))
    
    def _get_data_range(self, equip_name: str) -> tuple[Optional[datetime], Optional[datetime]]:
        """Get the available data range from SQL historian.
        
        Args:
            equip_name: Equipment name (e.g., 'FD_FAN')
            
        Returns:
            Tuple of (min_timestamp, max_timestamp) or (None, None) if no data
        """
        try:
            conn = self._get_sql_connection()
            cur = conn.cursor()
            
            table_name = f"{equip_name}_Data"
            query = f"SELECT MIN(EntryDateTime), MAX(EntryDateTime) FROM {table_name}"
            cur.execute(query)
            row = cur.fetchone()
            
            cur.close()
            conn.close()
            
            if row and row[0] and row[1]:
                return row[0], row[1]
            return None, None
            
        except Exception as e:
            Console.error(f"Failed to get data range for {equip_name}: {e}", equipment=equip_name, error=str(e))
            return None, None
    
    def _check_coldstart_status(self, equip_name: str) -> tuple[bool, int, int]:
        """Check if coldstart is complete for equipment.
        
        Args:
            equip_name: Equipment name
            
        Returns:
            Tuple of (is_complete, accumulated_rows, required_rows)
        """
        Console.info(f"[COLDSTART] {equip_name}: Checking coldstart status in SQL (ModelRegistry/ACM_ColdstartState)...", equipment=equip_name)
        try:
            conn = self._get_sql_connection()
            cur = conn.cursor()
            
            # Get EquipID from Equipment table
            cur.execute("SELECT EquipID FROM Equipment WHERE EquipCode = ?", (equip_name,))
            row = cur.fetchone()
            if not row:
                cur.close()
                conn.close()
                # When equip not found, use default min rows 50
                Console.warn(f"[COLDSTART] {equip_name}: Equipment not found in Equipment table; using default minimum rows=50", equipment=equip_name)
                return False, 0, 50
            
            equip_id = row[0]
            
            # Check ModelRegistry for existing models
            cur.execute("""
                SELECT COUNT(*) FROM ModelRegistry 
                WHERE EquipID = ? AND ModelType IN ('pca_model', 'gmm_model', 'iforest_model')
            """, (equip_id,))
            _row_mc = cur.fetchone()
            model_count = int(_row_mc[0]) if _row_mc else 0
            
            # Check coldstart state
            cur.execute("""
                SELECT Status, AccumulatedRows, RequiredRows 
                FROM ACM_ColdstartState 
                WHERE EquipID = ? AND Stage = 'score'
            """, (equip_id,))
            row = cur.fetchone()
            
            cur.close()
            conn.close()
            
            # Coldstart complete if models exist
            if model_count >= 3:
                Console.info(f"[COLDSTART] {equip_name}: Detected existing models in ModelRegistry (count={model_count})", equipment=equip_name, model_count=model_count)
                return True, 0, 0
            
            # Determine required rows: prefer ColdstartState.RequiredRows, else config runtime.coldstart_min_rows (default 50)
            min_required = self._get_config_int(equip_id, 'runtime.coldstart_min_rows', 50)
            if row:
                status, accum_rows, req_rows = row
                required = req_rows or min_required
                is_complete = status == 'COMPLETE'
                Console.info(
                    f"[COLDSTART] {equip_name}: Status={status}, "
                    f"AccumulatedRows={accum_rows or 0}, RequiredRows={required}",
                    equipment=equip_name, status=status, accumulated=accum_rows or 0, required=required
                )
                return is_complete, accum_rows or 0, required
            Console.info(f"[COLDSTART] {equip_name}: No ACM_ColdstartState row; using default minimum rows={min_required}", equipment=equip_name, min_required=min_required)
            return False, 0, min_required
            
        except Exception as e:
            Console.warn(f"Could not check coldstart status: {e}", error=str(e))
            return False, 0, 50
    
    def _run_acm_batch(self, equip_name: str, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None, *, dry_run: bool = False, batch_num: int = 0) -> tuple[bool, str]:
        """Run single ACM batch for equipment.
        
        Args:
            equip_name: Equipment name
            start_time: Optional start time override
            end_time: Optional end time override
            dry_run: If True, print command without running
            batch_num: Current batch number (for frequency control)
            
        Returns:
            Tuple of (success, outcome) where outcome is 'OK', 'NOOP', or 'FAIL'
        """
        cmd = [
            sys.executable, "-m", "core.acm_main",
            "--equip", equip_name,
        ]
        
        if start_time:
            cmd.extend(["--start-time", start_time.isoformat()])
        if end_time:
            cmd.extend(["--end-time", end_time.isoformat()])
        
        printable = " ".join(cmd)
        if dry_run:
            Console.info(f"{printable}", mode="dry-run", component="DRY")
            return True, "OK"
        
        Console.info(f"[RUN] {printable}", command=printable)
        # Force SQL mode in acm_main so that SQL historian + stored procedures
        # are used instead of legacy CSV/file mode, regardless of older config.
        # Also set ACM_BATCH_MODE to enable continuous learning mode detection
        # Pass batch number for threshold update frequency control
        env = dict(os.environ)
        env["ACM_FORCE_SQL_MODE"] = "1"
        env["ACM_BATCH_MODE"] = "1"
        # Propagate start-from-beginning intent to forecasting layer (used to force full-history model init)
        # Note: batch_num is 0-indexed internally; display as 1-indexed for users
        display_batch = batch_num + 1
        if self.start_from_beginning and batch_num == 0:
            env["ACM_FORECAST_FULL_HISTORY_MODE"] = "1"
            Console.info(f"[BATCH] {equip_name}: First batch (start-from-beginning) - will perform coldstart split and train fresh models", equipment=equip_name, batch_num=display_batch)
        else:
            Console.info(f"[BATCH] {equip_name}: Batch {display_batch} - will load existing models and evolve incrementally", equipment=equip_name, batch_num=display_batch)
        env["ACM_BATCH_NUM"] = str(batch_num)
        # Pass total batches info (if known) so acm_main can display "batch X/Y"
        if hasattr(self, '_current_total_batches'):
            env["ACM_BATCH_TOTAL"] = str(self._current_total_batches)

        # Stream child output live so devs can see progress (instead of buffering everything).
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env=env,
        )
        captured_lines: list[str] = []
        try:
            assert process.stdout is not None
            for line in process.stdout:
                # Stream child output directly to stdout (already has timestamp/level from ACMLog)
                # Don't use Console.info() which would add duplicate timestamp/level prefix
                print(line.rstrip("\n"), flush=True)
                captured_lines.append(line)
        except KeyboardInterrupt:
            process.kill()
            raise
        finally:
            if process.stdout:
                process.stdout.close()
        process.wait()

        stdout_text = "".join(captured_lines)
        # Parse outcome from logs
        outcome = "FAIL"
        if process.returncode == 0:
            for line in stdout_text.split('\n'):
                if 'outcome=OK' in line:
                    outcome = "OK"
                    break
                elif 'outcome=NOOP' in line:
                    outcome = "NOOP"
                    break
            else:
                outcome = "OK"
        
        success = process.returncode == 0

        # If the batch failed or outcome was not OK/NOOP, surface logs so the
        # caller can see exactly what went wrong inside acm_main.
        if not success or outcome == "FAIL":
            Console.error(f"[RUN-DEBUG] {equip_name}: acm_main exited with code {process.returncode}", equipment=equip_name, return_code=process.returncode)
            if stdout_text:
                Console.error(
                    f"[RUN-DEBUG] {equip_name}: --- acm_main stdout (captured) ---\n{stdout_text.rstrip()}",
                    equipment=equip_name,
                )

        if success and outcome in ("OK", "NOOP"):
            # After a successful batch, inspect SQL outputs for this equipment
            self._inspect_last_run_outputs(equip_name)
        return success, outcome
    
    def _process_coldstart(self, equip_name: str, *, dry_run: bool = False) -> tuple[bool, Optional[datetime]]:
        """Process coldstart phase for equipment.
        
        Continuously runs ACM until coldstart completes or max attempts reached.
        
        Args:
            equip_name: Equipment name
            dry_run: If True, simulate without running
            
        Returns:
            True if coldstart completed successfully
        """
        Console.info(f"\n{'='*60}")
        Console.info(f"[COLDSTART] Starting coldstart for {equip_name}", equipment=equip_name)
        Console.info(f"{'='*60}")
        
        # Get earliest data timestamp for historical replay
        min_ts, max_ts = self._get_data_range(equip_name)
        if not min_ts or not max_ts:
            Console.error(f"[COLDSTART] {equip_name}: No data available in historian", equipment=equip_name)
            return False, None
        
        Console.info(f"[COLDSTART] {equip_name}: Historical data range: {min_ts} to {max_ts}", equipment=equip_name, min_ts=min_ts, max_ts=max_ts)
        
        # Start coldstart from earliest timestamp
        coldstart_start = min_ts
        # SP uses <= for end time, so we need to include the full last day
        # For a 24h window, we want [00:00:00, 23:59:59] not [00:00:00, 00:00:00]
        coldstart_end = min_ts + timedelta(minutes=self.tick_minutes) - timedelta(seconds=1)
        
        last_processed_end: Optional[datetime] = None
        for attempt in range(1, self.max_coldstart_attempts + 1):
            Console.info(f"\n[COLDSTART] {equip_name}: Attempt {attempt}/{self.max_coldstart_attempts}", equipment=equip_name, attempt=attempt, max_attempts=self.max_coldstart_attempts)
            
            # Check current status
            is_complete, accum_rows, req_rows = self._check_coldstart_status(equip_name)
            if is_complete:
                Console.ok(f"[COLDSTART] {equip_name}: Coldstart COMPLETE!", equipment=equip_name)
                # If models already existed before any processing this run, we may not
                # have a concrete window end; return whatever we last computed (likely None)
                return True, last_processed_end
            
            Console.info(f"[COLDSTART] {equip_name}: Status - {accum_rows}/{req_rows} rows accumulated", equipment=equip_name, accumulated=accum_rows, required=req_rows)
            Console.info(f"[COLDSTART] {equip_name}: Processing window [{coldstart_start} to {coldstart_end})", equipment=equip_name, start=coldstart_start, end=coldstart_end)
            
            # Run ACM batch with historical time window
            success, outcome = self._run_acm_batch(equip_name, start_time=coldstart_start, end_time=coldstart_end, dry_run=dry_run)
            # Track the last processed coldstart window end so batch phase can continue after it
            last_processed_end = coldstart_end
            
            if not success and outcome == "FAIL":
                Console.error(f"[COLDSTART] {equip_name}: Attempt {attempt} FAILED (error)", equipment=equip_name, attempt=attempt)
                continue
            
            if outcome == "NOOP":
                Console.warn(f"[COLDSTART] {equip_name}: Deferred (insufficient data), will retry...", equipment=equip_name)
                continue
            
            if outcome == "OK":
                # Check if coldstart completed
                is_complete, _, _ = self._check_coldstart_status(equip_name)
                if is_complete:
                    Console.ok(f"[COLDSTART] {equip_name}: Coldstart COMPLETE!", equipment=equip_name)
                    return True, last_processed_end
                else:
                    Console.info(f"[COLDSTART] {equip_name}: Making progress, continuing...", equipment=equip_name)
                    # Advance window for next coldstart attempt
                    # Add 1 second back to move to start of next day, then subtract 1 second for the end bound
                    coldstart_start = coldstart_end + timedelta(seconds=1)
                    coldstart_end = coldstart_start + timedelta(minutes=self.tick_minutes) - timedelta(seconds=1)
                    if coldstart_end > max_ts:
                        coldstart_end = max_ts
        
        Console.warn(f"[COLDSTART] {equip_name}: Max attempts ({self.max_coldstart_attempts}) reached without completion", equipment=equip_name, max_attempts=self.max_coldstart_attempts)
        return False, last_processed_end
    
    def _process_batches(self, equip_name: str, start_from: Optional[datetime] = None, 
                        *, dry_run: bool = False, resume: bool = False) -> int:
        """Process all available data in batches.
        
        Args:
            equip_name: Equipment name
            start_from: Starting timestamp (if None, starts from beginning)
            dry_run: If True, simulate without running
            resume: If True, resume from last successful batch
            
        Returns:
            Number of batches successfully processed
        """
        Console.info(f"\n{'='*60}")
        Console.info(f"[BATCH] Starting batch processing for {equip_name}", equipment=equip_name)
        Console.info(f"{'='*60}")
        
        # Get data range
        min_ts, max_ts = self._get_data_range(equip_name)
        if not min_ts or not max_ts:
            Console.warn(f"[BATCH] {equip_name}: No data available in historian", equipment=equip_name)
            return 0
        
        Console.info(f"[BATCH] {equip_name}: Data available from {min_ts} to {max_ts}", equipment=equip_name, min_timestamp=min_ts, max_timestamp=max_ts)
        
        # Load progress
        progress = self._load_progress()
        equip_progress = progress.get(equip_name, {})
        
        # Determine starting point
        if resume and 'last_batch_end' in equip_progress:
            current_ts = datetime.fromisoformat(equip_progress['last_batch_end'])
            batches_completed = equip_progress.get('batches_completed', 0)
            Console.info(f"[BATCH] {equip_name}: Resuming from {current_ts} ({batches_completed} batches already completed)", equipment=equip_name, resume_from=current_ts, batches_completed=batches_completed)
        elif start_from:
            current_ts = start_from
            batches_completed = 0
        else:
            current_ts = min_ts
            batches_completed = 0
        
        # Calculate total batches
        total_minutes = max((max_ts - current_ts).total_seconds() / 60, 0)
        total_batches = int(total_minutes / self.tick_minutes) if self.tick_minutes > 0 else 0

        # If a demo cap is provided, automatically widen the batch window so
        # the full history fits in at most max_batches windows. This keeps
        # long histories from exploding into thousands of tiny batches.
        if self.max_batches is not None and self.max_batches > 0 and total_batches > self.max_batches:
            new_tick = int(math.ceil(total_minutes / self.max_batches)) or self.tick_minutes
            if new_tick > self.tick_minutes:
                Console.info(
                    f"[BATCH] {equip_name}: Adjusting tick_minutes from {self.tick_minutes} "
                    f"to {new_tick} to honor max-batches={self.max_batches}",
                    equipment=equip_name, old_tick=self.tick_minutes, new_tick=new_tick, max_batches=self.max_batches
                )
                self.tick_minutes = new_tick
                total_batches = int(total_minutes / self.tick_minutes) if self.tick_minutes > 0 else 0
        
        Console.info(f"[BATCH] {equip_name}: Processing {total_batches} batch(es) ({self.tick_minutes}-minute windows)", equipment=equip_name, total_batches=total_batches, tick_minutes=self.tick_minutes)
        
        # Store total for passing to child processes
        self._current_total_batches = total_batches
        
        # Process batches
        batch_num = 0
        while current_ts < max_ts:
            batch_num += 1
            # SP uses <= for end time, so subtract 1 second to get [start, end] inclusive of full last period
            next_ts = current_ts + timedelta(minutes=self.tick_minutes) - timedelta(seconds=1)
            
            # Don't go beyond available data
            if next_ts > max_ts:
                next_ts = max_ts
            
            Console.info(f"\n[BATCH] {equip_name}: Batch {batch_num}/{total_batches} - [{current_ts} to {next_ts}]", equipment=equip_name, batch=batch_num, total=total_batches)
            
            # Run ACM (it will automatically use the current batch window from SQL)
            # Pass batches_completed (total count including previous runs) for frequency control
            success, outcome = self._run_acm_batch(equip_name, start_time=current_ts, end_time=next_ts, dry_run=dry_run, batch_num=batches_completed)
            
            if not success:
                Console.error(f"[BATCH] {equip_name}: Batch {batch_num} FAILED", equipment=equip_name, batch=batch_num)
                break
            
            batches_completed += 1
            
            # Update progress
            equip_progress['last_batch_end'] = next_ts.isoformat()
            equip_progress['batches_completed'] = batches_completed
            equip_progress['coldstart_complete'] = True
            progress[equip_name] = equip_progress
            
            if not dry_run:
                self._save_progress(progress)
            
            Console.ok(f"[BATCH] {equip_name}: Batch {batch_num} completed (outcome={outcome})", equipment=equip_name, batch=batch_num, outcome=outcome)
            
            # Respect demo cap if provided
            if self.max_batches is not None and batch_num >= self.max_batches:
                Console.info(f"[BATCH] Reached max-batches cap ({self.max_batches}); stopping early", max_batches=self.max_batches)
                break

            # Move to next window (add 1 second to move past the end of the current window)
            current_ts = next_ts + timedelta(seconds=1)
        
        Console.info(f"\n[BATCH] {equip_name}: Processed {batches_completed} batch(es)", equipment=equip_name, batches_completed=batches_completed)
        return batches_completed
    
    def process_equipment(self, equip_name: str, *, dry_run: bool = False, 
                         resume: bool = False) -> bool:
        """Process single equipment through coldstart and batch phases.
        
        Args:
            equip_name: Equipment name
            dry_run: If True, simulate without running
            resume: If True, resume from last successful run
            
        Returns:
            True if processing completed successfully
        """
        import time
        start_time = time.time()
        
        Console.info(f"\n{'#'*60}")
        Console.info(f"# Processing Equipment: {equip_name}", equipment=equip_name)
        Console.info(f"{'#'*60}")

        # Fail fast if SQL is unreachable so we do not appear hung
        if not self._test_sql_connection():
            Console.error(f"{equip_name}: Skipping processing due to SQL connection failure", equipment=equip_name)
            return False
        
        # Load progress
        progress = self._load_progress()
        equip_progress = progress.get(equip_name, {})

        # Apply per-run configuration overrides
        equip_id = self._get_equip_id(equip_name)
        if equip_id:
            Console.info(f"[PRECHECK] {equip_name}: Resolved EquipID={equip_id}", equipment=equip_name, equip_id=equip_id)
            # In dev mode, optionally infer tick size from raw data
            if self.start_from_beginning and not resume:
                Console.info(f"[RESET] Starting from beginning for {equip_name} - performing full reset", equipment=equip_name)
                inferred = self._infer_tick_minutes_from_raw(equip_name)
                self.tick_minutes = inferred
                self._set_tick_minutes(equip_id, inferred)
                self._truncate_outputs_for_equip(equip_id)
                # CRITICAL: Delete ALL existing models (SQL + filesystem) so first batch
                # starts with fresh coldstart training. This ensures batch 0 trains new models,
                # and subsequent batches evolve those models incrementally.
                Console.info(f"[RESET] Deleting all existing models (SQL + filesystem) for {equip_name}", equipment=equip_name)
                self._delete_models_for_equip(equip_id)
                self._reset_progress_to_beginning(equip_id)
            else:
                self._set_tick_minutes(equip_id, self.tick_minutes)
            
            # CRITICAL: Adjust tick_minutes AFTER inference if max_batches specified
            # This ensures coldstart uses the same batch size as regular processing
            if self.max_batches is not None and self.max_batches > 0:
                min_ts, max_ts = self._get_data_range(equip_name)
                if min_ts and max_ts:
                    total_minutes = max((max_ts - min_ts).total_seconds() / 60, 0)
                    total_batches = int(total_minutes / self.tick_minutes) if self.tick_minutes > 0 else 0
                    if total_batches > self.max_batches:
                        new_tick = int(math.ceil(total_minutes / self.max_batches)) or self.tick_minutes
                        if new_tick > self.tick_minutes:
                            Console.info(
                                f"[CONFIG] {equip_name}: Adjusting tick_minutes from {self.tick_minutes} "
                                f"to {new_tick} to honor max-batches={self.max_batches} (applies to coldstart AND batches)",
                                equipment=equip_name, old_tick=self.tick_minutes, new_tick=new_tick, max_batches=self.max_batches
                            )
                            self.tick_minutes = new_tick
                            self._set_tick_minutes(equip_id, new_tick)
        else:
            Console.warn(f"[PRECHECK] {equip_name}: EquipID not found in dbo.Equipment; downstream writes will fail", equipment=equip_name)

        # Historian preflight: if no data rows, stop early with a clear message
        if not self._log_historian_overview(equip_name):
            Console.error(f"{equip_name}: Historian has no data — aborting this equipment run", equipment=equip_name)
            return False
        
        # Check if coldstart already complete
        coldstart_complete = equip_progress.get('coldstart_complete', False)
        
        if resume and coldstart_complete:
            Console.info(f"{equip_name}: Coldstart already complete, skipping to batch processing", equipment=equip_name)
            coldstart_last_end: Optional[datetime] = None
        else:
            # Phase 1: Coldstart
            cs_ok, coldstart_last_end = self._process_coldstart(equip_name, dry_run=dry_run)
            if not cs_ok:
                Console.error(f"{equip_name}: Coldstart failed", equipment=equip_name)
                return False
            
            # Update progress
            equip_progress['coldstart_complete'] = True
            progress[equip_name] = equip_progress
            if not dry_run:
                self._save_progress(progress)
        
        # Phase 2: Batch processing
        # If we just completed coldstart during this run, start the batch phase
        # immediately after the coldstart window to avoid reprocessing the same window.
        start_from_ts: Optional[datetime] = None
        try:
            # Only honor coldstart_last_end when we executed coldstart above and not in resume-fast path
            if not (resume and coldstart_complete) and 'coldstart_last_end' in locals() and coldstart_last_end is not None:
                start_from_ts = coldstart_last_end + timedelta(seconds=1)
        except Exception:
            start_from_ts = None

        batches = self._process_batches(equip_name, start_from=start_from_ts, dry_run=dry_run, resume=resume)
        
        elapsed_time = time.time() - start_time
        elapsed_minutes = int(elapsed_time / 60)
        elapsed_seconds = int(elapsed_time % 60)
        
        if batches > 0:
            Console.ok(f"{equip_name}: Completed - {batches} batch(es) processed", equipment=equip_name, batches=batches)
            Console.info(f"[TIMING] {equip_name}: Total time = {elapsed_minutes}m {elapsed_seconds}s", equipment=equip_name, minutes=elapsed_minutes, seconds=elapsed_seconds)
            return True
        else:
            Console.warn(f"{equip_name}: No batches processed", equipment=equip_name)
            Console.info(f"[TIMING] {equip_name}: Total time = {elapsed_minutes}m {elapsed_seconds}s", equipment=equip_name, minutes=elapsed_minutes, seconds=elapsed_seconds)
            return False


def main() -> int:
    parser = argparse.ArgumentParser(
        description="SQL Batch Runner - Continuous ACM processing from SQL historian",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent("""
            Processing Flow:
              1. COLDSTART PHASE: Repeatedly calls ACM until coldstart completes
                 - Auto-detects data cadence
                 - Loads from earliest available data
                 - Retries with exponential window expansion
                 - Tracks progress in ACM_ColdstartState table
              
              2. BATCH PHASE: Processes all available data in tick-sized windows
                 - Continues from coldstart end point
                 - Processes batches sequentially
                 - Tracks progress in .sql_batch_progress.json
              
            Notes:
              • Requires SQL mode: runtime.storage_backend='sql' in ACM_Config
              • Progress tracking allows resume after interruption
              • Use --dry-run to preview without execution
              • Use --resume to skip completed batches
        """),
    )
    parser.add_argument("--equip", nargs="+", required=True,
                        help="Equipment codes to process (e.g., FD_FAN GAS_TURBINE)")
    parser.add_argument("--sql-server", default="localhost\\B19CL3PCQLSERVER",
                        help="SQL Server instance (default: localhost\\B19CL3PCQLSERVER)")
    parser.add_argument("--sql-database", default="ACM",
                        help="SQL database name (default: ACM)")
    parser.add_argument("--tick-minutes", type=int, default=30,
                        help="Batch window size in minutes (default: 30)")
    parser.add_argument("--max-coldstart-attempts", type=int, default=10,
                        help="Max coldstart retry attempts (default: 10)")
    parser.add_argument("--max-workers", type=int, default=1,
                        help="Number of equipment to process in parallel (default: 1)")
    parser.add_argument("--max-batches", type=int, default=None,
                        help="For demos: cap number of batches per equipment (default: unlimited)")
    parser.add_argument("--start-from-beginning", action="store_true",
                        help="Development: reset Runs/Coldstart to begin at earliest data timestamp")
    parser.add_argument("--resume", action="store_true",
                        help="Resume from last successful batch")
    parser.add_argument("--dry-run", action="store_true",
                        help="Print commands without running")

    args = parser.parse_args()

    # Build SQL connection string (login timeout is controlled via pyodbc.connect timeout)
    sql_conn_string = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={args.sql_server};"
        f"DATABASE={args.sql_database};"
        f"Trusted_Connection=yes;"
    )

    artifact_root = Path("artifacts").resolve()
    
    # Initialize observability for batch runner logging to Loki/Tempo/Prometheus
    # Note: acm_main.py will re-init with per-equipment context, but this enables
    # batch runner Console calls to also go to Loki before ACM invocation
    import os
    loki_url = os.environ.get("LOKI_URL", "http://localhost:3100")
    otlp_endpoint = os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4318")
    
    init_observability(
        service_name="acm-batch-runner",
        equipment="batch_runner",
        equip_id=0,
        run_id="batch-runner-main",
        enable_loki=True,
        enable_tracing=True,
        enable_metrics=True,
        loki_endpoint=loki_url,
        otlp_endpoint=otlp_endpoint,
    )
    
    # Create runner
    runner = SQLBatchRunner(
        sql_conn_string=sql_conn_string,
        artifact_root=artifact_root,
        tick_minutes=args.tick_minutes,
        max_coldstart_attempts=args.max_coldstart_attempts,
        max_batches=args.max_batches,
        start_from_beginning=args.start_from_beginning
    )

    max_workers = max(1, args.max_workers)
    errors: List[str] = []

    Console.header("SQL BATCH RUNNER - Continuous ACM Processing")
    Console.info(f"Equipment: {', '.join(args.equip)}", equipment=args.equip)
    Console.info(f"SQL Server: {args.sql_server}/{args.sql_database}", server=args.sql_server, database=args.sql_database)
    Console.info(f"Tick Window: {args.tick_minutes} minutes", tick_minutes=args.tick_minutes)
    Console.info(f"Max Workers: {max_workers}", max_workers=max_workers)
    Console.info(f"Resume: {args.resume}", resume=args.resume)
    Console.info(f"Dry Run: {args.dry_run}", dry_run=args.dry_run)
    Console.status("="*60)

    import time
    overall_start_time = time.time()

    # Process equipment (sequentially or in parallel)
    if max_workers == 1:
        # Sequential processing
        for equip in args.equip:
            try:
                success = runner.process_equipment(equip, dry_run=args.dry_run, resume=args.resume)
                if not success:
                    errors.append(f"{equip}: Processing incomplete")
            except Exception as exc:
                errors.append(f"{equip}: {exc}")
                Console.error(f"{equip}: {exc}", equipment=equip, error=str(exc))
    else:
        # Parallel processing
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_map = {
                executor.submit(
                    runner.process_equipment,
                    equip,
                    dry_run=args.dry_run,
                    resume=args.resume
                ): equip for equip in args.equip
            }
            for future in as_completed(future_map):
                equip = future_map[future]
                try:
                    success = future.result()
                    if not success:
                        errors.append(f"{equip}: Processing incomplete")
                except Exception as exc:
                    # Sanitize exception text to ASCII to avoid Windows cp1252 encode errors
                    exc_text = str(exc)
                    try:
                        exc_text.encode("cp1252")
                    except Exception:
                        exc_text = exc_text.encode("ascii", "ignore").decode()
                    errors.append(f"{equip}: {exc_text}")
                    Console.error(f"{equip}: {exc_text}", equipment=equip, error=exc_text)

    overall_elapsed = time.time() - overall_start_time
    overall_minutes = int(overall_elapsed / 60)
    overall_seconds = int(overall_elapsed % 60)

    Console.status("\n" + "="*60)
    Console.info(f"[TIMING] Overall execution time: {overall_minutes}m {overall_seconds}s", minutes=overall_minutes, seconds=overall_seconds)
    Console.status("="*60)
    
    # Shutdown observability to flush any pending logs
    shutdown_observability()
    
    if errors:
        Console.error("BATCH RUNNER COMPLETED WITH ERRORS:")
        for line in errors:
            Console.error(f"  [FAIL] {line}")
        Console.status("="*60)
        return 1
    else:
        Console.ok("BATCH RUNNER COMPLETED SUCCESSFULLY")
        Console.status("="*60)
        return 0


if __name__ == "__main__":
    sys.exit(main())
